Quantifiers and Equality - Theorem Proving in Lean 4
Theorem Proving in Lean 4
1.
Introduction
2.
Dependent Type Theory
3.
Propositions and Proofs
4.
Quantifiers and Equality
5.
Tactics
6.
Interacting with Lean
7.
Inductive Types
8.
Induction and Recursion
9.
Structures and Records
10.
Type Classes
11.
The Conversion Tactic Mode
12.
Axioms and Computation
Light (default)
Rust
Coal
Navy
Ayu
Theorem Proving in Lean 4
Quantifiers and Equality
The last chapter introduced you to methods that construct proofs of
statements involving the propositional connectives. In this chapter,
we extend the repertoire of logical constructions to include the
universal and existential quantifiers, and the equality relation.
The Universal Quantifier
Notice that if
α
is any type, we can represent a unary predicate
p
on
α
as an object of type
α → Prop
. In that case, given
x : α
,
p x
denotes the assertion that
p
holds of
x
. Similarly, an object
r : α → α → Prop
denotes a binary
relation on
α
: given
x y : α
,
r x y
denotes the assertion
that
x
is related to
y
.
The universal quantifier,
∀ x : α, p x
is supposed to denote the
assertion that "for every
x : α
,
p x
" holds. As with the
propositional connectives, in systems of natural deduction, "forall"
is governed by an introduction and elimination rule. Informally, the
introduction rule states:
Given a proof of
p x
, in a context where
x : α
is arbitrary, we obtain a proof
∀ x : α, p x
.
The elimination rule states:
Given a proof
∀ x : α, p x
and any term
t : α
, we obtain a proof of
p t
.
As was the case for implication, the propositions-as-types
interpretation now comes into play. Remember the introduction and
elimination rules for dependent arrow types:
Given a term
t
of type
β x
, in a context where
x : α
is arbitrary, we have
(fun x : α => t) : (x : α) → β x
.
The elimination rule states:
Given a term
s : (x : α) → β x
and any term
t : α
, we have
s t : β t
.
In the case where
p x
has type
Prop
, if we replace
(x : α) → β x
with
∀ x : α, p x
, we can read these as the correct rules
for building proofs involving the universal quantifier.
The Calculus of Constructions therefore identifies dependent arrow
types with forall-expressions in this way. If
p
is any expression,
∀ x : α, p
is nothing more than alternative notation for
(x : α) → p
, with the idea that the former is more natural than the latter
in cases where
p
is a proposition. Typically, the expression
p
will depend on
x : α
. Recall that, in the case of ordinary
function spaces, we could interpret
α → β
as the special case of
(x : α) → β
in which
β
does not depend on
x
. Similarly, we
can think of an implication
p → q
between propositions as the
special case of
∀ x : p, q
in which the expression
q
does not
depend on
x
.
Here is an example of how the propositions-as-types correspondence gets put into practice.
example (α : Type) (p q : α → Prop) : (∀ x : α, p x ∧ q x) → ∀ y : α, p y :=
  fun h : ∀ x : α, p x ∧ q x =>
  fun y : α =>
  show p y from (h y).left
As a notational convention, we give the universal quantifier the
widest scope possible, so parentheses are needed to limit the
quantifier over
x
to the hypothesis in the example above. The
canonical way to prove
∀ y : α, p y
is to take an arbitrary
y
,
and prove
p y
. This is the introduction rule. Now, given that
h
has type
∀ x : α, p x ∧ q x
, the expression
h y
has type
p y ∧ q y
. This is the elimination rule. Taking the left conjunct
gives the desired conclusion,
p y
.
Remember that expressions which differ up to renaming of bound
variables are considered to be equivalent. So, for example, we could
have used the same variable,
x
, in both the hypothesis and
conclusion, and instantiated it by a different variable,
z
, in the
proof:
example (α : Type) (p q : α → Prop) : (∀ x : α, p x ∧ q x) → ∀ x : α, p x :=
  fun h : ∀ x : α, p x ∧ q x =>
  fun z : α =>
  show p z from And.left (h z)
As another example, here is how we can express the fact that a relation,
r
, is transitive:
variable (α : Type) (r : α → α → Prop)
variable (trans_r : ∀ x y z, r x y → r y z → r x z)

variable (a b c : α)
variable (hab : r a b) (hbc : r b c)

#check trans_r    -- ∀ (x y z : α), r x y → r y z → r x z
#check trans_r a b c -- r a b → r b c → r a c
#check trans_r a b c hab -- r b c → r a c
#check trans_r a b c hab hbc -- r a c
Think about what is going on here. When we instantiate
trans_r
at
the values
a b c
, we end up with a proof of
r a b → r b c → r a c
.
Applying this to the "hypothesis"
hab : r a b
, we get a proof
of the implication
r b c → r a c
. Finally, applying it to the
hypothesis
hbc
yields a proof of the conclusion
r a c
.
In situations like this, it can be tedious to supply the arguments
a b c
, when they can be inferred from
hab hbc
. For that reason, it
is common to make these arguments implicit:
variable (α : Type) (r : α → α → Prop)
variable (trans_r : ∀ {x y z}, r x y → r y z → r x z)

variable (a b c : α)
variable (hab : r a b) (hbc : r b c)

#check trans_r
#check trans_r hab
#check trans_r hab hbc
The advantage is that we can simply write
trans_r hab hbc
as a
proof of
r a c
. A disadvantage is that Lean does not have enough
information to infer the types of the arguments in the expressions
trans_r
and
trans_r hab
. The output of the first
#check
command is
r ?m.1 ?m.2 → r ?m.2 ?m.3 → r ?m.1 ?m.3
, indicating
that the implicit arguments are unspecified in this case.
Here is an example of how we can carry out elementary reasoning with an equivalence relation:
variable (α : Type) (r : α → α → Prop)

variable (refl_r : ∀ x, r x x)
variable (symm_r : ∀ {x y}, r x y → r y x)
variable (trans_r : ∀ {x y z}, r x y → r y z → r x z)

example (a b c d : α) (hab : r a b) (hcb : r c b) (hcd : r c d) : r a d :=
  trans_r (trans_r hab (symm_r hcb)) hcd
To get used to using universal quantifiers, you should try some of the
exercises at the end of this section.
It is the typing rule for dependent arrow types, and the universal
quantifier in particular, that distinguishes
Prop
from other
types.  Suppose we have
α : Sort i
and
β : Sort j
, where the
expression
β
may depend on a variable
x : α
. Then
(x : α) → β
is an element of
Sort (imax i j)
, where
imax i j
is the
maximum of
i
and
j
if
j
is not 0, and 0 otherwise.
The idea is as follows. If
j
is not
0
, then
(x : α) → β
is
an element of
Sort (max i j)
. In other words, the type of
dependent functions from
α
to
β
"lives" in the universe whose
index is the maximum of
i
and
j
. Suppose, however, that
β
is of
Sort 0
, that is, an element of
Prop
. In that case,
(x : α) → β
is an element of
Sort 0
as well, no matter which
type universe
α
lives in. In other words, if
β
is a
proposition depending on
α
, then
∀ x : α, β
is again a
proposition. This reflects the interpretation of
Prop
as the type
of propositions rather than data, and it is what makes
Prop
impredicative
.
The term "predicative" stems from foundational developments around the
turn of the twentieth century, when logicians such as Poincaré and
Russell blamed set-theoretic paradoxes on the "vicious circles" that
arise when we define a property by quantifying over a collection that
includes the very property being defined. Notice that if
α
is any
type, we can form the type
α → Prop
of all predicates on
α
(the "power type of
α
"). The impredicativity of
Prop
means that we
can form propositions that quantify over
α → Prop
. In particular,
we can define predicates on
α
by quantifying over all predicates
on
α
, which is exactly the type of circularity that was once
considered problematic.
Equality
Let us now turn to one of the most fundamental relations defined in
Lean's library, namely, the equality relation. In
Chapter Inductive Types
,
we will explain
how
equality is defined from the primitives of Lean's logical framework.
In the meanwhile, here we explain how to use it.
Of course, a fundamental property of equality is that it is an equivalence relation:
#check Eq.refl    -- Eq.refl.{u_1} {α : Sort u_1} (a : α) : a = a
#check Eq.symm    -- Eq.symm.{u} {α : Sort u} {a b : α} (h : a = b) : b = a
#check Eq.trans   -- Eq.trans.{u} {α : Sort u} {a b c : α} (h₁ : a = b) (h₂ : b = c) : a = c
We can make the output easier to read by telling Lean not to insert
the implicit arguments (which are displayed here as metavariables).
universe u

#check @Eq.refl.{u}   -- @Eq.refl : ∀ {α : Sort u} (a : α), a = a
#check @Eq.symm.{u}   -- @Eq.symm : ∀ {α : Sort u} {a b : α}, a = b → b = a
#check @Eq.trans.{u}  -- @Eq.trans : ∀ {α : Sort u} {a b c : α}, a = b → b = c → a = c
The inscription
.{u}
tells Lean to instantiate the constants at the universe
u
.
Thus, for example, we can specialize the example from the previous section to the equality relation:
variable (α : Type) (a b c d : α)
variable (hab : a = b) (hcb : c = b) (hcd : c = d)

example : a = d :=
  Eq.trans (Eq.trans hab (Eq.symm hcb)) hcd
We can also use the projection notation:
variable (α : Type) (a b c d : α)
variable (hab : a = b) (hcb : c = b) (hcd : c = d)
example : a = d := (hab.trans hcb.symm).trans hcd
Reflexivity is more powerful than it looks. Recall that terms in the
Calculus of Constructions have a computational interpretation, and
that the logical framework treats terms with a common reduct as the
same. As a result, some nontrivial identities can be proved by
reflexivity:
variable (α β : Type)

example (f : α → β) (a : α) : (fun x => f x) a = f a := Eq.refl _
example (a : α) (b : β) : (a, b).1 = a := Eq.refl _
example : 2 + 3 = 5 := Eq.refl _
This feature of the framework is so important that the library defines a notation
rfl
for
Eq.refl _
:
variable (α β : Type)
example (f : α → β) (a : α) : (fun x => f x) a = f a := rfl
example (a : α) (b : β) : (a, b).1 = a := rfl
example : 2 + 3 = 5 := rfl
Equality is much more than an equivalence relation, however. It has
the important property that every assertion respects the equivalence,
in the sense that we can substitute equal expressions without changing
the truth value. That is, given
h1 : a = b
and
h2 : p a
, we
can construct a proof for
p b
using substitution:
Eq.subst h1 h2
.
example (α : Type) (a b : α) (p : α → Prop)
        (h1 : a = b) (h2 : p a) : p b :=
  Eq.subst h1 h2

example (α : Type) (a b : α) (p : α → Prop)
    (h1 : a = b) (h2 : p a) : p b :=
  h1 ▸ h2
The triangle in the second presentation is a macro built on top of
Eq.subst
and
Eq.symm
, and you can enter it by typing
\t
.
The rule
Eq.subst
is used to define the following auxiliary rules,
which carry out more explicit substitutions. They are designed to deal
with applicative terms, that is, terms of form
s t
. Specifically,
congrArg
can be used to replace the argument,
congrFun
can be
used to replace the term that is being applied, and
congr
can be
used to replace both at once.
variable (α : Type)
variable (a b : α)
variable (f g : α → Nat)
variable (h₁ : a = b)
variable (h₂ : f = g)

example : f a = f b := congrArg f h₁
example : f a = g a := congrFun h₂ a
example : f a = g b := congr h₂ h₁
Lean's library contains a large number of common identities, such as these:
variable (a b c : Nat)

example : a + 0 = a := Nat.add_zero a
example : 0 + a = a := Nat.zero_add a
example : a * 1 = a := Nat.mul_one a
example : 1 * a = a := Nat.one_mul a
example : a + b = b + a := Nat.add_comm a b
example : a + b + c = a + (b + c) := Nat.add_assoc a b c
example : a * b = b * a := Nat.mul_comm a b
example : a * b * c = a * (b * c) := Nat.mul_assoc a b c
example : a * (b + c) = a * b + a * c := Nat.mul_add a b c
example : a * (b + c) = a * b + a * c := Nat.left_distrib a b c
example : (a + b) * c = a * c + b * c := Nat.add_mul a b c
example : (a + b) * c = a * c + b * c := Nat.right_distrib a b c
Note that
Nat.mul_add
and
Nat.add_mul
are alternative names
for
Nat.left_distrib
and
Nat.right_distrib
, respectively.  The
properties above are stated for the natural numbers (type
Nat
).
Here is an example of a calculation in the natural numbers that uses
substitution combined with associativity and distributivity.
example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  have h1 : (x + y) * (x + y) = (x + y) * x + (x + y) * y :=
    Nat.mul_add (x + y) x y
  have h2 : (x + y) * (x + y) = x * x + y * x + (x * y + y * y) :=
    (Nat.add_mul x y x) ▸ (Nat.add_mul x y y) ▸ h1
  h2.trans (Nat.add_assoc (x * x + y * x) (x * y) (y * y)).symm
Notice that the second implicit parameter to
Eq.subst
, which
provides the context in which the substitution is to occur, has type
α → Prop
.  Inferring this predicate therefore requires an instance
of
higher-order unification
. In full generality, the problem of
determining whether a higher-order unifier exists is undecidable, and
Lean can at best provide imperfect and approximate solutions to the
problem. As a result,
Eq.subst
doesn't always do what you want it
to.  The macro
h ▸ e
uses more effective heuristics for computing
this implicit parameter, and often succeeds in situations where
applying
Eq.subst
fails.
Because equational reasoning is so common and important, Lean provides
a number of mechanisms to carry it out more effectively. The next
section offers syntax that allow you to write calculational proofs in
a more natural and perspicuous way. But, more importantly, equational
reasoning is supported by a term rewriter, a simplifier, and other
kinds of automation. The term rewriter and simplifier are described
briefly in the next section, and then in greater detail in the next
chapter.
Calculational Proofs
A calculational proof is just a chain of intermediate results that are
meant to be composed by basic principles such as the transitivity of
equality. In Lean, a calculational proof starts with the keyword
calc
, and has the following syntax:
calc
  <expr>_0  'op_1'  <expr>_1  ':='  <proof>_1
  '_'       'op_2'  <expr>_2  ':='  <proof>_2
  ...
  '_'       'op_n'  <expr>_n  ':='  <proof>_n
Note that the
calc
relations all have the same indentation. Each
<proof>_i
is a proof for
<expr>_{i-1} op_i <expr>_i
.
We can also use
_
in the first relation (right after
<expr>_0
)
which is useful to align the sequence of relation/proof pairs:
calc <expr>_0 
    '_' 'op_1' <expr>_1 ':=' <proof>_1
    '_' 'op_2' <expr>_2 ':=' <proof>_2
    ...
    '_' 'op_n' <expr>_n ':=' <proof>_n
Here is an example:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)

theorem T : a = e :=
  calc
    a = b      := h1
    _ = c + 1  := h2
    _ = d + 1  := congrArg Nat.succ h3
    _ = 1 + d  := Nat.add_comm d 1
    _ = e      := Eq.symm h4
This style of writing proofs is most effective when it is used in
conjunction with the
simp
and
rewrite
tactics, which are
discussed in greater detail in the next chapter. For example, using
the abbreviation
rw
for rewrite, the proof above could be written
as follows:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)
theorem T : a = e :=
  calc
    a = b      := by rw [h1]
    _ = c + 1  := by rw [h2]
    _ = d + 1  := by rw [h3]
    _ = 1 + d  := by rw [Nat.add_comm]
    _ = e      := by rw [h4]
Essentially, the
rw
tactic uses a given equality (which can be a
hypothesis, a theorem name, or a complex term) to "rewrite" the
goal. If doing so reduces the goal to an identity
t = t
, the
tactic applies reflexivity to prove it.
Rewrites can be applied sequentially, so that the proof above can be
shortened to this:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)
theorem T : a = e :=
  calc
    a = d + 1  := by rw [h1, h2, h3]
    _ = 1 + d  := by rw [Nat.add_comm]
    _ = e      := by rw [h4]
Or even this:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)
theorem T : a = e :=
  by rw [h1, h2, h3, Nat.add_comm, h4]
The
simp
tactic, instead, rewrites the goal by applying the given
identities repeatedly, in any order, anywhere they are applicable in a
term. It also uses other rules that have been previously declared to
the system, and applies commutativity wisely to avoid looping. As a
result, we can also prove the theorem as follows:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)
theorem T : a = e :=
  by simp [h1, h2, h3, Nat.add_comm, h4]
We will discuss variations of
rw
and
simp
in the next chapter.
The
calc
command can be configured for any relation that supports
some form of transitivity. It can even combine different relations.
example (a b c d : Nat) (h1 : a = b) (h2 : b ≤ c) (h3 : c + 1 < d) : a < d :=
  calc
    a = b     := h1
    _ < b + 1 := Nat.lt_succ_self b
    _ ≤ c + 1 := Nat.succ_le_succ h2
    _ < d     := h3
You can "teach"
calc
new transitivity theorems by adding new instances
of the
Trans
type class. Type classes are introduced later, but the following
small example demonstrates how to extend the
calc
notation using new
Trans
instances.
def divides (x y : Nat) : Prop :=
  ∃ k, k*x = y

def divides_trans (h₁ : divides x y) (h₂ : divides y z) : divides x z :=
  let ⟨k₁, d₁⟩ := h₁
  let ⟨k₂, d₂⟩ := h₂
  ⟨k₁ * k₂, by rw [Nat.mul_comm k₁ k₂, Nat.mul_assoc, d₁, d₂]⟩

def divides_mul (x : Nat) (k : Nat) : divides x (k*x) :=
  ⟨k, rfl⟩

instance : Trans divides divides divides where
  trans := divides_trans

example (h₁ : divides x y) (h₂ : y = z) : divides x (2*z) :=
  calc
    divides x y     := h₁
    _ = z           := h₂
    divides _ (2*z) := divides_mul ..

infix:50 " ∣ " => divides

example (h₁ : divides x y) (h₂ : y = z) : divides x (2*z) :=
  calc
    x ∣ y   := h₁
    _ = z   := h₂
    _ ∣ 2*z := divides_mul ..
The example above also makes it clear that you can use
calc
even if you
do not have an infix notation for your relation. Finally we remark that
the vertical bar
∣
in the example above is the unicode one. We use
unicode to make sure we do not overload the ASCII
|
used in the
match .. with
expression.
With
calc
, we can write the proof in the last section in a more
natural and perspicuous way.
example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  calc
    (x + y) * (x + y) = (x + y) * x + (x + y) * y  := by rw [Nat.mul_add]
    _ = x * x + y * x + (x + y) * y                := by rw [Nat.add_mul]
    _ = x * x + y * x + (x * y + y * y)            := by rw [Nat.add_mul]
    _ = x * x + y * x + x * y + y * y              := by rw [←Nat.add_assoc]
The alternative
calc
notation is worth considering here. When the
first expression is taking this much space, using
_
in the first
relation naturally aligns all relations:
example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  calc (x + y) * (x + y)
    _ = (x + y) * x + (x + y) * y       := by rw [Nat.mul_add]
    _ = x * x + y * x + (x + y) * y     := by rw [Nat.add_mul]
    _ = x * x + y * x + (x * y + y * y) := by rw [Nat.add_mul]
    _ = x * x + y * x + x * y + y * y   := by rw [←Nat.add_assoc]
Here the left arrow before
Nat.add_assoc
tells rewrite to use the
identity in the opposite direction. (You can enter it with
\l
or
use the ascii equivalent,
<-
.) If brevity is what we are after,
both
rw
and
simp
can do the job on their own:
example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  by rw [Nat.mul_add, Nat.add_mul, Nat.add_mul, ←Nat.add_assoc]

example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  by simp [Nat.mul_add, Nat.add_mul, Nat.add_assoc]
The Existential Quantifier
Finally, consider the existential quantifier, which can be written as
either
exists x : α, p x
or
∃ x : α, p x
.  Both versions are
actually notationally convenient abbreviations for a more long-winded
expression,
Exists (fun x : α => p x)
, defined in Lean's library.
As you should by now expect, the library includes both an introduction
rule and an elimination rule. The introduction rule is
straightforward: to prove
∃ x : α, p x
, it suffices to provide a
suitable term
t
and a proof of
p t
. Here are some examples:
example : ∃ x : Nat, x > 0 :=
  have h : 1 > 0 := Nat.zero_lt_succ 0
  Exists.intro 1 h

example (x : Nat) (h : x > 0) : ∃ y, y < x :=
  Exists.intro 0 h

example (x y z : Nat) (hxy : x < y) (hyz : y < z) : ∃ w, x < w ∧ w < z :=
  Exists.intro y (And.intro hxy hyz)

#check @Exists.intro -- ∀ {α : Sort u_1} {p : α → Prop} (w : α), p w → Exists p
We can use the anonymous constructor notation
⟨t, h⟩
for
Exists.intro t h
, when the type is clear from the context.
example : ∃ x : Nat, x > 0 :=
  have h : 1 > 0 := Nat.zero_lt_succ 0
  ⟨1, h⟩

example (x : Nat) (h : x > 0) : ∃ y, y < x :=
  ⟨0, h⟩

example (x y z : Nat) (hxy : x < y) (hyz : y < z) : ∃ w, x < w ∧ w < z :=
  ⟨y, hxy, hyz⟩
Note that
Exists.intro
has implicit arguments: Lean has to infer
the predicate
p : α → Prop
in the conclusion
∃ x, p x
.  This
is not a trivial affair. For example, if we have
hg : g 0 0 = 0
and write
Exists.intro 0 hg
, there are many possible values
for the predicate
p
, corresponding to the theorems
∃ x, g x x = x
,
∃ x, g x x = 0
,
∃ x, g x 0 = x
, etc. Lean uses the
context to infer which one is appropriate. This is illustrated in the
following example, in which we set the option
pp.explicit
to true
to ask Lean's pretty-printer to show the implicit arguments.
variable (g : Nat → Nat → Nat)
variable (hg : g 0 0 = 0)

theorem gex1 : ∃ x, g x x = x := ⟨0, hg⟩
theorem gex2 : ∃ x, g x 0 = x := ⟨0, hg⟩
theorem gex3 : ∃ x, g 0 0 = x := ⟨0, hg⟩
theorem gex4 : ∃ x, g x x = 0 := ⟨0, hg⟩

set_option pp.explicit true  -- display implicit arguments
#print gex1
#print gex2
#print gex3
#print gex4
We can view
Exists.intro
as an information-hiding operation, since
it hides the witness to the body of the assertion. The existential
elimination rule,
Exists.elim
, performs the opposite operation. It
allows us to prove a proposition
q
from
∃ x : α, p x
, by
showing that
q
follows from
p w
for an arbitrary value
w
. Roughly speaking, since we know there is an
x
satisfying
p x
, we can give it a name, say,
w
. If
q
does not mention
w
, then showing that
q
follows from
p w
is tantamount to
showing that
q
follows from the existence of any such
x
. Here
is an example:
variable (α : Type) (p q : α → Prop)

example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  Exists.elim h
    (fun w =>
     fun hw : p w ∧ q w =>
     show ∃ x, q x ∧ p x from ⟨w, hw.right, hw.left⟩)
It may be helpful to compare the exists-elimination rule to the
or-elimination rule: the assertion
∃ x : α, p x
can be thought of
as a big disjunction of the propositions
p a
, as
a
ranges over
all the elements of
α
. Note that the anonymous constructor
notation
⟨w, hw.right, hw.left⟩
abbreviates a nested constructor
application; we could equally well have written
⟨w, ⟨hw.right, hw.left⟩⟩
.
Notice that an existential proposition is very similar to a sigma
type, as described in dependent types section.  The difference is that
given
a : α
and
h : p a
, the term
Exists.intro a h
has
type
(∃ x : α, p x) : Prop
and
Sigma.mk a h
has type
(Σ x : α, p x) : Type
. The similarity between
∃
and
Σ
is another
instance of the Curry-Howard isomorphism.
Lean provides a more convenient way to eliminate from an existential
quantifier with the
match
expression:
variable (α : Type) (p q : α → Prop)

example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  match h with
  | ⟨w, hw⟩ => ⟨w, hw.right, hw.left⟩
The
match
expression is part of Lean's function definition system,
which provides convenient and expressive ways of defining complex
functions.  Once again, it is the Curry-Howard isomorphism that allows
us to co-opt this mechanism for writing proofs as well.  The
match
statement "destructs" the existential assertion into the components
w
and
hw
, which can then be used in the body of the statement
to prove the proposition. We can annotate the types used in the match
for greater clarity:
variable (α : Type) (p q : α → Prop)
example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  match h with
  | ⟨(w : α), (hw : p w ∧ q w)⟩ => ⟨w, hw.right, hw.left⟩
We can even use the match statement to decompose the conjunction at the same time:
variable (α : Type) (p q : α → Prop)
example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  match h with
  | ⟨w, hpw, hqw⟩ => ⟨w, hqw, hpw⟩
Lean also provides a pattern-matching
let
expression:
variable (α : Type) (p q : α → Prop)
example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  let ⟨w, hpw, hqw⟩ := h
  ⟨w, hqw, hpw⟩
This is essentially just alternative notation for the
match
construct above. Lean will even allow us to use an implicit
match
in the
fun
expression:
variable (α : Type) (p q : α → Prop)
example : (∃ x, p x ∧ q x) → ∃ x, q x ∧ p x :=
  fun ⟨w, hpw, hqw⟩ => ⟨w, hqw, hpw⟩
We will see in
Chapter Induction and Recursion
that all these variations are
instances of a more general pattern-matching construct.
In the following example, we define
is_even a
as
∃ b, a = 2 * b
,
and then we show that the sum of two even numbers is an even number.
def is_even (a : Nat) := ∃ b, a = 2 * b


3. Logic — Mathematics in Lean 0.1 documentation
Mathematics in Lean
1. Introduction
2. Basics
3. Logic
3.1. Implication and the Universal Quantifier
3.2. The Existential Quantifier
3.3. Negation
3.4. Conjunction and Iff
3.5. Disjunction
3.6. Sequences and Convergence
4. Sets and Functions
5. Elementary Number Theory
6. Structures
7. Hierarchies
8. Groups and Rings
9. Linear algebra
10. Topology
11. Differential Calculus
12. Integration and Measure Theory
Index
Mathematics in Lean
3.
Logic
View page source
3.
Logic

In the last chapter, we dealt with equations, inequalities,
and basic mathematical statements like
“
\(x\)
divides
\(y\)
.”
Complex mathematical statements are built up from
simple ones like these
using logical terms like “and,” “or,” “not,” and
“if … then,” “every,” and “some.”
In this chapter, we show you how to work with statements
that are built up in this way.
3.1.
Implication and the Universal Quantifier

Consider the statement after the
#check
:
#check
∀
x
:
ℝ
,
0
≤
x
→
|
x
|
=
x
In words, we would say “for every real number
x
, if
0
≤
x
then
the absolute value of
x
equals
x
”.
We can also have more complicated statements like:
#check
∀
x
y
ε
:
ℝ
,
0
<
ε
→
ε
≤
1
→
|
x
|
<
ε
→
|
y
|
<
ε
→
|
x
*
y
|
<
ε
In words, we would say “for every
x
,
y
, and
ε
,
if
0
<
ε
≤
1
, the absolute value of
x
is less than
ε
,
and the absolute value of
y
is less than
ε
,
then the absolute value of
x
*
y
is less than
ε
.”
In Lean, in a sequence of implications there are
implicit parentheses grouped to the right.
So the expression above means
“if
0
<
ε
then if
ε
≤
1
then if
|x|
<
ε
…”
As a result, the expression says that all the
assumptions together imply the conclusion.
You have already seen that even though the universal quantifier
in this statement
ranges over objects and the implication arrows introduce hypotheses,
Lean treats the two in very similar ways.
In particular, if you have proved a theorem of that form,
you can apply it to objects and hypotheses in the same way.
We will use as an example the following statement that we will help you to prove a
bit later:
theorem
my_lemma
:
∀
x
y
ε
:
ℝ
,
0
<
ε
→
ε
≤
1
→
|
x
|
<
ε
→
|
y
|
<
ε
→
|
x
*
y
|
<
ε
:=
sorry
section
variable
(
a
b
δ
:
ℝ
)
variable
(
h₀
:
0
<
δ
)
(
h₁
:
δ
≤
1
)
variable
(
ha
:
|
a
|
<
δ
)
(
hb
:
|
b
|
<
δ
)
#check
my_lemma
a
b
δ
#check
my_lemma
a
b
δ
h₀
h₁
#check
my_lemma
a
b
δ
h₀
h₁
ha
hb
end
You have also already seen that it is common in Lean
to use curly brackets to make quantified variables implicit
when they can be inferred from subsequent hypotheses.
When we do that, we can just apply a lemma to the hypotheses without
mentioning the objects.
theorem
my_lemma2
:
∀
{
x
y
ε
:
ℝ
},
0
<
ε
→
ε
≤
1
→
|
x
|
<
ε
→
|
y
|
<
ε
→
|
x
*
y
|
<
ε
:=
sorry
section
variable
(
a
b
δ
:
ℝ
)
variable
(
h₀
:
0
<
δ
)
(
h₁
:
δ
≤
1
)
variable
(
ha
:
|
a
|
<
δ
)
(
hb
:
|
b
|
<
δ
)
#check
my_lemma2
h₀
h₁
ha
hb
end
At this stage, you also know that if you use
the
apply
tactic to apply
my_lemma
to a goal of the form
|a
*
b|
<
δ
,
you are left with new goals that require you to prove
each of the hypotheses.
To prove a statement like this, use the
intro
tactic.
Take a look at what it does in this example:
theorem
my_lemma3
:
∀
{
x
y
ε
:
ℝ
},
0
<
ε
→
ε
≤
1
→
|
x
|
<
ε
→
|
y
|
<
ε
→
|
x
*
y
|
<
ε
:=
by
intro
x
y
ε
epos
ele1
xlt
ylt
sorry
We can use any names we want for the universally quantified variables;
they do not have to be
x
,
y
, and
ε
.
Notice that we have to introduce the variables
even though they are marked implicit:
making them implicit means that we leave them out when
we write an expression
using
my_lemma
,
but they are still an essential part of the statement
that we are proving.
After the
intro
command,
the goal is what it would have been at the start if we
listed all the variables and hypotheses
before
the colon,
as we did in the last section.
In a moment, we will see why it is sometimes necessary to
introduce variables and hypotheses after the proof begins.
To help you prove the lemma, we will start you off:
theorem
my_lemma4
:
∀
{
x
y
ε
:
ℝ
},
0
<
ε
→
ε
≤
1
→
|
x
|
<
ε
→
|
y
|
<
ε
→
|
x
*
y
|
<
ε
:=
by
intro
x
y
ε
epos
ele1
xlt
ylt
calc
|
x
*
y
|
=
|
x
|
*
|
y
|
:=
sorry
_
≤
|
x
|
*
ε
:=
sorry
_
<
1
*
ε
:=
sorry
_
=
ε
:=
sorry
Finish the proof using the theorems
abs_mul
,
mul_le_mul
,
abs_nonneg
,
mul_lt_mul_right
, and
one_mul
.
Remember that you can find theorems like these using
Ctrl-space completion (or Cmd-space completion on a Mac).
Remember also that you can use
.mp
and
.mpr
or
.1
and
.2
to extract the two directions
of an if-and-only-if statement.
Universal quantifiers are often hidden in definitions,
and Lean will unfold definitions to expose them when necessary.
For example, let’s define two predicates,
FnUb
f
a
and
FnLb
f
a
,
where
f
is a function from the real numbers to the real
numbers and
a
is a real number.
The first says that
a
is an upper bound on the
values of
f
,
and the second says that
a
is a lower bound
on the values of
f
.
def
FnUb
(
f
:
ℝ
→
ℝ
)
(
a
:
ℝ
)
:
Prop
:=
∀
x
,
f
x
≤
a
def
FnLb
(
f
:
ℝ
→
ℝ
)
(
a
:
ℝ
)
:
Prop
:=
∀
x
,
a
≤
f
x
In the next example,
fun
x
↦
f
x
+
g
x
is the
function that maps
x
to
f
x
+
g
x
. Going from the expression
f
x
+
g
x
to this function is called a lambda abstraction in type theory.
example
(
hfa
:
FnUb
f
a
)
(
hgb
:
FnUb
g
b
)
:
FnUb
(
fun
x
↦
f
x
+
g
x
)
(
a
+
b
)
:=
by
intro
x
dsimp
apply
add_le_add
apply
hfa
apply
hgb
Applying
intro
to the goal
FnUb
(fun
x
↦
f
x
+
g
x)
(a
+
b)
forces Lean to unfold the definition of
FnUb
and introduce
x
for the universal quantifier.
The goal is then
(fun
(x
:
ℝ)
↦
f
x
+
g
x)
x
≤
a
+
b
.
But applying
(fun
x
↦
f
x
+
g
x)
to
x
should result in
f
x
+
g
x
,
and the
dsimp
command performs that simplification.
(The “d” stands for “definitional.”)
You can delete that command and the proof still works;
Lean would have to perform that contraction anyhow to make
sense of the next
apply
.
The
dsimp
command simply makes the goal more readable
and helps us figure out what to do next.
Another option is to use the
change
tactic
by writing
change
f
x
+
g
x
≤
a
+
b
.
This helps make the proof more readable,
and gives you more control over how the goal is transformed.
The rest of the proof is routine.
The last two
apply
commands force Lean to unfold the definitions
of
FnUb
in the hypotheses.
Try carrying out similar proofs of these:
example
(
hfa
:
FnLb
f
a
)
(
hgb
:
FnLb
g
b
)
:
FnLb
(
fun
x
↦
f
x
+
g
x
)
(
a
+
b
)
:=
sorry
example
(
nnf
:
FnLb
f
0
)
(
nng
:
FnLb
g
0
)
:
FnLb
(
fun
x
↦
f
x
*
g
x
)
0
:=
sorry
example
(
hfa
:
FnUb
f
a
)
(
hgb
:
FnUb
g
b
)
(
nng
:
FnLb
g
0
)
(
nna
:
0
≤
a
)
:
FnUb
(
fun
x
↦
f
x
*
g
x
)
(
a
*
b
)
:=
sorry
Even though we have defined
FnUb
and
FnLb
for functions
from the reals to the reals,
you should recognize that the definitions and proofs are much
more general.
The definitions make sense for functions between any two types
for which there is a notion of order on the codomain.
Checking the type of the theorem
add_le_add
shows that it holds
of any structure that is an “ordered additive commutative monoid”;
the details of what that means don’t matter now,
but it is worth knowing that the natural numbers, integers, rationals,
and real numbers are all instances.
So if we prove the theorem
fnUb_add
at that level of generality,
it will apply in all these instances.
variable
{
α
:
Type
*
}
{
R
:
Type
*
}
[
AddCommMonoid
R
]
[
PartialOrder
R
]
[
IsOrderedCancelAddMonoid
R
]
#check
add_le_add
def
FnUb'
(
f
:
α
→
R
)
(
a
:
R
)
:
Prop
:=
∀
x
,
f
x
≤
a
theorem
fnUb_add
{
f
g
:
α
→
R
}
{
a
b
:
R
}
(
hfa
:
FnUb'
f
a
)
(
hgb
:
FnUb'
g
b
)
:
FnUb'
(
fun
x
↦
f
x
+
g
x
)
(
a
+
b
)
:=
fun
x
↦
add_le_add
(
hfa
x
)
(
hgb
x
)
You have already seen square brackets like these in
Section
Section 2.2
,
though we still haven’t explained what they mean.
For concreteness, we will stick to the real numbers
for most of our examples,
but it is worth knowing that Mathlib contains definitions and theorems
that work at a high level of generality.
For another example of a hidden universal quantifier,
Mathlib defines a predicate
Monotone
,
which says that a function is nondecreasing in its arguments:
example
(
f
:
ℝ
→
ℝ
)
(
h
:
Monotone
f
)
:
∀
{
a
b
},
a
≤
b
→
f
a
≤
f
b
:=
@
h
The property
Monotone
f
is defined to be exactly the expression
after the colon. We need to put the
@
symbol before
h
because
if we don’t,
Lean expands the implicit arguments to
h
and inserts placeholders.
Proving statements about monotonicity
involves using
intro
to introduce two variables,
say,
a
and
b
, and the hypothesis
a
≤
b
.
To
use
a monotonicity hypothesis,
you can apply it to suitable arguments and hypotheses,
and then apply the resulting expression to the goal.
Or you can apply it to the goal and let Lean help you
work backwards by displaying the remaining hypotheses
as new subgoals.
example
(
mf
:
Monotone
f
)
(
mg
:
Monotone
g
)
:
Monotone
fun
x
↦
f
x
+
g
x
:=
by
intro
a
b
aleb
apply
add_le_add
apply
mf
aleb
apply
mg
aleb
When a proof is this short, it is often convenient
to give a proof term instead.
To describe a proof that temporarily introduces objects
a
and
b
and a hypothesis
aleb
,
Lean uses the notation
fun
a
b
aleb
↦
...
.
This is analogous to the way that an expression
like
fun
x
↦
x^2
describes a function
by temporarily naming an object,
x
,
and then using it to describe a value.
So the
intro
command in the previous proof
corresponds to the lambda abstraction in the next proof term.
The
apply
commands then correspond to building
the application of the theorem to its arguments.
example
(
mf
:
Monotone
f
)
(
mg
:
Monotone
g
)
:
Monotone
fun
x
↦
f
x
+
g
x
:=
fun
a
b
aleb
↦
add_le_add
(
mf
aleb
)
(
mg
aleb
)
Here is a useful trick: if you start writing
the proof term
fun
a
b
aleb
↦
_
using
an underscore where the rest of the
expression should go,
Lean will flag an error,
indicating that it can’t guess the value of that expression.
If you check the Lean Goal window in VS Code or
hover over the squiggly error marker,
Lean will show you the goal that the remaining
expression has to solve.
Try proving these, with either tactics or proof terms:
example
{
c
:
ℝ
}
(
mf
:
Monotone
f
)
(
nnc
:
0
≤
c
)
:
Monotone
fun
x
↦
c
*
f
x
:=
sorry
example
(
mf
:
Monotone
f
)
(
mg
:
Monotone
g
)
:
Monotone
fun
x
↦
f
(
g
x
)
:=
sorry
Here are some more examples.
A function
\(f\)
from
\(\Bbb R\)
to
\(\Bbb R\)
is said to be
even
if
\(f(-x) = f(x)\)
for every
\(x\)
,
and
odd
if
\(f(-x) = -f(x)\)
for every
\(x\)
.
The following example defines these two notions formally
and establishes one fact about them.
You can complete the proofs of the others.
def
FnEven
(
f
:
ℝ
→
ℝ
)
:
Prop
:=
∀
x
,
f
x
=
f
(
-
x
)
def
FnOdd
(
f
:
ℝ
→
ℝ
)
:
Prop
:=
∀
x
,
f
x
=
-
f
(
-
x
)
example
(
ef
:
FnEven
f
)
(
eg
:
FnEven
g
)
:
FnEven
fun
x
↦
f
x
+
g
x
:=
by
intro
x
calc
(
fun
x
↦
f
x
+
g
x
)
x
=
f
x
+
g
x
:=
rfl
_
=
f
(
-
x
)
+
g
(
-
x
)
:=
by
rw
[
ef
,
eg
]
example
(
of
:
FnOdd
f
)
(
og
:
FnOdd
g
)
:
FnEven
fun
x
↦
f
x
*
g
x
:=
by
sorry
example
(
ef
:
FnEven
f
)
(
og
:
FnOdd
g
)
:
FnOdd
fun
x
↦
f
x
*
g
x
:=
by
sorry
example
(
ef
:
FnEven
f
)
(
og
:
FnOdd
g
)
:
FnEven
fun
x
↦
f
(
g
x
)
:=
by
sorry
The first proof can be shortened using
dsimp
or
change
to get rid of the lambda abstraction.
But you can check that the subsequent
rw
won’t work
unless we get rid of the lambda abstraction explicitly,
because otherwise it cannot find the patterns
f
x
and
g
x
in the expression.
Contrary to some other tactics,
rw
operates on the syntactic level,
it won’t unfold definitions or apply reductions for you
(it has a variant called
erw
that tries a little harder in this
direction, but not much harder).
You can find implicit universal quantifiers all over the place,
once you know how to spot them.
Mathlib includes a good library for manipulating sets. Recall that Lean does not
use foundations based on set theory, so here the word set has its mundane meaning
of a collection of mathematical objects of some given type
α
.
If
x
has type
α
and
s
has type
Set
α
, then
x
∈
s
is a proposition
that asserts that
x
is an element of
s
. If
y
has some different type
β
then the
expression
y
∈
s
makes no sense. Here “makes no sense” means “has no type hence Lean does not
accept it as a well-formed statement”. This contrasts with Zermelo-Fraenkel set theory for instance
where
a
∈
b
is a well-formed statement for every mathematical objects
a
and
b
.
For instance
sin
∈
cos
is a well-formed statement in ZF. This defect of set theoretic
foundations is an important motivation for not using it in a proof assistant which is meant to assist
us by detecting meaningless expressions. In Lean
sin
has type
ℝ
→
ℝ
and
cos
has type
ℝ
→
ℝ
which is not equal to
Set
(ℝ
→
ℝ)
, even after unfolding definitions, so the statement
sin
∈
cos
makes no sense.
One can also use Lean to work on set theory itself. For instance the independence of the continuum
hypothesis from the axioms of Zermelo-Fraenkel has been formalized in Lean. But such a meta-theory
of set theory is completely beyond the scope of this book.
If
s
and
t
are of type
Set
α
,
then the subset relation
s
⊆
t
is defined to mean
∀
{x
:
α},
x
∈
s
→
x
∈
t
.
The variable in the quantifier is marked implicit so that
given
h
:
s
⊆
t
and
h'
:
x
∈
s
,
we can write
h
h'
as justification for
x
∈
t
.
The following example provides a tactic proof and a proof term
justifying the reflexivity of the subset relation,
and asks you to do the same for transitivity.
variable
{
α
:
Type
*
}
(
r
s
t
:
Set
α
)
example
:
s
⊆
s
:=
by
intro
x
xs
exact
xs
theorem
Subset.refl
:
s
⊆
s
:=
fun
x
xs
↦
xs
theorem
Subset.trans
:
r
⊆
s
→
s
⊆
t
→
r
⊆
t
:=
by
sorry
Just as we defined
FnUb
for functions,
we can define
SetUb
s
a
to mean that
a
is an upper bound on the set
s
,
assuming
s
is a set of elements of some type that
has an order associated with it.
In the next example, we ask you to prove that
if
a
is a bound on
s
and
a
≤
b
,
then
b
is a bound on
s
as well.
variable
{
α
:
Type
*
}
[
PartialOrder
α
]
variable
(
s
:
Set
α
)
(
a
b
:
α
)
def
SetUb
(
s
:
Set
α
)
(
a
:
α
)
:=
∀
x
,
x
∈
s
→
x
≤
a
example
(
h
:
SetUb
s
a
)
(
h'
:
a
≤
b
)
:
SetUb
s
b
:=
sorry
We close this section with one last important example.
A function
\(f\)
is said to be
injective
if for
every
\(x_1\)
and
\(x_2\)
,
if
\(f(x_1) = f(x_2)\)
then
\(x_1 = x_2\)
.
Mathlib defines
Function.Injective
f
with
x₁
and
x₂
implicit.
The next example shows that, on the real numbers,
any function that adds a constant is injective.
We then ask you to show that multiplication by a nonzero
constant is also injective, using the lemma name in the example as a source
of inspiration. Recall you should use Ctrl-space completion after guessing the beginning of
a lemma name.
open
Function
example
(
c
:
ℝ
)
:
Injective
fun
x
↦
x
+
c
:=
by
intro
x₁
x₂
h'
exact
(
add_left_inj
c
)
.
mp
h'
example
{
c
:
ℝ
}
(
h
:
c
≠
0
)
:
Injective
fun
x
↦
c
*
x
:=
by
sorry
Finally, show that the composition of two injective functions is injective:
variable
{
α
:
Type
*
}
{
β
:
Type
*
}
{
γ
:
Type
*
}
variable
{
g
:
β
→
γ
}
{
f
:
α
→
β
}
example
(
injg
:
Injective
g
)
(
injf
:
Injective
f
)
:
Injective
fun
x
↦
g
(
f
x
)
:=
by
sorry
3.2.
The Existential Quantifier

The existential quantifier, which can be entered as
\ex
in VS Code,
is used to represent the phrase “there exists.”
The formal expression
∃
x
:
ℝ,
2
<
x
∧
x
<
3
in Lean says
that there is a real number between 2 and 3.
(We will discuss the conjunction symbol,
∧
, in
Section 3.4
.)
The canonical way to prove such a statement is to exhibit a real number
and show that it has the stated property.
The number 2.5, which we can enter as
5
/
2
or
(5
:
ℝ)
/
2
when Lean cannot infer from context that we have
the real numbers in mind, has the required property,
and the
norm_num
tactic can prove that it meets the description.
There are a few ways we can put the information together.
Given a goal that begins with an existential quantifier,
the
use
tactic is used to provide the object,
leaving the goal of proving the property.
example
:
∃
x
:
ℝ
,
2
<
x
∧
x
<
3
:=
by
use
5
/
2
norm_num
You can give the
use
tactic proofs as well as data:
example
:
∃
x
:
ℝ
,
2
<
x
∧
x
<
3
:=
by
have
h1
:
2
<
(
5
:
ℝ
)
/
2
:=
by
norm_num
have
h2
:
(
5
:
ℝ
)
/
2
<
3
:=
by
norm_num
use
5
/
2
,
h1
,
h2
In fact, the
use
tactic automatically tries to use available assumptions as well.
example
:
∃
x
:
ℝ
,
2
<
x
∧
x
<
3
:=
by
have
h
:
2
<
(
5
:
ℝ
)
/
2
∧
(
5
:
ℝ
)
/
2
<
3
:=
by
norm_num
use
5
/
2
Alternatively, we can use Lean’s
anonymous constructor
notation
to construct a proof of an existential quantifier.
example
:
∃
x
:
ℝ
,
2
<
x
∧
x
<
3
:=
have
h
:
2
<
(
5
:
ℝ
)
/
2
∧
(
5
:
ℝ
)
/
2
<
3
:=
by
norm_num
⟨
5
/
2
,
h
⟩
Notice that there is no
by
; here we are giving an explicit proof term.
The left and right angle brackets,
which can be entered as
\<
and
\>
respectively,
tell Lean to put together the given data using
whatever construction is appropriate
for the current goal.
We can use the notation without going first into tactic mode:
example
:
∃
x
:
ℝ
,
2
<
x
∧
x
<
3
:=
⟨
5
/
2
,
by
norm_num
⟩
So now we know how to
prove
an exists statement.
But how do we
use
one?
If we know that there exists an object with a certain property,
we should be able to give a name to an arbitrary one
and reason about it.
For example, remember the predicates
FnUb
f
a
and
FnLb
f
a
from the last section,
which say that
a
is an upper bound or lower bound on
f
,
respectively.
We can use the existential quantifier to say that “
f
is bounded”
without specifying the bound:
def
FnUb
(
f
:
ℝ
→
ℝ
)
(
a
:
ℝ
)
:
Prop
:=
∀
x
,
f
x
≤
a
def
FnLb
(
f
:
ℝ
→
ℝ
)
(
a
:
ℝ
)
:
Prop
:=
∀
x
,
a
≤
f
x
def
FnHasUb
(
f
:
ℝ
→
ℝ
)
:=
∃
a
,
FnUb
f
a
def
FnHasLb
(
f
:
ℝ
→
ℝ
)
:=
∃
a
,
FnLb
f
a
We can use the theorem
FnUb_add
from the last section
to prove that if
f
and
g
have upper bounds,
then so does
fun
x
↦
f
x
+
g
x
.
variable
{
f
g
:
ℝ
→
ℝ
}
example
(
ubf
:
FnHasUb
f
)
(
ubg
:
FnHasUb
g
)
:
FnHasUb
fun
x
↦
f
x
+
g
x
:=
by
rcases
ubf
with
⟨
a
,
ubfa
⟩
rcases
ubg
with
⟨
b
,
ubgb
⟩
use
a
+
b
apply
fnUb_add
ubfa
ubgb
The
rcases
tactic unpacks the information
in the existential quantifier.
The annotations like
⟨a,
ubfa⟩
, written with the
same angle brackets as the anonymous constructors,
are known as
patterns
, and they describe the information
that we expect to find when we unpack the main argument.
Given the hypothesis
ubf
that there is an upper bound
for
f
,
rcases
ubf
with
⟨a,
ubfa⟩
adds a new variable
a
for an upper bound to the context,
together with the hypothesis
ubfa
that it has the given property.
The goal is left unchanged;
what
has
changed is that we can now use
the new object and the new hypothesis
to prove the goal.
This is a common method of reasoning in mathematics:
we unpack objects whose existence is asserted or implied
by some hypothesis, and then use it to establish the existence
of something else.
Try using this method to establish the following.
You might find it useful to turn some of the examples
from the last section into named theorems,
as we did with
fn_ub_add
,
or you can insert the arguments directly
into the proofs.
example
(
lbf
:
FnHasLb
f
)
(
lbg
:
FnHasLb
g
)
:
FnHasLb
fun
x
↦
f
x
+
g
x
:=
by
sorry
example
{
c
:
ℝ
}
(
ubf
:
FnHasUb
f
)
(
h
:
c
≥
0
)
:
FnHasUb
fun
x
↦
c
*
f
x
:=
by
sorry
The “r” in
rcases
stands for “recursive,” because it allows
us to use arbitrarily complex patterns to unpack nested data.
The
rintro
tactic
is a combination of
intro
and
rcases
:
example
:
FnHasUb
f
→
FnHasUb
g
→
FnHasUb
fun
x
↦
f
x
+
g
x
:=
by
rintro
⟨
a
,
ubfa
⟩
⟨
b
,
ubgb
⟩
exact
⟨
a
+
b
,
fnUb_add
ubfa
ubgb
⟩
In fact, Lean also supports a pattern-matching fun
in expressions and proof terms:
example
:
FnHasUb
f
→
FnHasUb
g
→
FnHasUb
fun
x
↦
f
x
+
g
x
:=
fun
⟨
a
,
ubfa
⟩
⟨
b
,
ubgb
⟩
↦
⟨
a
+
b
,
fnUb_add
ubfa
ubgb
⟩
The task of unpacking information in a hypothesis is
so important that Lean and Mathlib provide a number of
ways to do it. For example, the
obtain
tactic provides suggestive syntax:
example
(
ubf
:
FnHasUb
f
)
(
ubg
:
FnHasUb
g
)
:
FnHasUb
fun
x
↦
f
x
+
g
x
:=
by
obtain
⟨
a
,
ubfa
⟩
:=
ubf
obtain
⟨
b
,
ubgb
⟩
:=
ubg
exact
⟨
a
+
b
,
fnUb_add
ubfa
ubgb
⟩
Think of the first
obtain
instruction as matching the “contents” of
ubf
with the given pattern and assigning the components to the named variables.
rcases
and
obtain
are said to
destruct
their arguments.
Lean also supports syntax that is similar to that used in other functional programming
languages:
example
(
ubf
:
FnHasUb
f
)
(
ubg
:
FnHasUb
g
)
:
FnHasUb
fun
x
↦
f
x
+
g
x
:=
by
cases
ubf
case
intro
a
ubfa
=>
cases
ubg
case
intro
b
ubgb
=>
exact
⟨
a
+
b
,
fnUb_add
ubfa
ubgb
⟩
example
(
ubf
:
FnHasUb
f
)
(
ubg
:
FnHasUb
g
)
:
FnHasUb
fun
x
↦
f
x
+
g
x
:=
by
cases
ubf
next
a
ubfa
=>
cases
ubg
next
b
ubgb
=>
exact
⟨
a
+
b
,
fnUb_add
ubfa
ubgb
⟩
example
(
ubf
:
FnHasUb
f
)
(
ubg
:
FnHasUb
g
)
:
FnHasUb
fun
x
↦
f
x
+
g
x
:=
by
match
ubf
,
ubg
with
|
⟨
a
,
ubfa
⟩,
⟨
b
,
ubgb
⟩
=>
exact
⟨
a
+
b
,
fnUb_add
ubfa
ubgb
⟩
example
(
ubf
:
FnHasUb
f
)
(
ubg
:
FnHasUb
g
)
:
FnHasUb
fun
x
↦
f
x
+
g
x
:=
match
ubf
,
ubg
with
|
⟨
a
,
ubfa
⟩,
⟨
b
,
ubgb
⟩
=>
⟨
a
+
b
,
fnUb_add
ubfa
ubgb
⟩
In the first example, if you put your cursor after
cases
ubf
,
you will see that the tactic produces a single goal, which Lean has tagged
intro
. (The particular name chosen comes from the internal name for
the axiomatic primitive that builds a proof of an existential statement.)
The
case
tactic then names the components. The second example is similar,
except using
next
instead of
case
means that you can avoid mentioning
intro
. The word
match
in the last two examples highlights that
what we are doing here is what computer scientists call “pattern matching.”
Notice that the third proof begins by
by
, after which the tactic version
of
match
expects a tactic proof on the right side of the arrow.
The last example is a proof term: there are no tactics in sight.
For the rest of this book, we will stick to
rcases
,
rintro
, and
obtain
,
as the preferred ways of using an existential quant

incrementally. Start writing the proof from the top down, using
sorry
to fill in subproofs. Make sure Lean accepts the term with
all the
sorry
's; if not, there are errors that you need to
correct. Then go back and replace each
sorry
with an actual proof,
until no more remain.
Here is another useful trick. Instead of using
sorry
, you can use
an underscore
_
as a placeholder. Recall this tells Lean that
the argument is implicit, and should be filled in automatically. If
Lean tries to do so and fails, it returns with an error message "don't
know how to synthesize placeholder," followed by the type of
the term it is expecting, and all the objects and hypotheses available
in the context. In other words, for each unresolved placeholder, Lean
reports the subgoal that needs to be filled at that point. You can
then construct a proof by incrementally filling in these placeholders.
For reference, here are two sample proofs of validities taken from the
list above.
open Classical

-- distributivity
example (p q r : Prop) : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) :=
  Iff.intro
    (fun h : p ∧ (q ∨ r) =>
      have hp : p := h.left
      Or.elim (h.right)
        (fun hq : q =>
          show (p ∧ q) ∨ (p ∧ r) from Or.inl ⟨hp, hq⟩)
        (fun hr : r =>
          show (p ∧ q) ∨ (p ∧ r) from Or.inr ⟨hp, hr⟩))
    (fun h : (p ∧ q) ∨ (p ∧ r) =>
      Or.elim h
        (fun hpq : p ∧ q =>
          have hp : p := hpq.left
          have hq : q := hpq.right
          show p ∧ (q ∨ r) from ⟨hp, Or.inl hq⟩)
        (fun hpr : p ∧ r =>
          have hp : p := hpr.left
          have hr : r := hpr.right
          show p ∧ (q ∨ r) from ⟨hp, Or.inr hr⟩))

-- an example that requires classical reasoning
example (p q : Prop) : ¬(p ∧ ¬q) → (p → q) :=
  fun h : ¬(p ∧ ¬q) =>
  fun hp : p =>
  show q from
    Or.elim (em q)
      (fun hq : q => hq)
      (fun hnq : ¬q => absurd (And.intro hp hnq) h)
Exercises
Prove the following identities, replacing the "sorry" placeholders with actual proofs.
variable (p q r : Prop)

-- commutativity of ∧ and ∨
example : p ∧ q ↔ q ∧ p := sorry
example : p ∨ q ↔ q ∨ p := sorry

-- associativity of ∧ and ∨
example : (p ∧ q) ∧ r ↔ p ∧ (q ∧ r) := sorry
example : (p ∨ q) ∨ r ↔ p ∨ (q ∨ r) := sorry

-- distributivity
example : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) := sorry
example : p ∨ (q ∧ r) ↔ (p ∨ q) ∧ (p ∨ r) := sorry

-- other properties
example : (p → (q → r)) ↔ (p ∧ q → r) := sorry
example : ((p ∨ q) → r) ↔ (p → r) ∧ (q → r) := sorry
example : ¬(p ∨ q) ↔ ¬p ∧ ¬q := sorry
example : ¬p ∨ ¬q → ¬(p ∧ q) := sorry
example : ¬(p ∧ ¬p) := sorry
example : p ∧ ¬q → ¬(p → q) := sorry
example : ¬p → (p → q) := sorry
example : (¬p ∨ q) → (p → q) := sorry
example : p ∨ False ↔ p := sorry
example : p ∧ False ↔ False := sorry
example : (p → q) → (¬q → ¬p) := sorry
Prove the following identities, replacing the "sorry" placeholders
with actual proofs. These require classical reasoning.
open Classical

variable (p q r : Prop)

example : (p → q ∨ r) → ((p → q) ∨ (p → r)) := sorry
example : ¬(p ∧ q) → ¬p ∨ ¬q := sorry
example : ¬(p → q) → p ∧ ¬q := sorry
example : (p → q) → (¬p ∨ q) := sorry
example : (¬q → ¬p) → (p → q) := sorry
example : p ∨ ¬p := sorry
example : (((p → q) → p) → p) := sorry
Prove
¬(p ↔ ¬p)
without using classical logic.