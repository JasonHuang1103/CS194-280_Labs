ifier.
But it can’t hurt to see the alternative syntax, especially if there is
a chance you will find yourself in the company of computer scientists.
To illustrate one way that
rcases
can be used,
we prove an old mathematical chestnut:
if two integers
x
and
y
can each be written as
a sum of two squares,
then so can their product,
x
*
y
.
In fact, the statement is true for any commutative
ring, not just the integers.
In the next example,
rcases
unpacks two existential
quantifiers at once.
We then provide the magic values needed to express
x
*
y
as a sum of squares as a list to the
use
statement,
and we use
ring
to verify that they work.
variable
{
α
:
Type
*
}
[
CommRing
α
]
def
SumOfSquares
(
x
:
α
)
:=
∃
a
b
,
x
=
a
^
2
+
b
^
2
theorem
sumOfSquares_mul
{
x
y
:
α
}
(
sosx
:
SumOfSquares
x
)
(
sosy
:
SumOfSquares
y
)
:
SumOfSquares
(
x
*
y
)
:=
by
rcases
sosx
with
⟨
a
,
b
,
xeq
⟩
rcases
sosy
with
⟨
c
,
d
,
yeq
⟩
rw
[
xeq
,
yeq
]
use
a
*
c
-
b
*
d
,
a
*
d
+
b
*
c
ring
This proof doesn’t provide much insight,
but here is one way to motivate it.
A
Gaussian integer
is a number of the form
\(a + bi\)
where
\(a\)
and
\(b\)
are integers and
\(i = \sqrt{-1}\)
.
The
norm
of the Gaussian integer
\(a + bi\)
is, by definition,
\(a^2 + b^2\)
.
So the norm of a Gaussian integer is a sum of squares,
and any sum of squares can be expressed in this way.
The theorem above reflects the fact that norm of a product of
Gaussian integers is the product of their norms:
if
\(x\)
is the norm of
\(a + bi\)
and
\(y\)
in the norm of
\(c + di\)
,
then
\(xy\)
is the norm of
\((a + bi) (c + di)\)
.
Our cryptic proof illustrates the fact that
the proof that is easiest to formalize isn’t always
the most perspicuous one.
In
Section 6.3
,
we will provide you with the means to define the Gaussian
integers and use them to provide an alternative proof.
The pattern of unpacking an equation inside an existential quantifier
and then using it to rewrite an expression in the goal
comes up often,
so much so that the
rcases
tactic provides
an abbreviation:
if you use the keyword
rfl
in place of a new identifier,
rcases
does the rewriting automatically (this trick doesn’t work
with pattern-matching lambdas).
theorem
sumOfSquares_mul'
{
x
y
:
α
}
(
sosx
:
SumOfSquares
x
)
(
sosy
:
SumOfSquares
y
)
:
SumOfSquares
(
x
*
y
)
:=
by
rcases
sosx
with
⟨
a
,
b
,
rfl
⟩
rcases
sosy
with
⟨
c
,
d
,
rfl
⟩
use
a
*
c
-
b
*
d
,
a
*
d
+
b
*
c
ring
As with the universal quantifier,
you can find existential quantifiers hidden all over
if you know how to spot them.
For example, divisibility is implicitly an “exists” statement.
example
(
divab
:
a
∣
b
)
(
divbc
:
b
∣
c
)
:
a
∣
c
:=
by
rcases
divab
with
⟨
d
,
beq
⟩
rcases
divbc
with
⟨
e
,
ceq
⟩
rw
[
ceq
,
beq
]
use
d
*
e
;
ring
And once again, this provides a nice setting for using
rcases
with
rfl
.
Try it out in the proof above.
It feels pretty good!
Then try proving the following:
example
(
divab
:
a
∣
b
)
(
divac
:
a
∣
c
)
:
a
∣
b
+
c
:=
by
sorry
For another important example, a function
\(f : \alpha \to \beta\)
is said to be
surjective
if for every
\(y\)
in the
codomain,
\(\beta\)
,
there is an
\(x\)
in the domain,
\(\alpha\)
,
such that
\(f(x) = y\)
.
Notice that this statement includes both a universal
and an existential quantifier, which explains
why the next example makes use of both
intro
and
use
.
example
{
c
:
ℝ
}
:
Surjective
fun
x
↦
x
+
c
:=
by
intro
x
use
x
-
c
dsimp
;
ring
Try this example yourself using the theorem
mul_div_cancel₀
.:
example
{
c
:
ℝ
}
(
h
:
c
≠
0
)
:
Surjective
fun
x
↦
c
*
x
:=
by
sorry
At this point, it is worth mentioning that there is a tactic,
field_simp
,
that will often clear denominators in a useful way.
It can be used in conjunction with the
ring
tactic.
example
(
x
y
:
ℝ
)
(
h
:
x
-
y
≠
0
)
:
(
x
^
2
-
y
^
2
)
/
(
x
-
y
)
=
x
+
y
:=
by
field_simp
[
h
]
ring
The next example uses a surjectivity hypothesis
by applying it to a suitable value.
Note that you can use
rcases
with any expression,
not just a hypothesis.
example
{
f
:
ℝ
→
ℝ
}
(
h
:
Surjective
f
)
:
∃
x
,
f
x
^
2
=
4
:=
by
rcases
h
2
with
⟨
x
,
hx
⟩
use
x
rw
[
hx
]
norm_num
See if you can use these methods to show that
the composition of surjective functions is surjective.
variable
{
α
:
Type
*
}
{
β
:
Type
*
}
{
γ
:
Type
*
}
variable
{
g
:
β
→
γ
}
{
f
:
α
→
β
}
example
(
surjg
:
Surjective
g
)
(
surjf
:
Surjective
f
)
:
Surjective
fun
x
↦
g
(
f
x
)
:=
by
sorry
3.3.
Negation

The symbol
¬
is meant to express negation,
so
¬
x
<
y
says that
x
is not less than
y
,
¬
x
=
y
(or, equivalently,
x
≠
y
) says that
x
is not equal to
y
,
and
¬
∃
z,
x
<
z
∧
z
<
y
says that there does not exist a
z
strictly between
x
and
y
.
In Lean, the notation
¬
A
abbreviates
A
→
False
,
which you can think of as saying that
A
implies a contradiction.
Practically speaking, this means that you already know something
about how to work with negations:
you can prove
¬
A
by introducing a hypothesis
h
:
A
and proving
False
,
and if you have
h
:
¬
A
and
h'
:
A
,
then applying
h
to
h'
yields
False
.
To illustrate, consider the irreflexivity principle
lt_irrefl
for a strict order,
which says that we have
¬
a
<
a
for every
a
.
The asymmetry principle
lt_asymm
says that we have
a
<
b
→
¬
b
<
a
. Let’s show that
lt_asymm
follows
from
lt_irrefl
.
example
(
h
:
a
<
b
)
:
¬
b
<
a
:=
by
intro
h'
have
:
a
<
a
:=
lt_trans
h
h'
apply
lt_irrefl
a
this
This example introduces a couple of new tricks.
First, when you use
have
without providing
a label,
Lean uses the name
this
,
providing a convenient way to refer back to it.
Because the proof is so short, we provide an explicit proof term.
But what you should really be paying attention to in this
proof is the result of the
intro
tactic,
which leaves a goal of
False
,
and the fact that we eventually prove
False
by applying
lt_irrefl
to a proof of
a
<
a
.
Here is another example, which uses the
predicate
FnHasUb
defined in the last section,
which says that a function has an upper bound.
example
(
h
:
∀
a
,
∃
x
,
f
x
>
a
)
:
¬
FnHasUb
f
:=
by
intro
fnub
rcases
fnub
with
⟨
a
,
fnuba
⟩
rcases
h
a
with
⟨
x
,
hx
⟩
have
:
f
x
≤
a
:=
fnuba
x
linarith
Remember that it is often convenient to use
linarith
when a goal follows from linear equations and
inequalities that are in the context.
See if you can prove these in a similar way:
example
(
h
:
∀
a
,
∃
x
,
f
x
<
a
)
:
¬
FnHasLb
f
:=
sorry
example
:
¬
FnHasUb
fun
x
↦
x
:=
sorry
Mathlib offers a number of useful theorems for relating orders
and negations:
#check
(
not_le_of_gt
:
a
>
b
→
¬
a
≤
b
)
#check
(
not_lt_of_ge
:
a
≥
b
→
¬
a
<
b
)
#check
(
lt_of_not_ge
:
¬
a
≥
b
→
a
<
b
)
#check
(
le_of_not_gt
:
¬
a
>
b
→
a
≤
b
)
Recall the predicate
Monotone
f
,
which says that
f
is nondecreasing.
Use some of the theorems just enumerated to prove the following:
example
(
h
:
Monotone
f
)
(
h'
:
f
a
<
f
b
)
:
a
<
b
:=
by
sorry
example
(
h
:
a
≤
b
)
(
h'
:
f
b
<
f
a
)
:
¬
Monotone
f
:=
by
sorry
We can show that the first example in the last snippet
cannot be proved if we replace
<
by
≤
.
Notice that we can prove the negation of a universally
quantified statement by giving a counterexample.
Complete the proof.
example
:
¬∀
{
f
:
ℝ
→
ℝ
},
Monotone
f
→
∀
{
a
b
},
f
a
≤
f
b
→
a
≤
b
:=
by
intro
h
let
f
:=
fun
x
:
ℝ
↦
(
0
:
ℝ
)
have
monof
:
Monotone
f
:=
by
sorry
have
h'
:
f
1
≤
f
0
:=
le_refl
_
sorry
This example introduces the
let
tactic,
which adds a
local definition
to the context.
If you put the cursor after the
let
command,
in the goal window you will see that the definition
f
:
ℝ
→
ℝ
:=
fun
x
↦
0
has been added to the context.
Lean will unfold the definition of
f
when it has to.
In particular, when we prove
f
1
≤
f
0
with
le_refl
,
Lean reduces
f
1
and
f
0
to
0
.
Use
le_of_not_gt
to prove the following:
example
(
x
:
ℝ
)
(
h
:
∀
ε
>
0
,
x
<
ε
)
:
x
≤
0
:=
by
sorry
Implicit in many of the proofs we have just done
is the fact that if
P
is any property,
saying that there is nothing with property
P
is the same as saying that everything fails to have
property
P
,
and saying that not everything has property
P
is equivalent to saying that something fails to have property
P
.
In other words, all four of the following implications
are valid (but one of them cannot be proved with what we explained so
far):
variable
{
α
:
Type
*
}
(
P
:
α
→
Prop
)
(
Q
:
Prop
)
example
(
h
:
¬∃
x
,
P
x
)
:
∀
x
,
¬
P
x
:=
by
sorry
example
(
h
:
∀
x
,
¬
P
x
)
:
¬∃
x
,
P
x
:=
by
sorry
example
(
h
:
¬∀
x
,
P
x
)
:
∃
x
,
¬
P
x
:=
by
sorry
example
(
h
:
∃
x
,
¬
P
x
)
:
¬∀
x
,
P
x
:=
by
sorry
The first, second, and fourth are straightforward to
prove using the methods you have already seen.
We encourage you to try it.
The third is more difficult, however,
because it concludes that an object exists
from the fact that its nonexistence is contradictory.
This is an instance of
classical
mathematical reasoning.
We can use proof by contradiction
to prove the third implication as follows.
example
(
h
:
¬∀
x
,
P
x
)
:
∃
x
,
¬
P
x
:=
by
by_contra
h'
apply
h
intro
x
show
P
x
by_contra
h''
exact
h'
⟨
x
,
h''
⟩
Make sure you understand how this works.
The
by_contra
tactic
allows us to prove a goal
Q
by assuming
¬
Q
and deriving a contradiction.
In fact, it is equivalent to using the
equivalence
not_not
:
¬
¬
Q
↔
Q
.
Confirm that you can prove the forward direction
of this equivalence using
by_contra
,
while the reverse direction follows from the
ordinary rules for negation.
example
(
h
:
¬¬
Q
)
:
Q
:=
by
sorry
example
(
h
:
Q
)
:
¬¬
Q
:=
by
sorry
Use proof by contradiction to establish the following,
which is the converse of one of the implications we proved above.
(Hint: use
intro
first.)
example
(
h
:
¬
FnHasUb
f
)
:
∀
a
,
∃
x
,
f
x
>
a
:=
by
sorry
It is often tedious to work with compound statements with
a negation in front,
and it is a common mathematical pattern to replace such
statements with equivalent forms in which the negation
has been pushed inward.
To facilitate this, Mathlib offers a
push_neg
tactic,
which restates the goal in this way.
The command
push_neg
at
h
restates the hypothesis
h
.
example
(
h
:
¬∀
a
,
∃
x
,
f
x
>
a
)
:
FnHasUb
f
:=
by
push_neg
at
h
exact
h
example
(
h
:
¬
FnHasUb
f
)
:
∀
a
,
∃
x
,
f
x
>
a
:=
by
dsimp
only
[
FnHasUb
,
FnUb
]
at
h
push_neg
at
h
exact
h
In the second example, we use dsimp to
expand the definitions of
FnHasUb
and
FnUb
.
(We need to use
dsimp
rather than
rw
to expand
FnUb
,
because it appears in the scope of a quantifier.)
You can verify that in the examples above
with
¬∃
x,
P
x
and
¬∀
x,
P
x
,
the
push_neg
tactic does the expected thing.
Without even knowing how to use the conjunction
symbol,
you should be able to use
push_neg
to prove the following:
example
(
h
:
¬
Monotone
f
)
:
∃
x
y
,
x
≤
y
∧
f
y
<
f
x
:=
by
sorry
Mathlib also has a tactic,
contrapose
,
which transforms a goal
A
→
B
to
¬B
→
¬A
.
Similarly, given a goal of proving
B
from
hypothesis
h
:
A
,
contrapose
h
leaves you with a goal of proving
¬A
from hypothesis
¬B
.
Using
contrapose!
instead of
contrapose
applies
push_neg
to the goal and the relevant
hypothesis as well.
example
(
h
:
¬
FnHasUb
f
)
:
∀
a
,
∃
x
,
f
x
>
a
:=
by
contrapose
!
h
exact
h
example
(
x
:
ℝ
)
(
h
:
∀
ε
>
0
,
x
≤
ε
)
:
x
≤
0
:=
by
contrapose
!
h
use
x
/
2
constructor
<;>
linarith
We have not yet explained the
constructor
command
or the use of the semicolon after it,
but we will do that in the next section.
We close this section with
the principle of
ex falso
,
which says that anything follows from a contradiction.
In Lean, this is represented by
False.elim
,
which establishes
False
→
P
for any proposition
P
.
This may seem like a strange principle,
but it comes up fairly often.
We often prove a theorem by splitting on cases,
and sometimes we can show that one of
the cases is contradictory.
In that case, we need to assert that the contradiction
establishes the goal so we can move on to the next one.
(We will see instances of reasoning by cases in
Section 3.5
.)
Lean provides a number of ways of closing
a goal once a contradiction has been reached.
example
(
h
:
0
<
0
)
:
a
>
37
:=
by
exfalso
apply
lt_irrefl
0
h
example
(
h
:
0
<
0
)
:
a
>
37
:=
absurd
h
(
lt_irrefl
0
)
example
(
h
:
0
<
0
)
:
a
>
37
:=
by
have
h'
:
¬
0
<
0
:=
lt_irrefl
0
contradiction
The
exfalso
tactic replaces the current goal with
the goal of proving
False
.
Given
h
:
P
and
h'
:
¬
P
,
the term
absurd
h
h'
establishes any proposition.
Finally, the
contradiction
tactic tries to close a goal
by finding a contradiction in the hypotheses,
such as a pair of the form
h
:
P
and
h'
:
¬
P
.
Of course, in this example,
linarith
also works.
3.4.
Conjunction and Iff

You have already seen that the conjunction symbol,
∧
,
is used to express “and.”
The
constructor
tactic allows you to prove a statement of
the form
A
∧
B
by proving
A
and then proving
B
.
example
{
x
y
:
ℝ
}
(
h₀
:
x
≤
y
)
(
h₁
:
¬
y
≤
x
)
:
x
≤
y
∧
x
≠
y
:=
by
constructor
·
assumption
intro
h
apply
h₁
rw
[
h
]
In this example, the
assumption
tactic
tells Lean to find an assumption that will solve the goal.
Notice that the final
rw
finishes the goal by
applying the reflexivity of
≤
.
The following are alternative ways of carrying out
the previous examples using the anonymous constructor
angle brackets.
The first is a slick proof-term version of the
previous proof,
which drops into tactic mode at the keyword
by
.
example
{
x
y
:
ℝ
}
(
h₀
:
x
≤
y
)
(
h₁
:
¬
y
≤
x
)
:
x
≤
y
∧
x
≠
y
:=
⟨
h₀
,
fun
h
↦
h₁
(
by
rw
[
h
])⟩
example
{
x
y
:
ℝ
}
(
h₀
:
x
≤
y
)
(
h₁
:
¬
y
≤
x
)
:
x
≤
y
∧
x
≠
y
:=
have
h
:
x
≠
y
:=
by
contrapose
!
h₁
rw
[
h₁
]
⟨
h₀
,
h
⟩
Using
a conjunction instead of proving one involves unpacking the proofs of the
two parts.
You can use the
rcases
tactic for that,
as well as
rintro
or a pattern-matching
fun
,
all in a manner similar to the way they are used with
the existential quantifier.
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
rcases
h
with
⟨
h₀
,
h₁
⟩
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
example
{
x
y
:
ℝ
}
:
x
≤
y
∧
x
≠
y
→
¬
y
≤
x
:=
by
rintro
⟨
h₀
,
h₁
⟩
h'
exact
h₁
(
le_antisymm
h₀
h'
)
example
{
x
y
:
ℝ
}
:
x
≤
y
∧
x
≠
y
→
¬
y
≤
x
:=
fun
⟨
h₀
,
h₁
⟩
h'
↦
h₁
(
le_antisymm
h₀
h'
)
In analogy to the
obtain
tactic, there is also a pattern-matching
have
:
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
have
⟨
h₀
,
h₁
⟩
:=
h
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
In contrast to
rcases
, here the
have
tactic leaves
h
in the context.
And even though we won’t use them, once again we have the computer scientists’
pattern-matching syntax:
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
cases
h
case
intro
h₀
h₁
=>
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
cases
h
next
h₀
h₁
=>
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
match
h
with
|
⟨
h₀
,
h₁
⟩
=>
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
In contrast to using an existential quantifier,
you can also extract proofs of the two components
of a hypothesis
h
:
A
∧
B
by writing
h.left
and
h.right
,
or, equivalently,
h.1
and
h.2
.
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
intro
h'
apply
h.right
exact
le_antisymm
h.left
h'
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
fun
h'
↦
h.right
(
le_antisymm
h.left
h'
)
Try using these techniques to come up with various ways of proving of the following:
example
{
m
n
:
ℕ
}
(
h
:
m
∣
n
∧
m
≠
n
)
:
m
∣
n
∧
¬
n
∣
m
:=
sorry
You can nest uses of
∃
and
∧
with anonymous constructors,
rintro
, and
rcases
.
example
:
∃
x
:
ℝ
,
2
<
x
∧
x
<
4
:=
⟨
5
/
2
,
by
norm_num
,
by
norm_num
⟩
example
(
x
y
:
ℝ
)
:
(
∃
z
:
ℝ
,
x
<
z
∧
z
<
y
)
→
x
<
y
:=
by
rintro
⟨
z
,
xltz
,
zlty
⟩
exact
lt_trans
xltz
zlty
example
(
x
y
:
ℝ
)
:
(
∃
z
:
ℝ
,
x
<
z
∧
z
<
y
)
→
x
<
y
:=
fun
⟨
z
,
xltz
,
zlty
⟩
↦
lt_trans
xltz
zlty
You can also use the
use
tactic:
example
:
∃
x
:
ℝ
,
2
<
x
∧
x
<
4
:=
by
use
5
/
2
constructor
<;>
norm_num
example
:
∃
m
n
:
ℕ
,
4
<
m
∧
m
<
n
∧
n
<
10
∧
Nat.Prime
m
∧
Nat.Prime
n
:=
by
use
5
use
7
norm_num
example
{
x
y
:
ℝ
}
:
x
≤
y
∧
x
≠
y
→
x
≤
y
∧
¬
y
≤
x
:=
by
rintro
⟨
h₀
,
h₁
⟩
use
h₀
exact
fun
h'
↦
h₁
(
le_antisymm
h₀
h'
)
In the first example, the semicolon after the
constructor
command tells Lean to use the
norm_num
tactic on both of the goals that result.
In Lean,
A
↔
B
is
not
defined to be
(A
→
B)
∧
(B
→
A)
,
but it could have been,
and it behaves roughly the same way.
You have already seen that you can write
h.mp
and
h.mpr
or
h.1
and
h.2
for the two directions of
h
:
A
↔
B
.
You can also use
cases
and friends.
To prove an if-and-only-if statement,
you can use
constructor
or angle brackets,
just as you would if you were proving a conjunction.
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
)
:
¬
y
≤
x
↔
x
≠
y
:=
by
constructor
·
contrapose
!
rintro
rfl
rfl
contrapose
!
exact
le_antisymm
h
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
)
:
¬
y
≤
x
↔
x
≠
y
:=
⟨
fun
h₀
h₁
↦
h₀
(
by
rw
[
h₁
]),
fun
h₀
h₁
↦
h₀
(
le_antisymm
h
h₁
)⟩
The last proof term is inscrutable. Remember that you can
use underscores while writing an expression like that to
see what Lean expects.
Try out the various techniques and gadgets you have just seen
in order to prove the following:
example
{
x
y
:
ℝ
}
:
x
≤
y
∧
¬
y
≤
x
↔
x
≤
y
∧
x
≠
y
:=
sorry
For a more interesting exercise, show that for any
two real numbers
x
and
y
,
x^2
+
y^2
=
0
if and only if
x
=
0
and
y
=
0
.
We suggest proving an auxiliary lemma using
linarith
,
pow_two_nonneg
, and
pow_eq_zero
.
theorem
aux
{
x
y
:
ℝ
}
(
h
:
x
^
2
+
y
^
2
=
0
)
:
x
=
0
:=
have
h'
:
x
^
2
=
0
:=
by
sorry
pow_eq_zero
h'
example
(
x
y
:
ℝ
)
:
x
^
2
+
y
^
2
=
0
↔
x
=
0
∧
y
=
0
:=
sorry
In Lean, bi-implication leads a double-life.
You can treat it like a conjunction and use its two
parts separately.
But Lean also knows that it is a reflexive, symmetric,
and transitive relation between propositions,
and you can also use it with
calc
and
rw
.
It is often convenient to rewrite a statement to
an equivalent one.
In the next example, we use
abs_lt
to
replace an expression of the form
|x|
<
y
by the equivalent expression
-
y
<
x
∧
x
<
y
,
and in the one after that we use
Nat.dvd_gcd_iff
to replace an expression of the form
m
∣
Nat.gcd
n
k
by the equivalent expression
m
∣
n
∧
m
∣
k
.
example
(
x
:
ℝ
)
:
|
x
+
3
|
<
5
→
-
8
<
x
∧
x
<
2
:=
by
rw
[
abs_lt
]
intro
h
constructor
<;>
linarith
example
:
3
∣
Nat.gcd
6
15
:=
by
rw
[
Nat.dvd_gcd_iff
]
constructor
<;>
norm_num
See if you can use
rw
with the theorem below
to provide a short proof that negation is not a
nondecreasing function. (Note that
push_neg
won’t
unfold definitions for you, so the
rw
[Monotone]
in
the proof of the theorem is needed.)
theorem
not_monotone_iff
{
f
:
ℝ
→
ℝ
}
:
¬
Monotone
f
↔
∃
x
y
,
x
≤
y
∧
f
x
>
f
y
:=
by
rw
[
Monotone
]
push_neg
rfl
example
:
¬
Monotone
fun
x
:
ℝ
↦
-
x
:=
by
sorry
The remaining exercises in this section are designed
to give you some more practice with conjunction and
bi-implication. Remember that a
partial order
is a
binary relation that is transitive, reflexive, and
antisymmetric.
An even weaker notion sometimes arises:
a
preorder
is just a reflexive, transitive relation.
For any pre-order
≤
,
Lean axiomatizes the associated strict pre-order by
a
<
b
↔
a
≤
b
∧
¬
b
≤
a
.
Show that if
≤
is a partial order,
then
a
<
b
is equivalent to
a
≤
b
∧
a
≠
b
:
variable
{
α
:
Type
*
}
[
PartialOrder
α
]
variable
(
a
b
:
α
)
example
:
a
<
b
↔
a
≤
b
∧
a
≠
b
:=
by
rw
[
lt_iff_le_not_le
]
sorry
Beyond logical operations, you do not need
anything more than
le_refl
and
le_trans
.
Show that even in the case where
≤
is only assumed to be a preorder,
we can prove that the strict order is irreflexive
and transitive.
In the second example,
for convenience, we use the simplifier rather than
rw
to express
<
in terms of
≤
and
¬
.
We will come back to the simplifier later,
but here we are only relying on the fact that it will
use the indicated lemma repeatedly, even if it needs
to be instantiated to different values.
variable
{
α
:
Type
*
}
[
Preorder
α
]
variable
(
a
b
c
:
α
)
example
:
¬
a
<
a
:=
by
rw
[
lt_iff_le_not_le
]
sorry
example
:
a
<
b
→
b
<
c
→
a
<
c
:=
by
simp
only
[
lt_iff_le_not_le
]
sorry
3.5.
Disjunction

The canonical way to prove a disjunction
A
∨
B
is to prove
A
or to prove
B
.
The
left
tactic chooses
A
,
and the
right
tactic chooses
B
.
variable
{
x
y
:
ℝ
}
example

2. Basics — Mathematics in Lean 0.1 documentation
Mathematics in Lean
1. Introduction
2. Basics
2.1. Calculating
2.2. Proving Identities in Algebraic Structures
2.3. Using Theorems and Lemmas
2.4. More examples using apply and rw
2.5. Proving Facts about Algebraic Structures
3. Logic
4. Sets and Functions
5. Elementary Number Theory
6. Structures
7. Hierarchies
8. Groups and Rings
9. Linear algebra
10. Topology
11. Differential Calculus
12. Integration and Measure Theory
Index
Mathematics in Lean
2.
Basics
View page source
2.
Basics

This chapter is designed to introduce you to the nuts and
bolts of mathematical reasoning in Lean: calculating,
applying lemmas and theorems,
and reasoning about generic structures.
2.1.
Calculating

We generally learn to carry out mathematical calculations
without thinking of them as proofs.
But when we justify each step in a calculation,
as Lean requires us to do,
the net result is a proof that the left-hand side of the calculation
is equal to the right-hand side.
In Lean, stating a theorem is tantamount to stating a goal,
namely, the goal of proving the theorem.
Lean provides the rewriting tactic
rw
,
to replace the left-hand side of an identity by the right-hand side
in the goal. If
a
,
b
, and
c
are real numbers,
mul_assoc
a
b
c
is the identity
a
*
b
*
c
=
a
*
(b
*
c)
and
mul_comm
a
b
is the identity
a
*
b
=
b
*
a
.
Lean provides automation that generally eliminates the need
to refer the facts like these explicitly,
but they are useful for the purposes of illustration.
In Lean, multiplication associates to the left,
so the left-hand side of
mul_assoc
could also be written
(a
*
b)
*
c
.
However, it is generally good style to be mindful of Lean’s
notational conventions and leave out parentheses when Lean does as well.
Let’s try out
rw
.
example
(
a
b
c
:
ℝ
)
:
a
*
b
*
c
=
b
*
(
a
*
c
)
:=
by
rw
[
mul_comm
a
b
]
rw
[
mul_assoc
b
a
c
]
The
import
lines at the beginning of the associated examples file
import the theory of the real numbers from Mathlib, as well as useful automation.
For the sake of brevity,
we generally suppress information like this in the textbook.
You are welcome to make changes to see what happens.
You can type the
ℝ
character as
\R
or
\real
in VS Code.
The symbol doesn’t appear until you hit space or the tab key.
If you hover over a symbol when reading a Lean file,
VS Code will show you the syntax that can be used to enter it.
If you are curious to see all available abbreviations, you can hit Ctrl-Shift-P
and then type abbreviations to get access to the
Lean
4:
Show
Unicode
Input
Abbreviations
command.
If your keyboard does not have an easily accessible backslash,
you can change the leading character by changing the
lean4.input.leader
setting.
When a cursor is in the middle of a tactic proof,
Lean reports on the current
proof state
in the
Lean Infoview
window.
As you move your cursor past each step of the proof,
you can see the state change.
A typical proof state in Lean might look as follows:
1
goal
x
y
:
ℕ
,
h₁
:
Prime
x
,
h₂
:
¬
Even
x
,
h₃
:
y
>
x
⊢
y
≥
4
The lines before the one that begins with
⊢
denote the
context
:
they are the objects and assumptions currently at play.
In this example, these include two objects,
x
and
y
,
each a natural number.
They also include three assumptions,
labelled
h₁
,
h₂
, and
h₃
.
In Lean, everything in a context is labelled with an identifier.
You can type these subscripted labels as
h\1
,
h\2
, and
h\3
,
but any legal identifiers would do:
you can use
h1
,
h2
,
h3
instead,
or
foo
,
bar
, and
baz
.
The last line represents the
goal
,
that is, the fact to be proved.
Sometimes people use
target
for the fact to be proved,
and
goal
for the combination of the context and the target.
In practice, the intended meaning is usually clear.
Try proving these identities,
in each case replacing
sorry
by a tactic proof.
With the
rw
tactic, you can use a left arrow (
\l
)
to reverse an identity.
For example,
rw
[←
mul_assoc
a
b
c]
replaces
a
*
(b
*
c)
by
a
*
b
*
c
in the current goal. Note that
the left-pointing arrow refers to going from right to left in the identity provided
by
mul_assoc
, it has nothing to do with the left or right side of the goal.
example
(
a
b
c
:
ℝ
)
:
c
*
b
*
a
=
b
*
(
a
*
c
)
:=
by
sorry
example
(
a
b
c
:
ℝ
)
:
a
*
(
b
*
c
)
=
b
*
(
a
*
c
)
:=
by
sorry
You can also use identities like
mul_assoc
and
mul_comm
without arguments.
In this case, the rewrite tactic tries to match the left-hand side with
an expression in the goal,
using the first pattern it finds.
example
(
a
b
c
:
ℝ
)
:
a
*
b
*
c
=
b
*
c
*
a
:=
by
rw
[
mul_assoc
]
rw
[
mul_comm
]
You can also provide
partial
information.
For example,
mul_comm
a
matches any pattern of the form
a
*
?
and rewrites it to
?
*
a
.
Try doing the first of these examples without
providing any arguments at all,
and the second with only one argument.
example
(
a
b
c
:
ℝ
)
:
a
*
(
b
*
c
)
=
b
*
(
c
*
a
)
:=
by
sorry
example
(
a
b
c
:
ℝ
)
:
a
*
(
b
*
c
)
=
b
*
(
a
*
c
)
:=
by
sorry
You can also use
rw
with facts from the local context.
example
(
a
b
c
d
e
f
:
ℝ
)
(
h
:
a
*
b
=
c
*
d
)
(
h'
:
e
=
f
)
:
a
*
(
b
*
e
)
=
c
*
(
d
*
f
)
:=
by
rw
[
h'
]
rw
[
←
mul_assoc
]
rw
[
h
]
rw
[
mul_assoc
]
Try these, using the theorem
sub_self
for the second one:
example
(
a
b
c
d
e
f
:
ℝ
)
(
h
:
b
*
c
=
e
*
f
)
:
a
*
b
*
c
*
d
=
a
*
e
*
f
*
d
:=
by
sorry
example
(
a
b
c
d
:
ℝ
)
(
hyp
:
c
=
b
*
a
-
d
)
(
hyp'
:
d
=
a
*
b
)
:
c
=
0
:=
by
sorry
Multiple rewrite commands can be carried out with a single command,
by listing the relevant identities separated by commas inside the square brackets.
example
(
a
b
c
d
e
f
:
ℝ
)
(
h
:
a
*
b
=
c
*
d
)
(
h'
:
e
=
f
)
:
a
*
(
b
*
e
)
=
c
*
(
d
*
f
)
:=
by
rw
[
h'
,
←
mul_assoc
,
h
,
mul_assoc
]
You still see the incremental progress by placing the cursor after
a comma in any list of rewrites.
Another trick is that we can declare variables once and for all outside
an example or theorem. Lean then includes them automatically.
variable
(
a
b
c
d
e
f
:
ℝ
)
example
(
h
:
a
*
b
=
c
*
d
)
(
h'
:
e
=
f
)
:
a
*
(
b
*
e
)
=
c
*
(
d
*
f
)
:=
by
rw
[
h'
,
←
mul_assoc
,
h
,
mul_assoc
]
Inspection of the tactic state at the beginning of the above proof
reveals that Lean indeed included all variables.
We can delimit the scope of the declaration by putting it
in a
section
...
end
block.
Finally, recall from the introduction that Lean provides us with a
command to determine the type of an expression:
section
variable
(
a
b
c
:
ℝ
)
#check
a
#check
a
+
b
#check
(
a
:
ℝ
)
#check
mul_comm
a
b
#check
(
mul_comm
a
b
:
a
*
b
=
b
*
a
)
#check
mul_assoc
c
a
b
#check
mul_comm
a
#check
mul_comm
end
The
#check
command works for both objects and facts.
In response to the command
#check
a
, Lean reports that
a
has type
ℝ
.
In response to the command
#check
mul_comm
a
b
,
Lean reports that
mul_comm
a
b
is a proof of the fact
a
*
b
=
b
*
a
.
The command
#check
(a
:
ℝ)
states our expectation that the
type of
a
is
ℝ
,
and Lean will raise an error if that is not the case.
We will explain the output of the last three
#check
commands later,
but in the meanwhile, you can take a look at them,
and experiment with some
#check
commands of your own.
Let’s try some more examples. The theorem
two_mul
a
says
that
2
*
a
=
a
+
a
. The theorems
add_mul
and
mul_add
express the distributivity of multiplication over addition,
and the theorem
add_assoc
expresses the associativity of addition.
Use the
#check
command to see the precise statements.
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
rw
[
mul_add
,
add_mul
,
add_mul
]
rw
[
←
add_assoc
,
add_assoc
(
a
*
a
)]
rw
[
mul_comm
b
a
,
←
two_mul
]
Whereas it is possible to figure out what is going on in this proof
by stepping through it in the editor,
it is hard to read on its own.
Lean provides a more structured way of writing proofs like this
using the
calc
keyword.
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
calc
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
b
*
a
+
(
a
*
b
+
b
*
b
)
:=
by
rw
[
mul_add
,
add_mul
,
add_mul
]
_
=
a
*
a
+
(
b
*
a
+
a
*
b
)
+
b
*
b
:=
by
rw
[
←
add_assoc
,
add_assoc
(
a
*
a
)]
_
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
rw
[
mul_comm
b
a
,
←
two_mul
]
Notice that the proof does
not
begin with
by
:
an expression that begins with
calc
is a
proof term
.
A
calc
expression can also be used inside a tactic proof,
but Lean interprets it as the instruction to use the resulting
proof term to solve the goal.
The
calc
syntax is finicky: the underscores and justification
have to be in the format indicated above.
Lean uses indentation to determine things like where a block
of tactics or a
calc
block begins and ends;
try changing the indentation in the proof above to see what happens.
One way to write a
calc
proof is to outline it first
using the
sorry
tactic for justification,
make sure Lean accepts the expression modulo these,
and then justify the individual steps using tactics.
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
calc
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
b
*
a
+
(
a
*
b
+
b
*
b
)
:=
by
sorry
_
=
a
*
a
+
(
b
*
a
+
a
*
b
)
+
b
*
b
:=
by
sorry
_
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
sorry
Try proving the following identity using both a pure
rw
proof
and a more structured
calc
proof:
example
:
(
a
+
b
)
*
(
c
+
d
)
=
a
*
c
+
a
*
d
+
b
*
c
+
b
*
d
:=
by
sorry
The following exercise is a little more challenging.
You can use the theorems listed underneath.
example
(
a
b
:
ℝ
)
:
(
a
+
b
)
*
(
a
-
b
)
=
a
^
2
-
b
^
2
:=
by
sorry
#check
pow_two
a
#check
mul_sub
a
b
c
#check
add_mul
a
b
c
#check
add_sub
a
b
c
#check
sub_sub
a
b
c
#check
add_zero
a
We can also perform rewriting in an assumption in the context.
For example,
rw
[mul_comm
a
b]
at
hyp
replaces
a
*
b
by
b
*
a
in the assumption
hyp
.
example
(
a
b
c
d
:
ℝ
)
(
hyp
:
c
=
d
*
a
+
b
)
(
hyp'
:
b
=
a
*
d
)
:
c
=
2
*
a
*
d
:=
by
rw
[
hyp'
]
at
hyp
rw
[
mul_comm
d
a
]
at
hyp
rw
[
←
two_mul
(
a
*
d
)]
at
hyp
rw
[
←
mul_assoc
2
a
d
]
at
hyp
exact
hyp
In the last step, the
exact
tactic can use
hyp
to solve the goal
because at that point
hyp
matches the goal exactly.
We close this section by noting that Mathlib provides a
useful bit of automation with a
ring
tactic,
which is designed to prove identities in any commutative ring as long as they follow
purely from the ring axioms, without using any local assumption.
example
:
c
*
b
*
a
=
b
*
(
a
*
c
)
:=
by
ring
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
ring
example
:
(
a
+
b
)
*
(
a
-
b
)
=
a
^
2
-
b
^
2
:=
by
ring
example
(
hyp
:
c
=
d
*
a
+
b
)
(
hyp'
:
b
=
a
*
d
)
:
c
=
2
*
a
*
d
:=
by
rw
[
hyp
,
hyp'
]
ring
The
ring
tactic is imported indirectly when we
import
Mathlib.Data.Real.Basic
,
but we will see in the next section that it can be used
for calculations on structures other than the real numbers.
It can be imported explicitly with the command
import
Mathlib.Tactic
.
We will see there are similar tactics for other common kind of algebraic
structures.
There is a variation of
rw
called
nth_rw
that allows you to replace only particular instances of an expression in the goal.
Possible matches are enumerated starting with 1,
so in the following example,
nth_rw
2
[h]
replaces the second
occurrence of
a
+
b
with
c
.
example
(
a
b
c
:
ℕ
)
(
h
:
a
+
b
=
c
)
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
c
+
b
*
c
:=
by
nth_rw
2
[
h
]
rw
[
add_mul
]
2.2.
Proving Identities in Algebraic Structures

Mathematically, a ring consists of a collection of objects,
\(R\)
, operations
\(+\)
\(\times\)
, and constants
\(0\)
and
\(1\)
, and an operation
\(x \mapsto -x\)
such that:
\(R\)
with
\(+\)
is an
abelian group
, with
\(0\)
as the additive identity and negation as inverse.
Multiplication is associative with identity
\(1\)
,
and multiplication distributes over addition.
In Lean, the collection of objects is represented as a
type
,
R
.
The ring axioms are as follows:
variable
(
R
:
Type
*
)
[
Ring
R
]
#check
(
add_assoc
:
∀
a
b
c
:
R
,
a
+
b
+
c
=
a
+
(
b
+
c
))
#check
(
add_comm
:
∀
a
b
:
R
,
a
+
b
=
b
+
a
)
#check
(
zero_add
:
∀
a
:
R
,
0
+
a
=
a
)
#check
(
neg_add_cancel
:
∀
a
:
R
,
-
a
+
a
=
0
)
#check
(
mul_assoc
:
∀
a
b
c
:
R
,
a
*
b
*
c
=
a
*
(
b
*
c
))
#check
(
mul_one
:
∀
a
:
R
,
a
*
1
=
a
)
#check
(
one_mul
:
∀
a
:
R
,
1
*
a
=
a
)
#check
(
mul_add
:
∀
a
b
c
:
R
,
a
*
(
b
+
c
)
=
a
*
b
+
a
*
c
)
#check
(
add_mul
:
∀
a
b
c
:
R
,
(
a
+
b
)
*
c
=
a
*
c
+
b
*
c
)
You will learn more about the square brackets in the first line later,
but for the time being,
suffice it to say that the declaration gives us a type,
R
,
and a ring structure on
R
.
Lean then allows us to use generic ring notation with elements of
R
,
and to make use of a library of theorems about rings.
The names of some of the theorems should look familiar:
they are exactly the ones we used to calculate with the real numbers
in the last section.
Lean is good not only for proving things about concrete mathematical
structures like the natural numbers and the integers,
but also for proving things about abstract structures,
characterized axiomatically, like rings.
Moreover, Lean supports
generic reasoning
about
both abstract and concrete structures,
and can be trained to recognize appropriate instances.
So any theorem about rings can be applied to concrete rings
like the integers,
ℤ
, the rational numbers,
ℚ
,
and the complex numbers
ℂ
.
It can also be applied to any instance of an abstract
structure that extends rings,
such as any ordered ring or any field.
Not all important properties of the real numbers hold in an
arbitrary ring, however.
For example, multiplication on the real numbers
is commutative,
but that does not hold in general.
If you have taken a course in linear algebra,
you will recognize that, for every
\(n\)
,
the
\(n\)
by
\(n\)
matrices of real numbers
form a ring in which commutativity usually fails. If we declare
R
to be a
commutative
ring, in fact, all the theorems
in the last section continue to hold when we replace
ℝ
by
R
.
variable
(
R
:
Type
*
)
[
CommRing
R
]
variable
(
a
b
c
d
:
R
)
example
:
c
*
b
*
a
=
b
*
(
a
*
c
)
:=
by
ring
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
ring
example
:
(
a
+
b
)
*
(
a
-
b
)
=
a
^
2
-
b
^
2
:=
by
ring
example
(
hyp
:
c
=
d
*
a
+
b
)
(
hyp'
:
b
=
a
*
d
)
:
c
=
2
*
a
*
d
:=
by
rw
[
hyp
,
hyp'
]
ring
We leave it to you to check that all the other proofs go through unchanged.
Notice that when a proof is short, like
by
ring
or
by
linarith
or
by
sorry
,
it is common (and permissible) to put it on the same line as
the
by
.
Good proof-writing style should strike a balance between concision and readability.
The goal of this section is to strengthen the skills
you have developed in the last section
and apply them to reasoning axiomatically about rings.
We will start with the axioms listed above,
and use them to derive other facts.
Most of the facts we prove are already in Mathlib.
We will give the versions we prove the same names
to help you learn the contents of the library
as well as the naming conventions.
Lean provides an organizational mechanism similar
to those used in programming languages:
when a definition or theorem
foo
is introduced in a
namespace
bar
, its full name is
bar.foo
.
The command
open
bar
later
opens
the namespace,
which allows us to use the shorter name
foo
.
To avoid errors due to name clashes,
in the next example we put our versions of the library
theorems in a new namespace called
MyRing.
The next example shows that we do not need
add_zero
or
add_neg_cancel
as ring axioms, because they follow from the other axioms.
namespace
MyRing
variable
{
R
:
Type
*
}
[
Ring
R
]
theorem
add_zero
(
a
:
R
)
:
a
+
0
=
a
:=
by
rw
[
add_comm
,
zero_add
]
theorem
add_neg_cancel
(
a
:
R
)
:
a
+
-
a
=
0
:=
by
rw
[
add_comm
,
neg_add_cancel
]
#check
MyRing.add_zero
#check
add_zero
end
MyRing
The net effect is that we can temporarily reprove a theorem in the library,
and then go on using the library version after that.
But don’t cheat!
In the exercises that follow, take care to use only the
general facts about rings that we have proved earlier in this section.
(If you are paying careful attention, you may have noticed that we
changed the round brackets in
(R
:
Type*)
for
curly brackets in
{R
:
Type*}
.
This declares
R
to be an
implicit argument
.
We will explain what this means in a moment,
but don’t worry about it in the meanwhile.)
Here is a useful theorem:
theorem
neg_add_cancel_left
(
a
b
:
R
)
:
-
a
+
(
a
+
b
)
=
b
:=
by
rw
[
←
add_assoc
,
neg_add_cancel
,
zero_add
]
Prove the companion version:
theorem
add_neg_cancel_right
(
a
b
:
R
)
:
a
+
b
+
-
b
=
a
:=
by
sorry
Use these to prove the following:
theorem
add_left_cancel
{
a
b
c
:
R
}
(
h
:
a
+
b
=
a
+
c
)
:
b
=
c
:=
by
sorry
theorem
add_right_cancel
{
a
b
c
:
R
}
(
h
:
a
+
b
=
c
+
b
)
:
a
=
c
:=
by
sorry
With enough planning, you can do each of them with three rewrites.
We will now explain the use of the curly braces.
Imagine you are in a situation where you have
a
,
b
, and
c
in your context,
as well as a hypothesis
h
:
a
+
b
=
a
+
c
,
and you would like to draw the conclusion
b
=
c
.
In Lean, you can apply a theorem to hypotheses and facts just
the same way that you can apply them to objects,
so you might think that
add_left_cancel
a
b
c
h
is a
proof of the fact
b
=
c
.
But notice that explicitly writing
a
,
b
, and
c
is redundant, because the hypothesis
h
makes it clear that
those are the objects we have in mind.
In this case, typing a few extra characters is not onerous,
but if we wanted to apply
add_left_cancel
to more complicated expressions,
writing them would be tedious.
In cases like these,
Lean allows us to mark arguments as
implicit
,
meaning that they are supposed to be left out and inferred by other means,
such as later arguments and hypotheses.
The curly brackets in
{a
b
c
:
R}
do exactly that.
So, given the statement of the theorem above,
the correct expression is simply
add_left_cancel
h
.
To illustrate, let us show that
a
*
0
=
0
follows from the ring axioms.
theorem
mul_zero
(
a
:
R
)
:
a
*
0
=
0
:=
by
have
h
:
a
*
0
+
a
*
0
=
a
*
0
+
0
:=
by
rw
[
←
mul_add
,
add_zero
,
add_zero
]
rw
[
add_left_cancel
h
]
We have used a new trick!
If you step through the proof,
you can see what is going on.
The
have
tactic introduces a new goal,
a
*
0
+
a
*
0
=
a
*
0
+
0
,
with the same context as the original goal.
The fact that the next line is indented indicates that Lean
is expecting a block of tactics that serves to prove this
new goal.
The indentation therefore promotes a modular style of proof:
the indented subproof establishes the goal
that was introduced by the
have
.
After that, we are back to proving the original goal,
except a new hypothesis
h
has been added:
having proved it, we are now free to use it.
At this point, the goal is exactly the result of
add_left_cancel
h
.
We could equally well have closed the proof with
apply
add_left_cancel
h
or
exact
add_left_cancel
h
.
The
exact
tactic takes as argument a proof term which completely proves the
current goal, without creating any new goal. The
apply
tactic is a variant
whose argument is not necessarily a complete proof. The missing pieces are either
inferred automatically by Lean or become new goals to prove.
While the
exact
tactic is technically redundant since it is strictly less powerful
than
apply
, it makes proof scripts slightly clearer to
human readers and easier to maintain when the library evolves.
Remember that multiplication is not assumed to be commutative,
so the following theorem also requires some work.
theorem
zero_mul
(
a
:
R
)
:
0
*
a
=
0
:=
by
sorry
By now, you should also be able replace each
sorry
in the next
exercise with a proof,
still using only facts about rings that we have
established in this section.
theorem
neg_eq_of_add_eq_zero
{
a
b
:
R
}
(
h
:
a
+
b
=
0
)
:
-
a
=
b
:=
by
sorry
theorem
eq_neg_of_add_eq_zero
{
a
b
:
R
}
(
h
:
a
+
b
=
0
)
:
a
=
-
b
:=
by
sorry
theorem
neg_zero
:
(
-
0
:
R
)
=
0
:=
by
apply
neg_eq_of_add_eq_zero
rw
[
add_zero
]
theorem
neg_neg
(
a
:
R
)
:
-
-
a
=
a
:=
by
sorry
We had to use the annotation
(-0
:
R)
instead of
0
in the third theorem
because without specifying
R
it is impossible for Lean to infer which
0
we have in mind,
and by default it would be interpreted as a natural number.
In Lean, subtraction in a ring is provably equal to
addition of the additive inverse.
example
(
a
b
:
R
)
:
a
-
b
=
a
+
-
b
:=
sub_eq_add_neg
a
b
On the real numbers, it is
defined
that way:
example
(
a
b
:
ℝ
)
:
a
-
b
=
a
+
-
b
:=
rfl
example
(
a
b
:
ℝ
)
:
a
-
b
=
a
+
-
b
:=
by
rfl
The proof term
rfl
is short for “reflexivity”.
Presenting it as a proof of
a
-
b
=
a
+
-b
forces Lean
to unfold the definition and recognize both sides as being the same.
The
rfl
tactic does the same.
This is an instance of what is known as a
definitional equality
in Lean’s underlying logic.
This means that not only can one rewrite with
sub_eq_add_neg
to replace
a
-
b
=
a
+
-b
,
but in some contexts, when dealing with the real numbers,
you can use the two sides of the equation interchangeably.
For example, you now have enough information to prove the theorem
self_sub
from the last section:
theorem
self_sub
(
a
:
R
)
:
a
-
a
=
0
:=
by
sorry
Show that you can prove this using
rw
,
but if you replace the arbitrary ring
R
by
the real numbers, you can also prove it
using either
apply
or
exact
.
Lean knows that
1
+
1
=
2
holds in any ring.
With a bit of effort,
you can use that to prove the theorem
two_mul
from
the last section:
theorem
one_add_one_eq_two
:
1
+
1
=
(
2
:
R
)
:=
by
norm_num
theorem
two_mul
(
a
:
R
)
:
2
*
a
=
a
+
a
:=
by
sorry
We close this section by noting that some of the facts about
addition and negation that we established above do not
need the full strength of the ring axioms, or even
commutativity of addition. The weaker notion of a
group
can be axiomatized as follows:
variable
(
A
:
Type
*
)
[
AddGroup
A
]
#check
(
add_assoc
:
∀
a
b
c
:
A
,
a
+
b
+
c
=
a
+
(
b
+
c
))
#check
(
zero_add
:
∀
a
:
A
,
0
+
a
=
a
)
#check
(
neg_add_cancel
:
∀
a
:
A
,
-
a
+
a
=
0
)
It is conventional to use additive notation when
the group operation is commutative,
and multiplicative notation otherwise.
So Lean defines a multiplicative version as well as the
additive version (and also their abelian variants,
AddCommGroup
and
CommGroup
).
variable
{
G
:
Type
*
}
[
Group
G
]
#check
(
mul_assoc
:
∀
a
b
c
:
G
,
a
*
b
*
c
=
a
*
(
b
*
c
))
#check
(
one_mul
:
∀
a
:
G
,
1
*
a
=
a
)
#check
(
inv_mul_cancel
:
∀
a
:
G
,
a
⁻¹
*
a
=
1
)
If you are feeling cocky, try proving the following facts about
groups, using only these axioms.
You will need to prove a number of helper lemmas along the way.
The proofs we have carried out in this section provide some hints.
theorem
mul_inv_cancel
(
a
:
G
)
:
a
*
a
⁻¹
=
1
:=
by
sorry
theorem
mul_one
(
a
:
G
)
:
a
*
1
=
a
:=
by
sorry
theorem
mul_inv_rev
(
a
b
:
G
)
:
(
a
*
b
)
⁻¹
=
b
⁻¹
*
a
⁻¹
:=
by
sorry
Explicitly invoking those lemmas is tedious, so Mathlib provides
tactics similar to
ring
in order to cover most uses:
group
is for non-commutative multiplicative groups,
abel
for abelian
additive groups, and
noncomm_ring
for non-commutative rings.
It may seem odd that the algebraic structures are called
Ring
and
CommRing
while the tactics are named
noncomm_ring
and
ring
. This is partly for historical reasons,
but also for the convenience of using a shorter name for the
tactic that deals with commutative rings, since it is used more often.
2.3.
Using Theorems and Lemmas

Rewriting is great for proving equations,
but what about other sorts of theorems?
For example, how can we prove an inequality,
like the fact that
\(a + e^b \le a + e^c\)
holds whenever
\(b \le c\)
?
We have already seen that theorems can be applied to arguments and hypotheses,
and that the
apply
and
exact
tactics can be used to solve goals.
In this section, we will make good use of these tools.
Consider the library theorems
le_refl
and
le_trans
:
#check
(
le_refl
:
∀
a
:
ℝ
,
a
≤
a
)
#check
(
le_trans
:
a
≤
b
→
b
≤
c
→
a
≤
c
)
As we explain in more detail in
Section 3.1
,
the implicit parentheses in the statement of
le_trans
associate to the right, so it should be interpreted as
a
≤
b
→
(b
≤
c
→
a
≤
c)
.
The library designers have set the arguments
a
,
b
and
c
to
le_trans
implicit,
so that Lean will
not
let you provide them explicitly (unless you
really insist, as we will discuss later).
Rather, it expects to infer them from the context in which they are used.
For example, when hypotheses

Propositions and Proofs - Theorem Proving in Lean 4
Theorem Proving in Lean 4
1.
Introduction
2.
Dependent Type Theory
3.
Propositions and Proofs
4.
Quantifiers and Equality
5.
Tactics
6.
Interacting with Lean
7.
Inductive Types
8.
Induction and Recursion
9.
Structures and Records
10.
Type Classes
11.
The Conversion Tactic Mode
12.
Axioms and Computation
Light (default)
Rust
Coal
Navy
Ayu
Theorem Proving in Lean 4
Propositions and Proofs
By now, you have seen some ways of defining objects and functions in
Lean. In this chapter, we will begin to explain how to write
mathematical assertions and proofs in the language of dependent type
theory as well.
Propositions as Types
One strategy for proving assertions about objects defined in the
language of dependent type theory is to layer an assertion language
and a proof language on top of the definition language. But there is
no reason to multiply languages in this way: dependent type theory is
flexible and expressive, and there is no reason we cannot represent
assertions and proofs in the same general framework.
For example, we could introduce a new type,
Prop
, to represent
propositions, and introduce constructors to build new propositions
from others.
def Implies (p q : Prop) : Prop := p → q
#check And     -- Prop → Prop → Prop
#check Or      -- Prop → Prop → Prop
#check Not     -- Prop → Prop
#check Implies -- Prop → Prop → Prop

variable (p q r : Prop)
#check And p q                      -- Prop
#check Or (And p q) r               -- Prop
#check Implies (And p q) (And q p)  -- Prop
We could then introduce, for each element
p : Prop
, another type
Proof p
, for the type of proofs of
p
.  An "axiom" would be a
constant of such a type.
def Implies (p q : Prop) : Prop := p → q
structure Proof (p : Prop) : Type where
proof : p
#check Proof   -- Proof : Prop → Type

axiom and_comm (p q : Prop) : Proof (Implies (And p q) (And q p))

variable (p q : Prop)
#check and_comm p q     -- Proof (Implies (And p q) (And q p))
In addition to axioms, however, we would also need rules to build new
proofs from old ones. For example, in many proof systems for
propositional logic, we have the rule of
modus ponens
:
From a proof of
Implies p q
and a proof of
p
, we obtain a proof of
q
.
We could represent this as follows:
def Implies (p q : Prop) : Prop := p → q
structure Proof (p : Prop) : Type where
proof : p
axiom modus_ponens : (p q : Prop) → Proof (Implies p q) → Proof p → Proof q
Systems of natural deduction for propositional logic also typically rely on the following rule:
Suppose that, assuming
p
as a hypothesis, we have a proof of
q
. Then we can "cancel" the hypothesis and obtain a proof of
Implies p q
.
We could render this as follows:
def Implies (p q : Prop) : Prop := p → q
structure Proof (p : Prop) : Type where
proof : p
axiom implies_intro : (p q : Prop) → (Proof p → Proof q) → Proof (Implies p q)
This approach would provide us with a reasonable way of building assertions and proofs.
Determining that an expression
t
is a correct proof of assertion
p
would then
simply be a matter of checking that
t
has type
Proof p
.
Some simplifications are possible, however. To start with, we can
avoid writing the term
Proof
repeatedly by conflating
Proof p
with
p
itself. In other words, whenever we have
p : Prop
, we
can interpret
p
as a type, namely, the type of its proofs. We can
then read
t : p
as the assertion that
t
is a proof of
p
.
Moreover, once we make this identification, the rules for implication
show that we can pass back and forth between
Implies p q
and
p → q
. In other words, implication between propositions
p
and
q
corresponds to having a function that takes any element of
p
to an
element of
q
. As a result, the introduction of the connective
Implies
is entirely redundant: we can use the usual function space
constructor
p → q
from dependent type theory as our notion of
implication.
This is the approach followed in the Calculus of Constructions, and
hence in Lean as well. The fact that the rules for implication in a
proof system for natural deduction correspond exactly to the rules
governing abstraction and application for functions is an instance of
the
Curry-Howard isomorphism
, sometimes known as the
propositions-as-types
paradigm. In fact, the type
Prop
is
syntactic sugar for
Sort 0
, the very bottom of the type hierarchy
described in the last chapter. Moreover,
Type u
is also just
syntactic sugar for
Sort (u+1)
.
Prop
has some special
features, but like the other type universes, it is closed under the
arrow constructor: if we have
p q : Prop
, then
p → q : Prop
.
There are at least two ways of thinking about propositions as
types. To some who take a constructive view of logic and mathematics,
this is a faithful rendering of what it means to be a proposition: a
proposition
p
represents a sort of data type, namely, a
specification of the type of data that constitutes a proof. A proof of
p
is then simply an object
t : p
of the right type.
Those not inclined to this ideology can view it, rather, as a simple
coding trick. To each proposition
p
we associate a type that is
empty if
p
is false and has a single element, say
*
, if
p
is true. In the latter case, let us say that (the type associated
with)
p
is
inhabited
. It just so happens that the rules for
function application and abstraction can conveniently help us keep
track of which elements of
Prop
are inhabited. So constructing an
element
t : p
tells us that
p
is indeed true. You can think of
the inhabitant of
p
as being the "fact that
p
is true." A
proof of
p → q
uses "the fact that
p
is true" to obtain "the
fact that
q
is true."
Indeed, if
p : Prop
is any proposition, Lean's kernel treats any
two elements
t1 t2 : p
as being definitionally equal, much the
same way as it treats
(fun x => t) s
and
t[s/x]
as
definitionally equal. This is known as
proof irrelevance,
and is
consistent with the interpretation in the last paragraph. It means
that even though we can treat proofs
t : p
as ordinary objects in
the language of dependent type theory, they carry no information
beyond the fact that
p
is true.
The two ways we have suggested thinking about the
propositions-as-types paradigm differ in a fundamental way. From the
constructive point of view, proofs are abstract mathematical objects
that are
denoted
by suitable expressions in dependent type
theory. In contrast, if we think in terms of the coding trick
described above, then the expressions themselves do not denote
anything interesting. Rather, it is the fact that we can write them
down and check that they are well-typed that ensures that the
proposition in question is true. In other words, the expressions
themselves
are the proofs.
In the exposition below, we will slip back and forth between these two
ways of talking, at times saying that an expression "constructs" or
"produces" or "returns" a proof of a proposition, and at other times
simply saying that it "is" such a proof. This is similar to the way
that computer scientists occasionally blur the distinction between
syntax and semantics by saying, at times, that a program "computes" a
certain function, and at other times speaking as though the program
"is" the function in question.
In any case, all that really matters is the bottom line. To formally
express a mathematical assertion in the language of dependent type
theory, we need to exhibit a term
p : Prop
. To
prove
that
assertion, we need to exhibit a term
t : p
. Lean's task, as a
proof assistant, is to help us to construct such a term,
t
, and to
verify that it is well-formed and has the correct type.
Working with Propositions as Types
In the propositions-as-types paradigm, theorems involving only
→
can be proved using lambda abstraction and application. In Lean, the
theorem
command introduces a new theorem:
variable {p : Prop}
variable {q : Prop}

theorem t1 : p → q → p := fun hp : p => fun hq : q => hp
Compare this proof to the expression
fun x : α => fun y : β => x
of type
α → β → α
, where
α
and
β
are data types.
This describes the function that takes arguments
x
and
y
of type
α
and
β
, respectively, and returns
x
.
The proof of
t1
has the same form, the only difference being that
p
and
q
are elements of
Prop
rather than
Type
.
Intuitively, our proof of
p → q → p
assumes
p
and
q
are true, and uses the first
hypothesis (trivially) to establish that the conclusion,
p
, is
true.
Note that the
theorem
command is really a version of the
def
command: under the propositions and types
correspondence, proving the theorem
p → q → p
is really the same
as defining an element of the associated type. To the kernel type
checker, there is no difference between the two.
There are a few pragmatic differences between definitions and
theorems, however. In normal circumstances, it is never necessary to
unfold the "definition" of a theorem; by proof irrelevance, any two
proofs of that theorem are definitionally equal. Once the proof of a
theorem is complete, typically we only need to know that the proof
exists; it doesn't matter what the proof is. In light of that fact,
Lean tags proofs as
irreducible
, which serves as a hint to the
parser (more precisely, the
elaborator
) that there is generally no
need to unfold them when processing a file. In fact, Lean is generally
able to process and check proofs in parallel, since assessing the
correctness of one proof does not require knowing the details of
another.
As with definitions, the
#print
command will show you the proof of
a theorem:
variable {p : Prop}
variable {q : Prop}
theorem t1 : p → q → p := fun hp : p => fun hq : q => hp

#print t1
Notice that the lambda abstractions
hp : p
and
hq : q
can be
viewed as temporary assumptions in the proof of
t1
.  Lean also
allows us to specify the type of the final term
hp
, explicitly,
with a
show
statement:
variable {p : Prop}
variable {q : Prop}
theorem t1 : p → q → p :=
  fun hp : p =>
  fun hq : q =>
  show p from hp
Adding such extra information can improve the clarity of a proof and
help detect errors when writing a proof. The
show
command does
nothing more than annotate the type, and, internally, all the
presentations of
t1
that we have seen produce the same term.
As with ordinary definitions, we can move the lambda-abstracted
variables to the left of the colon:
variable {p : Prop}
variable {q : Prop}
theorem t1 (hp : p) (hq : q) : p := hp

#print t1    -- p → q → p
We can use the theorem
t1
just as a function application:
variable {p : Prop}
variable {q : Prop}
theorem t1 (hp : p) (hq : q) : p := hp

axiom hp : p

theorem t2 : q → p := t1 hp
The
axiom
declaration postulates the existence of an
element of the given type and may compromise logical consistency. For
example, we can use it to postulate that the empty type
False
has an
element:
axiom unsound : False
-- Everything follows from false
theorem ex : 1 = 0 :=
  False.elim unsound
Declaring an "axiom"
hp : p
is tantamount to declaring that
p
is true, as witnessed by
hp
. Applying the theorem
t1 : p → q → p
to the fact
hp : p
that
p
is true yields the theorem
t1 hp : q → p
.
Recall that we can also write theorem
t1
as follows:
theorem t1 {p q : Prop} (hp : p) (hq : q) : p := hp

#print t1
The type of
t1
is now
∀ {p q : Prop}, p → q → p
. We can read
this as the assertion "for every pair of propositions
p q
, we have
p → q → p
." For example, we can move all parameters to the right
of the colon:
theorem t1 : ∀ {p q : Prop}, p → q → p :=
  fun {p q : Prop} (hp : p) (hq : q) => hp
If
p
and
q
have been declared as variables, Lean will
generalize them for us automatically:
variable {p q : Prop}

theorem t1 : p → q → p := fun (hp : p) (hq : q) => hp
In fact, by the propositions-as-types correspondence, we can declare
the assumption
hp
that
p
holds, as another variable:
variable {p q : Prop}
variable (hp : p)

theorem t1 : q → p := fun (hq : q) => hp
Lean detects that the proof uses
hp
and automatically adds
hp : p
as a premise. In all cases, the command
#print t1
still yields
∀ p q : Prop, p → q → p
. Remember that this type can just as well
be written
∀ (p q : Prop) (hp : p) (hq : q), p
, since the arrow
denotes nothing more than an arrow type in which the target does not
depend on the bound variable.
When we generalize
t1
in such a way, we can then apply it to
different pairs of propositions, to obtain different instances of the
general theorem.
theorem t1 (p q : Prop) (hp : p) (hq : q) : p := hp

variable (p q r s : Prop)

#check t1 p q                -- p → q → p
#check t1 r s                -- r → s → r
#check t1 (r → s) (s → r)    -- (r → s) → (s → r) → r → s

variable (h : r → s)
#check t1 (r → s) (s → r) h  -- (s → r) → r → s
Once again, using the propositions-as-types correspondence, the
variable
h
of type
r → s
can be viewed as the hypothesis, or
premise, that
r → s
holds.
As another example, let us consider the composition function discussed
in the last chapter, now with propositions instead of types.
variable (p q r s : Prop)

theorem t2 (h₁ : q → r) (h₂ : p → q) : p → r :=
  fun h₃ : p =>
  show r from h₁ (h₂ h₃)
As a theorem of propositional logic, what does
t2
say?
Note that it is often useful to use numeric unicode subscripts,
entered as
\0
,
\1
,
\2
, ..., for hypotheses, as we did in
this example.
Propositional Logic
Lean defines all the standard logical connectives and notation. The propositional connectives come with the following notation:
Ascii
Unicode
Editor shortcut
Definition
True
True
False
False
Not
¬
\not
,
\neg
Not
/\
∧
\and
And
\/
∨
\or
Or
->
→
\to
,
\r
,
\imp
<->
↔
\iff
,
\lr
Iff
They all take values in
Prop
.
variable (p q : Prop)

#check p → q → p ∧ q
#check ¬p → p ↔ False
#check p ∨ q → q ∨ p
The order of operations is as follows: unary negation
¬
binds most
strongly, then
∧
, then
∨
, then
→
, and finally
↔
. For
example,
a ∧ b → c ∨ d ∧ e
means
(a ∧ b) → (c ∨ (d ∧ e))
. Remember that
→
associates to the right (nothing changes
now that the arguments are elements of
Prop
, instead of some other
Type
), as do the other binary connectives. So if we have
p q r : Prop
, the expression
p → q → r
reads "if
p
, then if
q
,
then
r
." This is just the "curried" form of
p ∧ q → r
.
In the last chapter we observed that lambda abstraction can be viewed
as an "introduction rule" for
→
. In the current setting, it shows
how to "introduce" or establish an implication. Application can be
viewed as an "elimination rule," showing how to "eliminate" or use an
implication in a proof. The other propositional connectives are
defined in Lean's library in the file
Prelude.core
(see
importing files
for more information on the library
hierarchy), and each connective comes with its canonical introduction
and elimination rules.
Conjunction
The expression
And.intro h1 h2
builds a proof of
p ∧ q
using
proofs
h1 : p
and
h2 : q
. It is common to describe
And.intro
as the
and-introduction
rule. In the next example we
use
And.intro
to create a proof of
p → q → p ∧ q
.
variable (p q : Prop)

example (hp : p) (hq : q) : p ∧ q := And.intro hp hq

#check fun (hp : p) (hq : q) => And.intro hp hq
The
example
command states a theorem without naming it or storing
it in the permanent context. Essentially, it just checks that the
given term has the indicated type. It is convenient for illustration,
and we will use it often.
The expression
And.left h
creates a proof of
p
from a proof
h : p ∧ q
. Similarly,
And.right h
is a proof of
q
. They
are commonly known as the left and right
and-elimination
rules.
variable (p q : Prop)

example (h : p ∧ q) : p := And.left h
example (h : p ∧ q) : q := And.right h
We can now prove
p ∧ q → q ∧ p
with the following proof term.
variable (p q : Prop)

example (h : p ∧ q) : q ∧ p :=
  And.intro (And.right h) (And.left h)
Notice that and-introduction and and-elimination are similar to the
pairing and projection operations for the Cartesian product. The
difference is that given
hp : p
and
hq : q
,
And.intro hp hq
has type
p ∧ q : Prop
, while
Prod hp hq
has type
p × q : Type
. The similarity between
∧
and
×
is another instance
of the Curry-Howard isomorphism, but in contrast to implication and
the function space constructor,
∧
and
×
are treated separately
in Lean. With the analogy, however, the proof we have just constructed
is similar to a function that swaps the elements of a pair.
We will see in
Chapter Structures and Records
that certain
types in Lean are
structures
, which is to say, the type is defined
with a single canonical
constructor
which builds an element of the
type from a sequence of suitable arguments. For every
p q : Prop
,
p ∧ q
is an example: the canonical way to construct an element is
to apply
And.intro
to suitable arguments
hp : p
and
hq : q
. Lean allows us to use
anonymous constructor
notation
⟨arg1, arg2, ...⟩
in situations like these, when the relevant type is an
inductive type and can be inferred from the context. In particular, we
can often write
⟨hp, hq⟩
instead of
And.intro hp hq
:
variable (p q : Prop)
variable (hp : p) (hq : q)

#check (⟨hp, hq⟩ : p ∧ q)
These angle brackets are obtained by typing
\<
and
\>
, respectively.
Lean provides another useful syntactic gadget. Given an expression
e
of an inductive type
Foo
(possibly applied to some
arguments), the notation
e.bar
is shorthand for
Foo.bar e
.
This provides a convenient way of accessing functions without opening
a namespace.  For example, the following two expressions mean the same
thing:
variable (xs : List Nat)

#check List.length xs
#check xs.length
As a result, given
h : p ∧ q
, we can write
h.left
for
And.left h
and
h.right
for
And.right h
. We can therefore
rewrite the sample proof above conveniently as follows:
variable (p q : Prop)

example (h : p ∧ q) : q ∧ p :=
  ⟨h.right, h.left⟩
There is a fine line between brevity and obfuscation, and omitting
information in this way can sometimes make a proof harder to read. But
for straightforward constructions like the one above, when the type of
h
and the goal of the construction are salient, the notation is
clean and effective.
It is common to iterate constructions like "And." Lean also allows you
to flatten nested constructors that associate to the right, so that
these two proofs are equivalent:
variable (p q : Prop)

example (h : p ∧ q) : q ∧ p ∧ q :=
  ⟨h.right, ⟨h.left, h.right⟩⟩

example (h : p ∧ q) : q ∧ p ∧ q :=
  ⟨h.right, h.left, h.right⟩
This is often useful as well.
Disjunction
The expression
Or.intro_left q hp
creates a proof of
p ∨ q
from a proof
hp : p
. Similarly,
Or.intro_right p hq
creates a
proof for
p ∨ q
using a proof
hq : q
. These are the left and
right
or-introduction
rules.
variable (p q : Prop)
example (hp : p) : p ∨ q := Or.intro_left q hp
example (hq : q) : p ∨ q := Or.intro_right p hq
The
or-elimination
rule is slightly more complicated. The idea is
that we can prove
r
from
p ∨ q
, by showing that
r
follows
from
p
and that
r
follows from
q
.  In other words, it is a
proof by cases. In the expression
Or.elim hpq hpr hqr
,
Or.elim
takes three arguments,
hpq : p ∨ q
,
hpr : p → r
and
hqr : q → r
, and produces a proof of
r
. In the following example, we use
Or.elim
to prove
p ∨ q → q ∨ p
.
variable (p q r : Prop)

example (h : p ∨ q) : q ∨ p :=
  Or.elim h
    (fun hp : p =>
      show q ∨ p from Or.intro_right q hp)
    (fun hq : q =>
      show q ∨ p from Or.intro_left p hq)
In most cases, the first argument of
Or.intro_right
and
Or.intro_left
can be inferred automatically by Lean. Lean
therefore provides
Or.inr
and
Or.inl
which can be viewed as
shorthand for
Or.intro_right _
and
Or.intro_left _
. Thus the
proof term above could be written more concisely:
variable (p q r : Prop)

example (h : p ∨ q) : q ∨ p :=
  Or.elim h (fun hp => Or.inr hp) (fun hq => Or.inl hq)
Notice that there is enough information in the full expression for
Lean to infer the types of
hp
and
hq
as well.  But using the
type annotations in the longer version makes the proof more readable,
and can help catch and debug errors.
Because
Or
has two constructors, we cannot use anonymous
constructor notation. But we can still write
h.elim
instead of
Or.elim h
:
variable (p q r : Prop)

example (h : p ∨ q) : q ∨ p :=
  h.elim (fun hp => Or.inr hp) (fun hq => Or.inl hq)
Once again, you should exercise judgment as to whether such
abbreviations enhance or diminish readability.
Negation and Falsity
Negation,
¬p
, is actually defined to be
p → False
, so we
obtain
¬p
by deriving a contradiction from
p
. Similarly, the
expression
hnp hp
produces a proof of
False
from
hp : p
and
hnp : ¬p
. The next example uses both these rules to produce a
proof of
(p → q) → ¬q → ¬p
. (The symbol
¬
is produced by
typing
\not
or
\neg
.)
variable (p q : Prop)

example (hpq : p → q) (hnq : ¬q) : ¬p :=
  fun hp : p =>
  show False from hnq (hpq hp)
The connective
False
has a single elimination rule,
False.elim
, which expresses the fact that anything follows from a
contradiction. This rule is sometimes called
ex falso
(short for
ex
falso sequitur quodlibet
), or the
principle of explosion
.
variable (p q : Prop)

example (hp : p) (hnp : ¬p) : q := False.elim (hnp hp)
The arbitrary fact,
q
, that follows from falsity is an implicit
argument in
False.elim
and is inferred automatically. This
pattern, deriving an arbitrary fact from contradictory hypotheses, is
quite common, and is represented by
absurd
.
variable (p q : Prop)

example (hp : p) (hnp : ¬p) : q := absurd hp hnp
Here, for example, is a proof of
¬p → q → (q → p) → r
:
variable (p q r : Prop)

example (hnp : ¬p) (hq : q) (hqp : q → p) : r :=
  absurd (hqp hq) hnp
Incidentally, just as
False
has only an elimination rule,
True
has only an introduction rule,
True.intro : true
.  In other words,
True
is simply true, and has a canonical proof,
True.intro
.
Logical Equivalence
The expression
Iff.intro h1 h2
produces a proof of
p ↔ q
from
h1 : p → q
and
h2 : q → p
.  The expression
Iff.mp h
produces a proof of
p → q
from
h : p ↔ q
. Similarly,
Iff.mpr h
produces a proof of
q → p
from
h : p ↔ q
. Here is a proof
of
p ∧ q ↔ q ∧ p
:
variable (p q : Prop)

theorem and_swap : p ∧ q ↔ q ∧ p :=
  Iff.intro
    (fun h : p ∧ q =>
     show q ∧ p from And.intro (And.right h) (And.left h))
    (fun h : q ∧ p =>
     show p ∧ q from And.intro (And.right h) (And.left h))

#check and_swap p q    -- p ∧ q ↔ q ∧ p

variable (h : p ∧ q)
example : q ∧ p := Iff.mp (and_swap p q) h
We can use the anonymous constructor notation to construct a proof of
p ↔ q
from proofs of the forward and backward directions, and we
can also use
.
notation with
mp
and
mpr
. The previous
examples can therefore be written concisely as follows:
variable (p q : Prop)

theorem and_swap : p ∧ q ↔ q ∧ p :=
  ⟨ fun h => ⟨h.right, h.left⟩, fun h => ⟨h.right, h.left⟩ ⟩

example (h : p ∧ q) : q ∧ p := (and_swap p q).mp h
Introducing Auxiliary Subgoals
This is a good place to introduce another device Lean offers to help
structure long proofs, namely, the
have
construct, which
introduces an auxiliary subgoal in a proof. Here is a small example,
adapted from the last section:
variable (p q : Prop)

example (h : p ∧ q) : q ∧ p :=
  have hp : p := h.left
  have hq : q := h.right
  show q ∧ p from And.intro hq hp
Internally, the expression
have h : p := s; t
produces the term
(fun (h : p) => t) s
. In other words,
s
is a proof of
p
,
t
is a proof of the desired conclusion assuming
h : p
, and the
two are combined by a lambda abstraction and application. This simple
device is extremely useful when it comes to structuring long proofs,
since we can use intermediate
have
's as stepping stones leading to
the final goal.
Lean also supports a structured way of reasoning backwards from a
goal, which models the "suffices to show" construction in ordinary
mathematics. The next example simply permutes the last two lines in
the previous proof.
variable (p q : Prop)

example (h : p ∧ q) : q ∧ p :=
  have hp : p := h.left
  suffices hq : q from And.intro hq hp
  show q from And.right h
Writing
suffices hq : q
leaves us with two goals. First, we have
to show that it indeed suffices to show
q
, by proving the original
goal of
q ∧ p
with the additional hypothesis
hq : q
. Finally,
we have to show
q
.
Classical Logic
The introduction and elimination rules we have seen so far are all
constructive, which is to say, they reflect a computational
understanding of the logical connectives based on the
propositions-as-types correspondence. Ordinary classical logic adds to
this the law of the excluded middle,
p ∨ ¬p
. To use this
principle, you have to open the classical namespace.
open Classical

variable (p : Prop)
#check em p
Intuitively, the constructive "Or" is very strong: asserting
p ∨ q
amounts to knowing which is the case. If
RH
represents the Riemann
hypothesis, a classical mathematician is willing to assert
RH ∨ ¬RH
, even though we cannot yet assert either disjunct.
One consequence of the law of the excluded middle is the principle of
double-negation elimination:
open Classical

theorem dne {p : Prop} (h : ¬¬p) : p :=
  Or.elim (em p)
    (fun hp : p => hp)
    (fun hnp : ¬p => absurd hnp h)
Double-negation elimination allows one to prove any proposition,
p
, by assuming
¬p
and deriving
false
, because that amounts
to proving
¬¬p
. In other words, double-negation elimination allows
one to carry out a proof by contradiction, something which is not
generally possible in constructive logic. As an exercise, you might
try proving the converse, that is, showing that
em
can be proved
from
dne
.
The classical axioms also give you access to additional patterns of
proof that can be justified by appeal to
em
.  For example, one can
carry out a proof by cases:
open Classical
variable (p : Prop)

example (h : ¬¬p) : p :=
  byCases
    (fun h1 : p => h1)
    (fun h1 : ¬p => absurd h1 h)
Or you can carry out a proof by contradiction:
open Classical
variable (p : Prop)

example (h : ¬¬p) : p :=
  byContradiction
    (fun h1 : ¬p =>
     show False from h h1)
If you are not used to thinking constructively, it may take some time
for you to get a sense of where classical reasoning is used.  It is
needed in the following example because, from a constructive
standpoint, knowing that
p
and
q
are not both true does not
necessarily tell you which one is false:
open Classical
variable (p q : Prop)
example (h : ¬(p ∧ q)) : ¬p ∨ ¬q :=
  Or.elim (em p)
    (fun hp : p =>
      Or.inr
        (show ¬q from
          fun hq : q =>
          h ⟨hp, hq⟩))
    (fun hp : ¬p =>
      Or.inl hp)
We will see later that there
are
situations in constructive logic
where principles like excluded middle and double-negation elimination
are permissible, and Lean supports the use of classical reasoning in
such contexts without relying on excluded middle.
The full list of axioms that are used in Lean to support classical
reasoning are discussed in
Axioms and Computation
.
Examples of Propositional Validities
Lean's standard library contains proofs of many valid statements of
propositional logic, all of which you are free to use in proofs of
your own. The following list includes a number of common identities.
Commutativity:
p ∧ q ↔ q ∧ p
p ∨ q ↔ q ∨ p
Associativity:
(p ∧ q) ∧ r ↔ p ∧ (q ∧ r)
(p ∨ q) ∨ r ↔ p ∨ (q ∨ r)
Distributivity:
p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r)
p ∨ (q ∧ r) ↔ (p ∨ q) ∧ (p ∨ r)
Other properties:
(p → (q → r)) ↔ (p ∧ q → r)
((p ∨ q) → r) ↔ (p → r) ∧ (q → r)
¬(p ∨ q) ↔ ¬p ∧ ¬q
¬p ∨ ¬q → ¬(p ∧ q)
¬(p ∧ ¬p)
p ∧ ¬q → ¬(p → q)
¬p → (p → q)
(¬p ∨ q) → (p → q)
p ∨ False ↔ p
p ∧ False ↔ False
¬(p ↔ ¬p)
(p → q) → (¬q → ¬p)
These require classical reasoning:
(p → r ∨ s) → ((p → r) ∨ (p → s))
¬(p ∧ q) → ¬p ∨ ¬q
¬(p → q) → p ∧ ¬q
(p → q) → (¬p ∨ q)
(¬q → ¬p) → (p → q)
p ∨ ¬p
(((p → q) → p) → p)
The
sorry
identifier magically produces a proof of anything, or
provides an object of any data type at all. Of course, it is unsound
as a proof method -- for example, you can use it to prove
False
--
and Lean produces severe warnings when files use or import theorems
which depend on it. But it is very useful for building long proofs