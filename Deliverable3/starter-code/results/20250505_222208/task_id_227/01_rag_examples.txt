2. Basics — Mathematics in Lean 0.1 documentation
Mathematics in Lean
1. Introduction
2. Basics
2.1. Calculating
2.2. Proving Identities in Algebraic Structures
2.3. Using Theorems and Lemmas
2.4. More examples using apply and rw
2.5. Proving Facts about Algebraic Structures
3. Logic
4. Sets and Functions
5. Elementary Number Theory
6. Structures
7. Hierarchies
8. Groups and Rings
9. Linear algebra
10. Topology
11. Differential Calculus
12. Integration and Measure Theory
Index
Mathematics in Lean
2.
Basics
View page source
2.
Basics

This chapter is designed to introduce you to the nuts and
bolts of mathematical reasoning in Lean: calculating,
applying lemmas and theorems,
and reasoning about generic structures.
2.1.
Calculating

We generally learn to carry out mathematical calculations
without thinking of them as proofs.
But when we justify each step in a calculation,
as Lean requires us to do,
the net result is a proof that the left-hand side of the calculation
is equal to the right-hand side.
In Lean, stating a theorem is tantamount to stating a goal,
namely, the goal of proving the theorem.
Lean provides the rewriting tactic
rw
,
to replace the left-hand side of an identity by the right-hand side
in the goal. If
a
,
b
, and
c
are real numbers,
mul_assoc
a
b
c
is the identity
a
*
b
*
c
=
a
*
(b
*
c)
and
mul_comm
a
b
is the identity
a
*
b
=
b
*
a
.
Lean provides automation that generally eliminates the need
to refer the facts like these explicitly,
but they are useful for the purposes of illustration.
In Lean, multiplication associates to the left,
so the left-hand side of
mul_assoc
could also be written
(a
*
b)
*
c
.
However, it is generally good style to be mindful of Lean’s
notational conventions and leave out parentheses when Lean does as well.
Let’s try out
rw
.
example
(
a
b
c
:
ℝ
)
:
a
*
b
*
c
=
b
*
(
a
*
c
)
:=
by
rw
[
mul_comm
a
b
]
rw
[
mul_assoc
b
a
c
]
The
import
lines at the beginning of the associated examples file
import the theory of the real numbers from Mathlib, as well as useful automation.
For the sake of brevity,
we generally suppress information like this in the textbook.
You are welcome to make changes to see what happens.
You can type the
ℝ
character as
\R
or
\real
in VS Code.
The symbol doesn’t appear until you hit space or the tab key.
If you hover over a symbol when reading a Lean file,
VS Code will show you the syntax that can be used to enter it.
If you are curious to see all available abbreviations, you can hit Ctrl-Shift-P
and then type abbreviations to get access to the
Lean
4:
Show
Unicode
Input
Abbreviations
command.
If your keyboard does not have an easily accessible backslash,
you can change the leading character by changing the
lean4.input.leader
setting.
When a cursor is in the middle of a tactic proof,
Lean reports on the current
proof state
in the
Lean Infoview
window.
As you move your cursor past each step of the proof,
you can see the state change.
A typical proof state in Lean might look as follows:
1
goal
x
y
:
ℕ
,
h₁
:
Prime
x
,
h₂
:
¬
Even
x
,
h₃
:
y
>
x
⊢
y
≥
4
The lines before the one that begins with
⊢
denote the
context
:
they are the objects and assumptions currently at play.
In this example, these include two objects,
x
and
y
,
each a natural number.
They also include three assumptions,
labelled
h₁
,
h₂
, and
h₃
.
In Lean, everything in a context is labelled with an identifier.
You can type these subscripted labels as
h\1
,
h\2
, and
h\3
,
but any legal identifiers would do:
you can use
h1
,
h2
,
h3
instead,
or
foo
,
bar
, and
baz
.
The last line represents the
goal
,
that is, the fact to be proved.
Sometimes people use
target
for the fact to be proved,
and
goal
for the combination of the context and the target.
In practice, the intended meaning is usually clear.
Try proving these identities,
in each case replacing
sorry
by a tactic proof.
With the
rw
tactic, you can use a left arrow (
\l
)
to reverse an identity.
For example,
rw
[←
mul_assoc
a
b
c]
replaces
a
*
(b
*
c)
by
a
*
b
*
c
in the current goal. Note that
the left-pointing arrow refers to going from right to left in the identity provided
by
mul_assoc
, it has nothing to do with the left or right side of the goal.
example
(
a
b
c
:
ℝ
)
:
c
*
b
*
a
=
b
*
(
a
*
c
)
:=
by
sorry
example
(
a
b
c
:
ℝ
)
:
a
*
(
b
*
c
)
=
b
*
(
a
*
c
)
:=
by
sorry
You can also use identities like
mul_assoc
and
mul_comm
without arguments.
In this case, the rewrite tactic tries to match the left-hand side with
an expression in the goal,
using the first pattern it finds.
example
(
a
b
c
:
ℝ
)
:
a
*
b
*
c
=
b
*
c
*
a
:=
by
rw
[
mul_assoc
]
rw
[
mul_comm
]
You can also provide
partial
information.
For example,
mul_comm
a
matches any pattern of the form
a
*
?
and rewrites it to
?
*
a
.
Try doing the first of these examples without
providing any arguments at all,
and the second with only one argument.
example
(
a
b
c
:
ℝ
)
:
a
*
(
b
*
c
)
=
b
*
(
c
*
a
)
:=
by
sorry
example
(
a
b
c
:
ℝ
)
:
a
*
(
b
*
c
)
=
b
*
(
a
*
c
)
:=
by
sorry
You can also use
rw
with facts from the local context.
example
(
a
b
c
d
e
f
:
ℝ
)
(
h
:
a
*
b
=
c
*
d
)
(
h'
:
e
=
f
)
:
a
*
(
b
*
e
)
=
c
*
(
d
*
f
)
:=
by
rw
[
h'
]
rw
[
←
mul_assoc
]
rw
[
h
]
rw
[
mul_assoc
]
Try these, using the theorem
sub_self
for the second one:
example
(
a
b
c
d
e
f
:
ℝ
)
(
h
:
b
*
c
=
e
*
f
)
:
a
*
b
*
c
*
d
=
a
*
e
*
f
*
d
:=
by
sorry
example
(
a
b
c
d
:
ℝ
)
(
hyp
:
c
=
b
*
a
-
d
)
(
hyp'
:
d
=
a
*
b
)
:
c
=
0
:=
by
sorry
Multiple rewrite commands can be carried out with a single command,
by listing the relevant identities separated by commas inside the square brackets.
example
(
a
b
c
d
e
f
:
ℝ
)
(
h
:
a
*
b
=
c
*
d
)
(
h'
:
e
=
f
)
:
a
*
(
b
*
e
)
=
c
*
(
d
*
f
)
:=
by
rw
[
h'
,
←
mul_assoc
,
h
,
mul_assoc
]
You still see the incremental progress by placing the cursor after
a comma in any list of rewrites.
Another trick is that we can declare variables once and for all outside
an example or theorem. Lean then includes them automatically.
variable
(
a
b
c
d
e
f
:
ℝ
)
example
(
h
:
a
*
b
=
c
*
d
)
(
h'
:
e
=
f
)
:
a
*
(
b
*
e
)
=
c
*
(
d
*
f
)
:=
by
rw
[
h'
,
←
mul_assoc
,
h
,
mul_assoc
]
Inspection of the tactic state at the beginning of the above proof
reveals that Lean indeed included all variables.
We can delimit the scope of the declaration by putting it
in a
section
...
end
block.
Finally, recall from the introduction that Lean provides us with a
command to determine the type of an expression:
section
variable
(
a
b
c
:
ℝ
)
#check
a
#check
a
+
b
#check
(
a
:
ℝ
)
#check
mul_comm
a
b
#check
(
mul_comm
a
b
:
a
*
b
=
b
*
a
)
#check
mul_assoc
c
a
b
#check
mul_comm
a
#check
mul_comm
end
The
#check
command works for both objects and facts.
In response to the command
#check
a
, Lean reports that
a
has type
ℝ
.
In response to the command
#check
mul_comm
a
b
,
Lean reports that
mul_comm
a
b
is a proof of the fact
a
*
b
=
b
*
a
.
The command
#check
(a
:
ℝ)
states our expectation that the
type of
a
is
ℝ
,
and Lean will raise an error if that is not the case.
We will explain the output of the last three
#check
commands later,
but in the meanwhile, you can take a look at them,
and experiment with some
#check
commands of your own.
Let’s try some more examples. The theorem
two_mul
a
says
that
2
*
a
=
a
+
a
. The theorems
add_mul
and
mul_add
express the distributivity of multiplication over addition,
and the theorem
add_assoc
expresses the associativity of addition.
Use the
#check
command to see the precise statements.
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
rw
[
mul_add
,
add_mul
,
add_mul
]
rw
[
←
add_assoc
,
add_assoc
(
a
*
a
)]
rw
[
mul_comm
b
a
,
←
two_mul
]
Whereas it is possible to figure out what is going on in this proof
by stepping through it in the editor,
it is hard to read on its own.
Lean provides a more structured way of writing proofs like this
using the
calc
keyword.
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
calc
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
b
*
a
+
(
a
*
b
+
b
*
b
)
:=
by
rw
[
mul_add
,
add_mul
,
add_mul
]
_
=
a
*
a
+
(
b
*
a
+
a
*
b
)
+
b
*
b
:=
by
rw
[
←
add_assoc
,
add_assoc
(
a
*
a
)]
_
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
rw
[
mul_comm
b
a
,
←
two_mul
]
Notice that the proof does
not
begin with
by
:
an expression that begins with
calc
is a
proof term
.
A
calc
expression can also be used inside a tactic proof,
but Lean interprets it as the instruction to use the resulting
proof term to solve the goal.
The
calc
syntax is finicky: the underscores and justification
have to be in the format indicated above.
Lean uses indentation to determine things like where a block
of tactics or a
calc
block begins and ends;
try changing the indentation in the proof above to see what happens.
One way to write a
calc
proof is to outline it first
using the
sorry
tactic for justification,
make sure Lean accepts the expression modulo these,
and then justify the individual steps using tactics.
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
calc
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
b
*
a
+
(
a
*
b
+
b
*
b
)
:=
by
sorry
_
=
a
*
a
+
(
b
*
a
+
a
*
b
)
+
b
*
b
:=
by
sorry
_
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
sorry
Try proving the following identity using both a pure
rw
proof
and a more structured
calc
proof:
example
:
(
a
+
b
)
*
(
c
+
d
)
=
a
*
c
+
a
*
d
+
b
*
c
+
b
*
d
:=
by
sorry
The following exercise is a little more challenging.
You can use the theorems listed underneath.
example
(
a
b
:
ℝ
)
:
(
a
+
b
)
*
(
a
-
b
)
=
a
^
2
-
b
^
2
:=
by
sorry
#check
pow_two
a
#check
mul_sub
a
b
c
#check
add_mul
a
b
c
#check
add_sub
a
b
c
#check
sub_sub
a
b
c
#check
add_zero
a
We can also perform rewriting in an assumption in the context.
For example,
rw
[mul_comm
a
b]
at
hyp
replaces
a
*
b
by
b
*
a
in the assumption
hyp
.
example
(
a
b
c
d
:
ℝ
)
(
hyp
:
c
=
d
*
a
+
b
)
(
hyp'
:
b
=
a
*
d
)
:
c
=
2
*
a
*
d
:=
by
rw
[
hyp'
]
at
hyp
rw
[
mul_comm
d
a
]
at
hyp
rw
[
←
two_mul
(
a
*
d
)]
at
hyp
rw
[
←
mul_assoc
2
a
d
]
at
hyp
exact
hyp
In the last step, the
exact
tactic can use
hyp
to solve the goal
because at that point
hyp
matches the goal exactly.
We close this section by noting that Mathlib provides a
useful bit of automation with a
ring
tactic,
which is designed to prove identities in any commutative ring as long as they follow
purely from the ring axioms, without using any local assumption.
example
:
c
*
b
*
a
=
b
*
(
a
*
c
)
:=
by
ring
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
ring
example
:
(
a
+
b
)
*
(
a
-
b
)
=
a
^
2
-
b
^
2
:=
by
ring
example
(
hyp
:
c
=
d
*
a
+
b
)
(
hyp'
:
b
=
a
*
d
)
:
c
=
2
*
a
*
d
:=
by
rw
[
hyp
,
hyp'
]
ring
The
ring
tactic is imported indirectly when we
import
Mathlib.Data.Real.Basic
,
but we will see in the next section that it can be used
for calculations on structures other than the real numbers.
It can be imported explicitly with the command
import
Mathlib.Tactic
.
We will see there are similar tactics for other common kind of algebraic
structures.
There is a variation of
rw
called
nth_rw
that allows you to replace only particular instances of an expression in the goal.
Possible matches are enumerated starting with 1,
so in the following example,
nth_rw
2
[h]
replaces the second
occurrence of
a
+
b
with
c
.
example
(
a
b
c
:
ℕ
)
(
h
:
a
+
b
=
c
)
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
c
+
b
*
c
:=
by
nth_rw
2
[
h
]
rw
[
add_mul
]
2.2.
Proving Identities in Algebraic Structures

Mathematically, a ring consists of a collection of objects,
\(R\)
, operations
\(+\)
\(\times\)
, and constants
\(0\)
and
\(1\)
, and an operation
\(x \mapsto -x\)
such that:
\(R\)
with
\(+\)
is an
abelian group
, with
\(0\)
as the additive identity and negation as inverse.
Multiplication is associative with identity
\(1\)
,
and multiplication distributes over addition.
In Lean, the collection of objects is represented as a
type
,
R
.
The ring axioms are as follows:
variable
(
R
:
Type
*
)
[
Ring
R
]
#check
(
add_assoc
:
∀
a
b
c
:
R
,
a
+
b
+
c
=
a
+
(
b
+
c
))
#check
(
add_comm
:
∀
a
b
:
R
,
a
+
b
=
b
+
a
)
#check
(
zero_add
:
∀
a
:
R
,
0
+
a
=
a
)
#check
(
neg_add_cancel
:
∀
a
:
R
,
-
a
+
a
=
0
)
#check
(
mul_assoc
:
∀
a
b
c
:
R
,
a
*
b
*
c
=
a
*
(
b
*
c
))
#check
(
mul_one
:
∀
a
:
R
,
a
*
1
=
a
)
#check
(
one_mul
:
∀
a
:
R
,
1
*
a
=
a
)
#check
(
mul_add
:
∀
a
b
c
:
R
,
a
*
(
b
+
c
)
=
a
*
b
+
a
*
c
)
#check
(
add_mul
:
∀
a
b
c
:
R
,
(
a
+
b
)
*
c
=
a
*
c
+
b
*
c
)
You will learn more about the square brackets in the first line later,
but for the time being,
suffice it to say that the declaration gives us a type,
R
,
and a ring structure on
R
.
Lean then allows us to use generic ring notation with elements of
R
,
and to make use of a library of theorems about rings.
The names of some of the theorems should look familiar:
they are exactly the ones we used to calculate with the real numbers
in the last section.
Lean is good not only for proving things about concrete mathematical
structures like the natural numbers and the integers,
but also for proving things about abstract structures,
characterized axiomatically, like rings.
Moreover, Lean supports
generic reasoning
about
both abstract and concrete structures,
and can be trained to recognize appropriate instances.
So any theorem about rings can be applied to concrete rings
like the integers,
ℤ
, the rational numbers,
ℚ
,
and the complex numbers
ℂ
.
It can also be applied to any instance of an abstract
structure that extends rings,
such as any ordered ring or any field.
Not all important properties of the real numbers hold in an
arbitrary ring, however.
For example, multiplication on the real numbers
is commutative,
but that does not hold in general.
If you have taken a course in linear algebra,
you will recognize that, for every
\(n\)
,
the
\(n\)
by
\(n\)
matrices of real numbers
form a ring in which commutativity usually fails. If we declare
R
to be a
commutative
ring, in fact, all the theorems
in the last section continue to hold when we replace
ℝ
by
R
.
variable
(
R
:
Type
*
)
[
CommRing
R
]
variable
(
a
b
c
d
:
R
)
example
:
c
*
b
*
a
=
b
*
(
a
*
c
)
:=
by
ring
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
ring
example
:
(
a
+
b
)
*
(
a
-
b
)
=
a
^
2
-
b
^
2
:=
by
ring
example
(
hyp
:
c
=
d
*
a
+
b
)
(
hyp'
:
b
=
a
*
d
)
:
c
=
2
*
a
*
d
:=
by
rw
[
hyp
,
hyp'
]
ring
We leave it to you to check that all the other proofs go through unchanged.
Notice that when a proof is short, like
by
ring
or
by
linarith
or
by
sorry
,
it is common (and permissible) to put it on the same line as
the
by
.
Good proof-writing style should strike a balance between concision and readability.
The goal of this section is to strengthen the skills
you have developed in the last section
and apply them to reasoning axiomatically about rings.
We will start with the axioms listed above,
and use them to derive other facts.
Most of the facts we prove are already in Mathlib.
We will give the versions we prove the same names
to help you learn the contents of the library
as well as the naming conventions.
Lean provides an organizational mechanism similar
to those used in programming languages:
when a definition or theorem
foo
is introduced in a
namespace
bar
, its full name is
bar.foo
.
The command
open
bar
later
opens
the namespace,
which allows us to use the shorter name
foo
.
To avoid errors due to name clashes,
in the next example we put our versions of the library
theorems in a new namespace called
MyRing.
The next example shows that we do not need
add_zero
or
add_neg_cancel
as ring axioms, because they follow from the other axioms.
namespace
MyRing
variable
{
R
:
Type
*
}
[
Ring
R
]
theorem
add_zero
(
a
:
R
)
:
a
+
0
=
a
:=
by
rw
[
add_comm
,
zero_add
]
theorem
add_neg_cancel
(
a
:
R
)
:
a
+
-
a
=
0
:=
by
rw
[
add_comm
,
neg_add_cancel
]
#check
MyRing.add_zero
#check
add_zero
end
MyRing
The net effect is that we can temporarily reprove a theorem in the library,
and then go on using the library version after that.
But don’t cheat!
In the exercises that follow, take care to use only the
general facts about rings that we have proved earlier in this section.
(If you are paying careful attention, you may have noticed that we
changed the round brackets in
(R
:
Type*)
for
curly brackets in
{R
:
Type*}
.
This declares
R
to be an
implicit argument
.
We will explain what this means in a moment,
but don’t worry about it in the meanwhile.)
Here is a useful theorem:
theorem
neg_add_cancel_left
(
a
b
:
R
)
:
-
a
+
(
a
+
b
)
=
b
:=
by
rw
[
←
add_assoc
,
neg_add_cancel
,
zero_add
]
Prove the companion version:
theorem
add_neg_cancel_right
(
a
b
:
R
)
:
a
+
b
+
-
b
=
a
:=
by
sorry
Use these to prove the following:
theorem
add_left_cancel
{
a
b
c
:
R
}
(
h
:
a
+
b
=
a
+
c
)
:
b
=
c
:=
by
sorry
theorem
add_right_cancel
{
a
b
c
:
R
}
(
h
:
a
+
b
=
c
+
b
)
:
a
=
c
:=
by
sorry
With enough planning, you can do each of them with three rewrites.
We will now explain the use of the curly braces.
Imagine you are in a situation where you have
a
,
b
, and
c
in your context,
as well as a hypothesis
h
:
a
+
b
=
a
+
c
,
and you would like to draw the conclusion
b
=
c
.
In Lean, you can apply a theorem to hypotheses and facts just
the same way that you can apply them to objects,
so you might think that
add_left_cancel
a
b
c
h
is a
proof of the fact
b
=
c
.
But notice that explicitly writing
a
,
b
, and
c
is redundant, because the hypothesis
h
makes it clear that
those are the objects we have in mind.
In this case, typing a few extra characters is not onerous,
but if we wanted to apply
add_left_cancel
to more complicated expressions,
writing them would be tedious.
In cases like these,
Lean allows us to mark arguments as
implicit
,
meaning that they are supposed to be left out and inferred by other means,
such as later arguments and hypotheses.
The curly brackets in
{a
b
c
:
R}
do exactly that.
So, given the statement of the theorem above,
the correct expression is simply
add_left_cancel
h
.
To illustrate, let us show that
a
*
0
=
0
follows from the ring axioms.
theorem
mul_zero
(
a
:
R
)
:
a
*
0
=
0
:=
by
have
h
:
a
*
0
+
a
*
0
=
a
*
0
+
0
:=
by
rw
[
←
mul_add
,
add_zero
,
add_zero
]
rw
[
add_left_cancel
h
]
We have used a new trick!
If you step through the proof,
you can see what is going on.
The
have
tactic introduces a new goal,
a
*
0
+
a
*
0
=
a
*
0
+
0
,
with the same context as the original goal.
The fact that the next line is indented indicates that Lean
is expecting a block of tactics that serves to prove this
new goal.
The indentation therefore promotes a modular style of proof:
the indented subproof establishes the goal
that was introduced by the
have
.
After that, we are back to proving the original goal,
except a new hypothesis
h
has been added:
having proved it, we are now free to use it.
At this point, the goal is exactly the result of
add_left_cancel
h
.
We could equally well have closed the proof with
apply
add_left_cancel
h
or
exact
add_left_cancel
h
.
The
exact
tactic takes as argument a proof term which completely proves the
current goal, without creating any new goal. The
apply
tactic is a variant
whose argument is not necessarily a complete proof. The missing pieces are either
inferred automatically by Lean or become new goals to prove.
While the
exact
tactic is technically redundant since it is strictly less powerful
than
apply
, it makes proof scripts slightly clearer to
human readers and easier to maintain when the library evolves.
Remember that multiplication is not assumed to be commutative,
so the following theorem also requires some work.
theorem
zero_mul
(
a
:
R
)
:
0
*
a
=
0
:=
by
sorry
By now, you should also be able replace each
sorry
in the next
exercise with a proof,
still using only facts about rings that we have
established in this section.
theorem
neg_eq_of_add_eq_zero
{
a
b
:
R
}
(
h
:
a
+
b
=
0
)
:
-
a
=
b
:=
by
sorry
theorem
eq_neg_of_add_eq_zero
{
a
b
:
R
}
(
h
:
a
+
b
=
0
)
:
a
=
-
b
:=
by
sorry
theorem
neg_zero
:
(
-
0
:
R
)
=
0
:=
by
apply
neg_eq_of_add_eq_zero
rw
[
add_zero
]
theorem
neg_neg
(
a
:
R
)
:
-
-
a
=
a
:=
by
sorry
We had to use the annotation
(-0
:
R)
instead of
0
in the third theorem
because without specifying
R
it is impossible for Lean to infer which
0
we have in mind,
and by default it would be interpreted as a natural number.
In Lean, subtraction in a ring is provably equal to
addition of the additive inverse.
example
(
a
b
:
R
)
:
a
-
b
=
a
+
-
b
:=
sub_eq_add_neg
a
b
On the real numbers, it is
defined
that way:
example
(
a
b
:
ℝ
)
:
a
-
b
=
a
+
-
b
:=
rfl
example
(
a
b
:
ℝ
)
:
a
-
b
=
a
+
-
b
:=
by
rfl
The proof term
rfl
is short for “reflexivity”.
Presenting it as a proof of
a
-
b
=
a
+
-b
forces Lean
to unfold the definition and recognize both sides as being the same.
The
rfl
tactic does the same.
This is an instance of what is known as a
definitional equality
in Lean’s underlying logic.
This means that not only can one rewrite with
sub_eq_add_neg
to replace
a
-
b
=
a
+
-b
,
but in some contexts, when dealing with the real numbers,
you can use the two sides of the equation interchangeably.
For example, you now have enough information to prove the theorem
self_sub
from the last section:
theorem
self_sub
(
a
:
R
)
:
a
-
a
=
0
:=
by
sorry
Show that you can prove this using
rw
,
but if you replace the arbitrary ring
R
by
the real numbers, you can also prove it
using either
apply
or
exact
.
Lean knows that
1
+
1
=
2
holds in any ring.
With a bit of effort,
you can use that to prove the theorem
two_mul
from
the last section:
theorem
one_add_one_eq_two
:
1
+
1
=
(
2
:
R
)
:=
by
norm_num
theorem
two_mul
(
a
:
R
)
:
2
*
a
=
a
+
a
:=
by
sorry
We close this section by noting that some of the facts about
addition and negation that we established above do not
need the full strength of the ring axioms, or even
commutativity of addition. The weaker notion of a
group
can be axiomatized as follows:
variable
(
A
:
Type
*
)
[
AddGroup
A
]
#check
(
add_assoc
:
∀
a
b
c
:
A
,
a
+
b
+
c
=
a
+
(
b
+
c
))
#check
(
zero_add
:
∀
a
:
A
,
0
+
a
=
a
)
#check
(
neg_add_cancel
:
∀
a
:
A
,
-
a
+
a
=
0
)
It is conventional to use additive notation when
the group operation is commutative,
and multiplicative notation otherwise.
So Lean defines a multiplicative version as well as the
additive version (and also their abelian variants,
AddCommGroup
and
CommGroup
).
variable
{
G
:
Type
*
}
[
Group
G
]
#check
(
mul_assoc
:
∀
a
b
c
:
G
,
a
*
b
*
c
=
a
*
(
b
*
c
))
#check
(
one_mul
:
∀
a
:
G
,
1
*
a
=
a
)
#check
(
inv_mul_cancel
:
∀
a
:
G
,
a
⁻¹
*
a
=
1
)
If you are feeling cocky, try proving the following facts about
groups, using only these axioms.
You will need to prove a number of helper lemmas along the way.
The proofs we have carried out in this section provide some hints.
theorem
mul_inv_cancel
(
a
:
G
)
:
a
*
a
⁻¹
=
1
:=
by
sorry
theorem
mul_one
(
a
:
G
)
:
a
*
1
=
a
:=
by
sorry
theorem
mul_inv_rev
(
a
b
:
G
)
:
(
a
*
b
)
⁻¹
=
b
⁻¹
*
a
⁻¹
:=
by
sorry
Explicitly invoking those lemmas is tedious, so Mathlib provides
tactics similar to
ring
in order to cover most uses:
group
is for non-commutative multiplicative groups,
abel
for abelian
additive groups, and
noncomm_ring
for non-commutative rings.
It may seem odd that the algebraic structures are called
Ring
and
CommRing
while the tactics are named
noncomm_ring
and
ring
. This is partly for historical reasons,
but also for the convenience of using a shorter name for the
tactic that deals with commutative rings, since it is used more often.
2.3.
Using Theorems and Lemmas

Rewriting is great for proving equations,
but what about other sorts of theorems?
For example, how can we prove an inequality,
like the fact that
\(a + e^b \le a + e^c\)
holds whenever
\(b \le c\)
?
We have already seen that theorems can be applied to arguments and hypotheses,
and that the
apply
and
exact
tactics can be used to solve goals.
In this section, we will make good use of these tools.
Consider the library theorems
le_refl
and
le_trans
:
#check
(
le_refl
:
∀
a
:
ℝ
,
a
≤
a
)
#check
(
le_trans
:
a
≤
b
→
b
≤
c
→
a
≤
c
)
As we explain in more detail in
Section 3.1
,
the implicit parentheses in the statement of
le_trans
associate to the right, so it should be interpreted as
a
≤
b
→
(b
≤
c
→
a
≤
c)
.
The library designers have set the arguments
a
,
b
and
c
to
le_trans
implicit,
so that Lean will
not
let you provide them explicitly (unless you
really insist, as we will discuss later).
Rather, it expects to infer them from the context in which they are used.
For example, when hypotheses

ifier.
But it can’t hurt to see the alternative syntax, especially if there is
a chance you will find yourself in the company of computer scientists.
To illustrate one way that
rcases
can be used,
we prove an old mathematical chestnut:
if two integers
x
and
y
can each be written as
a sum of two squares,
then so can their product,
x
*
y
.
In fact, the statement is true for any commutative
ring, not just the integers.
In the next example,
rcases
unpacks two existential
quantifiers at once.
We then provide the magic values needed to express
x
*
y
as a sum of squares as a list to the
use
statement,
and we use
ring
to verify that they work.
variable
{
α
:
Type
*
}
[
CommRing
α
]
def
SumOfSquares
(
x
:
α
)
:=
∃
a
b
,
x
=
a
^
2
+
b
^
2
theorem
sumOfSquares_mul
{
x
y
:
α
}
(
sosx
:
SumOfSquares
x
)
(
sosy
:
SumOfSquares
y
)
:
SumOfSquares
(
x
*
y
)
:=
by
rcases
sosx
with
⟨
a
,
b
,
xeq
⟩
rcases
sosy
with
⟨
c
,
d
,
yeq
⟩
rw
[
xeq
,
yeq
]
use
a
*
c
-
b
*
d
,
a
*
d
+
b
*
c
ring
This proof doesn’t provide much insight,
but here is one way to motivate it.
A
Gaussian integer
is a number of the form
\(a + bi\)
where
\(a\)
and
\(b\)
are integers and
\(i = \sqrt{-1}\)
.
The
norm
of the Gaussian integer
\(a + bi\)
is, by definition,
\(a^2 + b^2\)
.
So the norm of a Gaussian integer is a sum of squares,
and any sum of squares can be expressed in this way.
The theorem above reflects the fact that norm of a product of
Gaussian integers is the product of their norms:
if
\(x\)
is the norm of
\(a + bi\)
and
\(y\)
in the norm of
\(c + di\)
,
then
\(xy\)
is the norm of
\((a + bi) (c + di)\)
.
Our cryptic proof illustrates the fact that
the proof that is easiest to formalize isn’t always
the most perspicuous one.
In
Section 6.3
,
we will provide you with the means to define the Gaussian
integers and use them to provide an alternative proof.
The pattern of unpacking an equation inside an existential quantifier
and then using it to rewrite an expression in the goal
comes up often,
so much so that the
rcases
tactic provides
an abbreviation:
if you use the keyword
rfl
in place of a new identifier,
rcases
does the rewriting automatically (this trick doesn’t work
with pattern-matching lambdas).
theorem
sumOfSquares_mul'
{
x
y
:
α
}
(
sosx
:
SumOfSquares
x
)
(
sosy
:
SumOfSquares
y
)
:
SumOfSquares
(
x
*
y
)
:=
by
rcases
sosx
with
⟨
a
,
b
,
rfl
⟩
rcases
sosy
with
⟨
c
,
d
,
rfl
⟩
use
a
*
c
-
b
*
d
,
a
*
d
+
b
*
c
ring
As with the universal quantifier,
you can find existential quantifiers hidden all over
if you know how to spot them.
For example, divisibility is implicitly an “exists” statement.
example
(
divab
:
a
∣
b
)
(
divbc
:
b
∣
c
)
:
a
∣
c
:=
by
rcases
divab
with
⟨
d
,
beq
⟩
rcases
divbc
with
⟨
e
,
ceq
⟩
rw
[
ceq
,
beq
]
use
d
*
e
;
ring
And once again, this provides a nice setting for using
rcases
with
rfl
.
Try it out in the proof above.
It feels pretty good!
Then try proving the following:
example
(
divab
:
a
∣
b
)
(
divac
:
a
∣
c
)
:
a
∣
b
+
c
:=
by
sorry
For another important example, a function
\(f : \alpha \to \beta\)
is said to be
surjective
if for every
\(y\)
in the
codomain,
\(\beta\)
,
there is an
\(x\)
in the domain,
\(\alpha\)
,
such that
\(f(x) = y\)
.
Notice that this statement includes both a universal
and an existential quantifier, which explains
why the next example makes use of both
intro
and
use
.
example
{
c
:
ℝ
}
:
Surjective
fun
x
↦
x
+
c
:=
by
intro
x
use
x
-
c
dsimp
;
ring
Try this example yourself using the theorem
mul_div_cancel₀
.:
example
{
c
:
ℝ
}
(
h
:
c
≠
0
)
:
Surjective
fun
x
↦
c
*
x
:=
by
sorry
At this point, it is worth mentioning that there is a tactic,
field_simp
,
that will often clear denominators in a useful way.
It can be used in conjunction with the
ring
tactic.
example
(
x
y
:
ℝ
)
(
h
:
x
-
y
≠
0
)
:
(
x
^
2
-
y
^
2
)
/
(
x
-
y
)
=
x
+
y
:=
by
field_simp
[
h
]
ring
The next example uses a surjectivity hypothesis
by applying it to a suitable value.
Note that you can use
rcases
with any expression,
not just a hypothesis.
example
{
f
:
ℝ
→
ℝ
}
(
h
:
Surjective
f
)
:
∃
x
,
f
x
^
2
=
4
:=
by
rcases
h
2
with
⟨
x
,
hx
⟩
use
x
rw
[
hx
]
norm_num
See if you can use these methods to show that
the composition of surjective functions is surjective.
variable
{
α
:
Type
*
}
{
β
:
Type
*
}
{
γ
:
Type
*
}
variable
{
g
:
β
→
γ
}
{
f
:
α
→
β
}
example
(
surjg
:
Surjective
g
)
(
surjf
:
Surjective
f
)
:
Surjective
fun
x
↦
g
(
f
x
)
:=
by
sorry
3.3.
Negation

The symbol
¬
is meant to express negation,
so
¬
x
<
y
says that
x
is not less than
y
,
¬
x
=
y
(or, equivalently,
x
≠
y
) says that
x
is not equal to
y
,
and
¬
∃
z,
x
<
z
∧
z
<
y
says that there does not exist a
z
strictly between
x
and
y
.
In Lean, the notation
¬
A
abbreviates
A
→
False
,
which you can think of as saying that
A
implies a contradiction.
Practically speaking, this means that you already know something
about how to work with negations:
you can prove
¬
A
by introducing a hypothesis
h
:
A
and proving
False
,
and if you have
h
:
¬
A
and
h'
:
A
,
then applying
h
to
h'
yields
False
.
To illustrate, consider the irreflexivity principle
lt_irrefl
for a strict order,
which says that we have
¬
a
<
a
for every
a
.
The asymmetry principle
lt_asymm
says that we have
a
<
b
→
¬
b
<
a
. Let’s show that
lt_asymm
follows
from
lt_irrefl
.
example
(
h
:
a
<
b
)
:
¬
b
<
a
:=
by
intro
h'
have
:
a
<
a
:=
lt_trans
h
h'
apply
lt_irrefl
a
this
This example introduces a couple of new tricks.
First, when you use
have
without providing
a label,
Lean uses the name
this
,
providing a convenient way to refer back to it.
Because the proof is so short, we provide an explicit proof term.
But what you should really be paying attention to in this
proof is the result of the
intro
tactic,
which leaves a goal of
False
,
and the fact that we eventually prove
False
by applying
lt_irrefl
to a proof of
a
<
a
.
Here is another example, which uses the
predicate
FnHasUb
defined in the last section,
which says that a function has an upper bound.
example
(
h
:
∀
a
,
∃
x
,
f
x
>
a
)
:
¬
FnHasUb
f
:=
by
intro
fnub
rcases
fnub
with
⟨
a
,
fnuba
⟩
rcases
h
a
with
⟨
x
,
hx
⟩
have
:
f
x
≤
a
:=
fnuba
x
linarith
Remember that it is often convenient to use
linarith
when a goal follows from linear equations and
inequalities that are in the context.
See if you can prove these in a similar way:
example
(
h
:
∀
a
,
∃
x
,
f
x
<
a
)
:
¬
FnHasLb
f
:=
sorry
example
:
¬
FnHasUb
fun
x
↦
x
:=
sorry
Mathlib offers a number of useful theorems for relating orders
and negations:
#check
(
not_le_of_gt
:
a
>
b
→
¬
a
≤
b
)
#check
(
not_lt_of_ge
:
a
≥
b
→
¬
a
<
b
)
#check
(
lt_of_not_ge
:
¬
a
≥
b
→
a
<
b
)
#check
(
le_of_not_gt
:
¬
a
>
b
→
a
≤
b
)
Recall the predicate
Monotone
f
,
which says that
f
is nondecreasing.
Use some of the theorems just enumerated to prove the following:
example
(
h
:
Monotone
f
)
(
h'
:
f
a
<
f
b
)
:
a
<
b
:=
by
sorry
example
(
h
:
a
≤
b
)
(
h'
:
f
b
<
f
a
)
:
¬
Monotone
f
:=
by
sorry
We can show that the first example in the last snippet
cannot be proved if we replace
<
by
≤
.
Notice that we can prove the negation of a universally
quantified statement by giving a counterexample.
Complete the proof.
example
:
¬∀
{
f
:
ℝ
→
ℝ
},
Monotone
f
→
∀
{
a
b
},
f
a
≤
f
b
→
a
≤
b
:=
by
intro
h
let
f
:=
fun
x
:
ℝ
↦
(
0
:
ℝ
)
have
monof
:
Monotone
f
:=
by
sorry
have
h'
:
f
1
≤
f
0
:=
le_refl
_
sorry
This example introduces the
let
tactic,
which adds a
local definition
to the context.
If you put the cursor after the
let
command,
in the goal window you will see that the definition
f
:
ℝ
→
ℝ
:=
fun
x
↦
0
has been added to the context.
Lean will unfold the definition of
f
when it has to.
In particular, when we prove
f
1
≤
f
0
with
le_refl
,
Lean reduces
f
1
and
f
0
to
0
.
Use
le_of_not_gt
to prove the following:
example
(
x
:
ℝ
)
(
h
:
∀
ε
>
0
,
x
<
ε
)
:
x
≤
0
:=
by
sorry
Implicit in many of the proofs we have just done
is the fact that if
P
is any property,
saying that there is nothing with property
P
is the same as saying that everything fails to have
property
P
,
and saying that not everything has property
P
is equivalent to saying that something fails to have property
P
.
In other words, all four of the following implications
are valid (but one of them cannot be proved with what we explained so
far):
variable
{
α
:
Type
*
}
(
P
:
α
→
Prop
)
(
Q
:
Prop
)
example
(
h
:
¬∃
x
,
P
x
)
:
∀
x
,
¬
P
x
:=
by
sorry
example
(
h
:
∀
x
,
¬
P
x
)
:
¬∃
x
,
P
x
:=
by
sorry
example
(
h
:
¬∀
x
,
P
x
)
:
∃
x
,
¬
P
x
:=
by
sorry
example
(
h
:
∃
x
,
¬
P
x
)
:
¬∀
x
,
P
x
:=
by
sorry
The first, second, and fourth are straightforward to
prove using the methods you have already seen.
We encourage you to try it.
The third is more difficult, however,
because it concludes that an object exists
from the fact that its nonexistence is contradictory.
This is an instance of
classical
mathematical reasoning.
We can use proof by contradiction
to prove the third implication as follows.
example
(
h
:
¬∀
x
,
P
x
)
:
∃
x
,
¬
P
x
:=
by
by_contra
h'
apply
h
intro
x
show
P
x
by_contra
h''
exact
h'
⟨
x
,
h''
⟩
Make sure you understand how this works.
The
by_contra
tactic
allows us to prove a goal
Q
by assuming
¬
Q
and deriving a contradiction.
In fact, it is equivalent to using the
equivalence
not_not
:
¬
¬
Q
↔
Q
.
Confirm that you can prove the forward direction
of this equivalence using
by_contra
,
while the reverse direction follows from the
ordinary rules for negation.
example
(
h
:
¬¬
Q
)
:
Q
:=
by
sorry
example
(
h
:
Q
)
:
¬¬
Q
:=
by
sorry
Use proof by contradiction to establish the following,
which is the converse of one of the implications we proved above.
(Hint: use
intro
first.)
example
(
h
:
¬
FnHasUb
f
)
:
∀
a
,
∃
x
,
f
x
>
a
:=
by
sorry
It is often tedious to work with compound statements with
a negation in front,
and it is a common mathematical pattern to replace such
statements with equivalent forms in which the negation
has been pushed inward.
To facilitate this, Mathlib offers a
push_neg
tactic,
which restates the goal in this way.
The command
push_neg
at
h
restates the hypothesis
h
.
example
(
h
:
¬∀
a
,
∃
x
,
f
x
>
a
)
:
FnHasUb
f
:=
by
push_neg
at
h
exact
h
example
(
h
:
¬
FnHasUb
f
)
:
∀
a
,
∃
x
,
f
x
>
a
:=
by
dsimp
only
[
FnHasUb
,
FnUb
]
at
h
push_neg
at
h
exact
h
In the second example, we use dsimp to
expand the definitions of
FnHasUb
and
FnUb
.
(We need to use
dsimp
rather than
rw
to expand
FnUb
,
because it appears in the scope of a quantifier.)
You can verify that in the examples above
with
¬∃
x,
P
x
and
¬∀
x,
P
x
,
the
push_neg
tactic does the expected thing.
Without even knowing how to use the conjunction
symbol,
you should be able to use
push_neg
to prove the following:
example
(
h
:
¬
Monotone
f
)
:
∃
x
y
,
x
≤
y
∧
f
y
<
f
x
:=
by
sorry
Mathlib also has a tactic,
contrapose
,
which transforms a goal
A
→
B
to
¬B
→
¬A
.
Similarly, given a goal of proving
B
from
hypothesis
h
:
A
,
contrapose
h
leaves you with a goal of proving
¬A
from hypothesis
¬B
.
Using
contrapose!
instead of
contrapose
applies
push_neg
to the goal and the relevant
hypothesis as well.
example
(
h
:
¬
FnHasUb
f
)
:
∀
a
,
∃
x
,
f
x
>
a
:=
by
contrapose
!
h
exact
h
example
(
x
:
ℝ
)
(
h
:
∀
ε
>
0
,
x
≤
ε
)
:
x
≤
0
:=
by
contrapose
!
h
use
x
/
2
constructor
<;>
linarith
We have not yet explained the
constructor
command
or the use of the semicolon after it,
but we will do that in the next section.
We close this section with
the principle of
ex falso
,
which says that anything follows from a contradiction.
In Lean, this is represented by
False.elim
,
which establishes
False
→
P
for any proposition
P
.
This may seem like a strange principle,
but it comes up fairly often.
We often prove a theorem by splitting on cases,
and sometimes we can show that one of
the cases is contradictory.
In that case, we need to assert that the contradiction
establishes the goal so we can move on to the next one.
(We will see instances of reasoning by cases in
Section 3.5
.)
Lean provides a number of ways of closing
a goal once a contradiction has been reached.
example
(
h
:
0
<
0
)
:
a
>
37
:=
by
exfalso
apply
lt_irrefl
0
h
example
(
h
:
0
<
0
)
:
a
>
37
:=
absurd
h
(
lt_irrefl
0
)
example
(
h
:
0
<
0
)
:
a
>
37
:=
by
have
h'
:
¬
0
<
0
:=
lt_irrefl
0
contradiction
The
exfalso
tactic replaces the current goal with
the goal of proving
False
.
Given
h
:
P
and
h'
:
¬
P
,
the term
absurd
h
h'
establishes any proposition.
Finally, the
contradiction
tactic tries to close a goal
by finding a contradiction in the hypotheses,
such as a pair of the form
h
:
P
and
h'
:
¬
P
.
Of course, in this example,
linarith
also works.
3.4.
Conjunction and Iff

You have already seen that the conjunction symbol,
∧
,
is used to express “and.”
The
constructor
tactic allows you to prove a statement of
the form
A
∧
B
by proving
A
and then proving
B
.
example
{
x
y
:
ℝ
}
(
h₀
:
x
≤
y
)
(
h₁
:
¬
y
≤
x
)
:
x
≤
y
∧
x
≠
y
:=
by
constructor
·
assumption
intro
h
apply
h₁
rw
[
h
]
In this example, the
assumption
tactic
tells Lean to find an assumption that will solve the goal.
Notice that the final
rw
finishes the goal by
applying the reflexivity of
≤
.
The following are alternative ways of carrying out
the previous examples using the anonymous constructor
angle brackets.
The first is a slick proof-term version of the
previous proof,
which drops into tactic mode at the keyword
by
.
example
{
x
y
:
ℝ
}
(
h₀
:
x
≤
y
)
(
h₁
:
¬
y
≤
x
)
:
x
≤
y
∧
x
≠
y
:=
⟨
h₀
,
fun
h
↦
h₁
(
by
rw
[
h
])⟩
example
{
x
y
:
ℝ
}
(
h₀
:
x
≤
y
)
(
h₁
:
¬
y
≤
x
)
:
x
≤
y
∧
x
≠
y
:=
have
h
:
x
≠
y
:=
by
contrapose
!
h₁
rw
[
h₁
]
⟨
h₀
,
h
⟩
Using
a conjunction instead of proving one involves unpacking the proofs of the
two parts.
You can use the
rcases
tactic for that,
as well as
rintro
or a pattern-matching
fun
,
all in a manner similar to the way they are used with
the existential quantifier.
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
rcases
h
with
⟨
h₀
,
h₁
⟩
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
example
{
x
y
:
ℝ
}
:
x
≤
y
∧
x
≠
y
→
¬
y
≤
x
:=
by
rintro
⟨
h₀
,
h₁
⟩
h'
exact
h₁
(
le_antisymm
h₀
h'
)
example
{
x
y
:
ℝ
}
:
x
≤
y
∧
x
≠
y
→
¬
y
≤
x
:=
fun
⟨
h₀
,
h₁
⟩
h'
↦
h₁
(
le_antisymm
h₀
h'
)
In analogy to the
obtain
tactic, there is also a pattern-matching
have
:
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
have
⟨
h₀
,
h₁
⟩
:=
h
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
In contrast to
rcases
, here the
have
tactic leaves
h
in the context.
And even though we won’t use them, once again we have the computer scientists’
pattern-matching syntax:
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
cases
h
case
intro
h₀
h₁
=>
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
cases
h
next
h₀
h₁
=>
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
match
h
with
|
⟨
h₀
,
h₁
⟩
=>
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
In contrast to using an existential quantifier,
you can also extract proofs of the two components
of a hypothesis
h
:
A
∧
B
by writing
h.left
and
h.right
,
or, equivalently,
h.1
and
h.2
.
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
intro
h'
apply
h.right
exact
le_antisymm
h.left
h'
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
fun
h'
↦
h.right
(
le_antisymm
h.left
h'
)
Try using these techniques to come up with various ways of proving of the following:
example
{
m
n
:
ℕ
}
(
h
:
m
∣
n
∧
m
≠
n
)
:
m
∣
n
∧
¬
n
∣
m
:=
sorry
You can nest uses of
∃
and
∧
with anonymous constructors,
rintro
, and
rcases
.
example
:
∃
x
:
ℝ
,
2
<
x
∧
x
<
4
:=
⟨
5
/
2
,
by
norm_num
,
by
norm_num
⟩
example
(
x
y
:
ℝ
)
:
(
∃
z
:
ℝ
,
x
<
z
∧
z
<
y
)
→
x
<
y
:=
by
rintro
⟨
z
,
xltz
,
zlty
⟩
exact
lt_trans
xltz
zlty
example
(
x
y
:
ℝ
)
:
(
∃
z
:
ℝ
,
x
<
z
∧
z
<
y
)
→
x
<
y
:=
fun
⟨
z
,
xltz
,
zlty
⟩
↦
lt_trans
xltz
zlty
You can also use the
use
tactic:
example
:
∃
x
:
ℝ
,
2
<
x
∧
x
<
4
:=
by
use
5
/
2
constructor
<;>
norm_num
example
:
∃
m
n
:
ℕ
,
4
<
m
∧
m
<
n
∧
n
<
10
∧
Nat.Prime
m
∧
Nat.Prime
n
:=
by
use
5
use
7
norm_num
example
{
x
y
:
ℝ
}
:
x
≤
y
∧
x
≠
y
→
x
≤
y
∧
¬
y
≤
x
:=
by
rintro
⟨
h₀
,
h₁
⟩
use
h₀
exact
fun
h'
↦
h₁
(
le_antisymm
h₀
h'
)
In the first example, the semicolon after the
constructor
command tells Lean to use the
norm_num
tactic on both of the goals that result.
In Lean,
A
↔
B
is
not
defined to be
(A
→
B)
∧
(B
→
A)
,
but it could have been,
and it behaves roughly the same way.
You have already seen that you can write
h.mp
and
h.mpr
or
h.1
and
h.2
for the two directions of
h
:
A
↔
B
.
You can also use
cases
and friends.
To prove an if-and-only-if statement,
you can use
constructor
or angle brackets,
just as you would if you were proving a conjunction.
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
)
:
¬
y
≤
x
↔
x
≠
y
:=
by
constructor
·
contrapose
!
rintro
rfl
rfl
contrapose
!
exact
le_antisymm
h
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
)
:
¬
y
≤
x
↔
x
≠
y
:=
⟨
fun
h₀
h₁
↦
h₀
(
by
rw
[
h₁
]),
fun
h₀
h₁
↦
h₀
(
le_antisymm
h
h₁
)⟩
The last proof term is inscrutable. Remember that you can
use underscores while writing an expression like that to
see what Lean expects.
Try out the various techniques and gadgets you have just seen
in order to prove the following:
example
{
x
y
:
ℝ
}
:
x
≤
y
∧
¬
y
≤
x
↔
x
≤
y
∧
x
≠
y
:=
sorry
For a more interesting exercise, show that for any
two real numbers
x
and
y
,
x^2
+
y^2
=
0
if and only if
x
=
0
and
y
=
0
.
We suggest proving an auxiliary lemma using
linarith
,
pow_two_nonneg
, and
pow_eq_zero
.
theorem
aux
{
x
y
:
ℝ
}
(
h
:
x
^
2
+
y
^
2
=
0
)
:
x
=
0
:=
have
h'
:
x
^
2
=
0
:=
by
sorry
pow_eq_zero
h'
example
(
x
y
:
ℝ
)
:
x
^
2
+
y
^
2
=
0
↔
x
=
0
∧
y
=
0
:=
sorry
In Lean, bi-implication leads a double-life.
You can treat it like a conjunction and use its two
parts separately.
But Lean also knows that it is a reflexive, symmetric,
and transitive relation between propositions,
and you can also use it with
calc
and
rw
.
It is often convenient to rewrite a statement to
an equivalent one.
In the next example, we use
abs_lt
to
replace an expression of the form
|x|
<
y
by the equivalent expression
-
y
<
x
∧
x
<
y
,
and in the one after that we use
Nat.dvd_gcd_iff
to replace an expression of the form
m
∣
Nat.gcd
n
k
by the equivalent expression
m
∣
n
∧
m
∣
k
.
example
(
x
:
ℝ
)
:
|
x
+
3
|
<
5
→
-
8
<
x
∧
x
<
2
:=
by
rw
[
abs_lt
]
intro
h
constructor
<;>
linarith
example
:
3
∣
Nat.gcd
6
15
:=
by
rw
[
Nat.dvd_gcd_iff
]
constructor
<;>
norm_num
See if you can use
rw
with the theorem below
to provide a short proof that negation is not a
nondecreasing function. (Note that
push_neg
won’t
unfold definitions for you, so the
rw
[Monotone]
in
the proof of the theorem is needed.)
theorem
not_monotone_iff
{
f
:
ℝ
→
ℝ
}
:
¬
Monotone
f
↔
∃
x
y
,
x
≤
y
∧
f
x
>
f
y
:=
by
rw
[
Monotone
]
push_neg
rfl
example
:
¬
Monotone
fun
x
:
ℝ
↦
-
x
:=
by
sorry
The remaining exercises in this section are designed
to give you some more practice with conjunction and
bi-implication. Remember that a
partial order
is a
binary relation that is transitive, reflexive, and
antisymmetric.
An even weaker notion sometimes arises:
a
preorder
is just a reflexive, transitive relation.
For any pre-order
≤
,
Lean axiomatizes the associated strict pre-order by
a
<
b
↔
a
≤
b
∧
¬
b
≤
a
.
Show that if
≤
is a partial order,
then
a
<
b
is equivalent to
a
≤
b
∧
a
≠
b
:
variable
{
α
:
Type
*
}
[
PartialOrder
α
]
variable
(
a
b
:
α
)
example
:
a
<
b
↔
a
≤
b
∧
a
≠
b
:=
by
rw
[
lt_iff_le_not_le
]
sorry
Beyond logical operations, you do not need
anything more than
le_refl
and
le_trans
.
Show that even in the case where
≤
is only assumed to be a preorder,
we can prove that the strict order is irreflexive
and transitive.
In the second example,
for convenience, we use the simplifier rather than
rw
to express
<
in terms of
≤
and
¬
.
We will come back to the simplifier later,
but here we are only relying on the fact that it will
use the indicated lemma repeatedly, even if it needs
to be instantiated to different values.
variable
{
α
:
Type
*
}
[
Preorder
α
]
variable
(
a
b
c
:
α
)
example
:
¬
a
<
a
:=
by
rw
[
lt_iff_le_not_le
]
sorry
example
:
a
<
b
→
b
<
c
→
a
<
c
:=
by
simp
only
[
lt_iff_le_not_le
]
sorry
3.5.
Disjunction

The canonical way to prove a disjunction
A
∨
B
is to prove
A
or to prove
B
.
The
left
tactic chooses
A
,
and the
right
tactic chooses
B
.
variable
{
x
y
:
ℝ
}
example

Quantifiers and Equality - Theorem Proving in Lean 4
Theorem Proving in Lean 4
1.
Introduction
2.
Dependent Type Theory
3.
Propositions and Proofs
4.
Quantifiers and Equality
5.
Tactics
6.
Interacting with Lean
7.
Inductive Types
8.
Induction and Recursion
9.
Structures and Records
10.
Type Classes
11.
The Conversion Tactic Mode
12.
Axioms and Computation
Light (default)
Rust
Coal
Navy
Ayu
Theorem Proving in Lean 4
Quantifiers and Equality
The last chapter introduced you to methods that construct proofs of
statements involving the propositional connectives. In this chapter,
we extend the repertoire of logical constructions to include the
universal and existential quantifiers, and the equality relation.
The Universal Quantifier
Notice that if
α
is any type, we can represent a unary predicate
p
on
α
as an object of type
α → Prop
. In that case, given
x : α
,
p x
denotes the assertion that
p
holds of
x
. Similarly, an object
r : α → α → Prop
denotes a binary
relation on
α
: given
x y : α
,
r x y
denotes the assertion
that
x
is related to
y
.
The universal quantifier,
∀ x : α, p x
is supposed to denote the
assertion that "for every
x : α
,
p x
" holds. As with the
propositional connectives, in systems of natural deduction, "forall"
is governed by an introduction and elimination rule. Informally, the
introduction rule states:
Given a proof of
p x
, in a context where
x : α
is arbitrary, we obtain a proof
∀ x : α, p x
.
The elimination rule states:
Given a proof
∀ x : α, p x
and any term
t : α
, we obtain a proof of
p t
.
As was the case for implication, the propositions-as-types
interpretation now comes into play. Remember the introduction and
elimination rules for dependent arrow types:
Given a term
t
of type
β x
, in a context where
x : α
is arbitrary, we have
(fun x : α => t) : (x : α) → β x
.
The elimination rule states:
Given a term
s : (x : α) → β x
and any term
t : α
, we have
s t : β t
.
In the case where
p x
has type
Prop
, if we replace
(x : α) → β x
with
∀ x : α, p x
, we can read these as the correct rules
for building proofs involving the universal quantifier.
The Calculus of Constructions therefore identifies dependent arrow
types with forall-expressions in this way. If
p
is any expression,
∀ x : α, p
is nothing more than alternative notation for
(x : α) → p
, with the idea that the former is more natural than the latter
in cases where
p
is a proposition. Typically, the expression
p
will depend on
x : α
. Recall that, in the case of ordinary
function spaces, we could interpret
α → β
as the special case of
(x : α) → β
in which
β
does not depend on
x
. Similarly, we
can think of an implication
p → q
between propositions as the
special case of
∀ x : p, q
in which the expression
q
does not
depend on
x
.
Here is an example of how the propositions-as-types correspondence gets put into practice.
example (α : Type) (p q : α → Prop) : (∀ x : α, p x ∧ q x) → ∀ y : α, p y :=
  fun h : ∀ x : α, p x ∧ q x =>
  fun y : α =>
  show p y from (h y).left
As a notational convention, we give the universal quantifier the
widest scope possible, so parentheses are needed to limit the
quantifier over
x
to the hypothesis in the example above. The
canonical way to prove
∀ y : α, p y
is to take an arbitrary
y
,
and prove
p y
. This is the introduction rule. Now, given that
h
has type
∀ x : α, p x ∧ q x
, the expression
h y
has type
p y ∧ q y
. This is the elimination rule. Taking the left conjunct
gives the desired conclusion,
p y
.
Remember that expressions which differ up to renaming of bound
variables are considered to be equivalent. So, for example, we could
have used the same variable,
x
, in both the hypothesis and
conclusion, and instantiated it by a different variable,
z
, in the
proof:
example (α : Type) (p q : α → Prop) : (∀ x : α, p x ∧ q x) → ∀ x : α, p x :=
  fun h : ∀ x : α, p x ∧ q x =>
  fun z : α =>
  show p z from And.left (h z)
As another example, here is how we can express the fact that a relation,
r
, is transitive:
variable (α : Type) (r : α → α → Prop)
variable (trans_r : ∀ x y z, r x y → r y z → r x z)

variable (a b c : α)
variable (hab : r a b) (hbc : r b c)

#check trans_r    -- ∀ (x y z : α), r x y → r y z → r x z
#check trans_r a b c -- r a b → r b c → r a c
#check trans_r a b c hab -- r b c → r a c
#check trans_r a b c hab hbc -- r a c
Think about what is going on here. When we instantiate
trans_r
at
the values
a b c
, we end up with a proof of
r a b → r b c → r a c
.
Applying this to the "hypothesis"
hab : r a b
, we get a proof
of the implication
r b c → r a c
. Finally, applying it to the
hypothesis
hbc
yields a proof of the conclusion
r a c
.
In situations like this, it can be tedious to supply the arguments
a b c
, when they can be inferred from
hab hbc
. For that reason, it
is common to make these arguments implicit:
variable (α : Type) (r : α → α → Prop)
variable (trans_r : ∀ {x y z}, r x y → r y z → r x z)

variable (a b c : α)
variable (hab : r a b) (hbc : r b c)

#check trans_r
#check trans_r hab
#check trans_r hab hbc
The advantage is that we can simply write
trans_r hab hbc
as a
proof of
r a c
. A disadvantage is that Lean does not have enough
information to infer the types of the arguments in the expressions
trans_r
and
trans_r hab
. The output of the first
#check
command is
r ?m.1 ?m.2 → r ?m.2 ?m.3 → r ?m.1 ?m.3
, indicating
that the implicit arguments are unspecified in this case.
Here is an example of how we can carry out elementary reasoning with an equivalence relation:
variable (α : Type) (r : α → α → Prop)

variable (refl_r : ∀ x, r x x)
variable (symm_r : ∀ {x y}, r x y → r y x)
variable (trans_r : ∀ {x y z}, r x y → r y z → r x z)

example (a b c d : α) (hab : r a b) (hcb : r c b) (hcd : r c d) : r a d :=
  trans_r (trans_r hab (symm_r hcb)) hcd
To get used to using universal quantifiers, you should try some of the
exercises at the end of this section.
It is the typing rule for dependent arrow types, and the universal
quantifier in particular, that distinguishes
Prop
from other
types.  Suppose we have
α : Sort i
and
β : Sort j
, where the
expression
β
may depend on a variable
x : α
. Then
(x : α) → β
is an element of
Sort (imax i j)
, where
imax i j
is the
maximum of
i
and
j
if
j
is not 0, and 0 otherwise.
The idea is as follows. If
j
is not
0
, then
(x : α) → β
is
an element of
Sort (max i j)
. In other words, the type of
dependent functions from
α
to
β
"lives" in the universe whose
index is the maximum of
i
and
j
. Suppose, however, that
β
is of
Sort 0
, that is, an element of
Prop
. In that case,
(x : α) → β
is an element of
Sort 0
as well, no matter which
type universe
α
lives in. In other words, if
β
is a
proposition depending on
α
, then
∀ x : α, β
is again a
proposition. This reflects the interpretation of
Prop
as the type
of propositions rather than data, and it is what makes
Prop
impredicative
.
The term "predicative" stems from foundational developments around the
turn of the twentieth century, when logicians such as Poincaré and
Russell blamed set-theoretic paradoxes on the "vicious circles" that
arise when we define a property by quantifying over a collection that
includes the very property being defined. Notice that if
α
is any
type, we can form the type
α → Prop
of all predicates on
α
(the "power type of
α
"). The impredicativity of
Prop
means that we
can form propositions that quantify over
α → Prop
. In particular,
we can define predicates on
α
by quantifying over all predicates
on
α
, which is exactly the type of circularity that was once
considered problematic.
Equality
Let us now turn to one of the most fundamental relations defined in
Lean's library, namely, the equality relation. In
Chapter Inductive Types
,
we will explain
how
equality is defined from the primitives of Lean's logical framework.
In the meanwhile, here we explain how to use it.
Of course, a fundamental property of equality is that it is an equivalence relation:
#check Eq.refl    -- Eq.refl.{u_1} {α : Sort u_1} (a : α) : a = a
#check Eq.symm    -- Eq.symm.{u} {α : Sort u} {a b : α} (h : a = b) : b = a
#check Eq.trans   -- Eq.trans.{u} {α : Sort u} {a b c : α} (h₁ : a = b) (h₂ : b = c) : a = c
We can make the output easier to read by telling Lean not to insert
the implicit arguments (which are displayed here as metavariables).
universe u

#check @Eq.refl.{u}   -- @Eq.refl : ∀ {α : Sort u} (a : α), a = a
#check @Eq.symm.{u}   -- @Eq.symm : ∀ {α : Sort u} {a b : α}, a = b → b = a
#check @Eq.trans.{u}  -- @Eq.trans : ∀ {α : Sort u} {a b c : α}, a = b → b = c → a = c
The inscription
.{u}
tells Lean to instantiate the constants at the universe
u
.
Thus, for example, we can specialize the example from the previous section to the equality relation:
variable (α : Type) (a b c d : α)
variable (hab : a = b) (hcb : c = b) (hcd : c = d)

example : a = d :=
  Eq.trans (Eq.trans hab (Eq.symm hcb)) hcd
We can also use the projection notation:
variable (α : Type) (a b c d : α)
variable (hab : a = b) (hcb : c = b) (hcd : c = d)
example : a = d := (hab.trans hcb.symm).trans hcd
Reflexivity is more powerful than it looks. Recall that terms in the
Calculus of Constructions have a computational interpretation, and
that the logical framework treats terms with a common reduct as the
same. As a result, some nontrivial identities can be proved by
reflexivity:
variable (α β : Type)

example (f : α → β) (a : α) : (fun x => f x) a = f a := Eq.refl _
example (a : α) (b : β) : (a, b).1 = a := Eq.refl _
example : 2 + 3 = 5 := Eq.refl _
This feature of the framework is so important that the library defines a notation
rfl
for
Eq.refl _
:
variable (α β : Type)
example (f : α → β) (a : α) : (fun x => f x) a = f a := rfl
example (a : α) (b : β) : (a, b).1 = a := rfl
example : 2 + 3 = 5 := rfl
Equality is much more than an equivalence relation, however. It has
the important property that every assertion respects the equivalence,
in the sense that we can substitute equal expressions without changing
the truth value. That is, given
h1 : a = b
and
h2 : p a
, we
can construct a proof for
p b
using substitution:
Eq.subst h1 h2
.
example (α : Type) (a b : α) (p : α → Prop)
        (h1 : a = b) (h2 : p a) : p b :=
  Eq.subst h1 h2

example (α : Type) (a b : α) (p : α → Prop)
    (h1 : a = b) (h2 : p a) : p b :=
  h1 ▸ h2
The triangle in the second presentation is a macro built on top of
Eq.subst
and
Eq.symm
, and you can enter it by typing
\t
.
The rule
Eq.subst
is used to define the following auxiliary rules,
which carry out more explicit substitutions. They are designed to deal
with applicative terms, that is, terms of form
s t
. Specifically,
congrArg
can be used to replace the argument,
congrFun
can be
used to replace the term that is being applied, and
congr
can be
used to replace both at once.
variable (α : Type)
variable (a b : α)
variable (f g : α → Nat)
variable (h₁ : a = b)
variable (h₂ : f = g)

example : f a = f b := congrArg f h₁
example : f a = g a := congrFun h₂ a
example : f a = g b := congr h₂ h₁
Lean's library contains a large number of common identities, such as these:
variable (a b c : Nat)

example : a + 0 = a := Nat.add_zero a
example : 0 + a = a := Nat.zero_add a
example : a * 1 = a := Nat.mul_one a
example : 1 * a = a := Nat.one_mul a
example : a + b = b + a := Nat.add_comm a b
example : a + b + c = a + (b + c) := Nat.add_assoc a b c
example : a * b = b * a := Nat.mul_comm a b
example : a * b * c = a * (b * c) := Nat.mul_assoc a b c
example : a * (b + c) = a * b + a * c := Nat.mul_add a b c
example : a * (b + c) = a * b + a * c := Nat.left_distrib a b c
example : (a + b) * c = a * c + b * c := Nat.add_mul a b c
example : (a + b) * c = a * c + b * c := Nat.right_distrib a b c
Note that
Nat.mul_add
and
Nat.add_mul
are alternative names
for
Nat.left_distrib
and
Nat.right_distrib
, respectively.  The
properties above are stated for the natural numbers (type
Nat
).
Here is an example of a calculation in the natural numbers that uses
substitution combined with associativity and distributivity.
example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  have h1 : (x + y) * (x + y) = (x + y) * x + (x + y) * y :=
    Nat.mul_add (x + y) x y
  have h2 : (x + y) * (x + y) = x * x + y * x + (x * y + y * y) :=
    (Nat.add_mul x y x) ▸ (Nat.add_mul x y y) ▸ h1
  h2.trans (Nat.add_assoc (x * x + y * x) (x * y) (y * y)).symm
Notice that the second implicit parameter to
Eq.subst
, which
provides the context in which the substitution is to occur, has type
α → Prop
.  Inferring this predicate therefore requires an instance
of
higher-order unification
. In full generality, the problem of
determining whether a higher-order unifier exists is undecidable, and
Lean can at best provide imperfect and approximate solutions to the
problem. As a result,
Eq.subst
doesn't always do what you want it
to.  The macro
h ▸ e
uses more effective heuristics for computing
this implicit parameter, and often succeeds in situations where
applying
Eq.subst
fails.
Because equational reasoning is so common and important, Lean provides
a number of mechanisms to carry it out more effectively. The next
section offers syntax that allow you to write calculational proofs in
a more natural and perspicuous way. But, more importantly, equational
reasoning is supported by a term rewriter, a simplifier, and other
kinds of automation. The term rewriter and simplifier are described
briefly in the next section, and then in greater detail in the next
chapter.
Calculational Proofs
A calculational proof is just a chain of intermediate results that are
meant to be composed by basic principles such as the transitivity of
equality. In Lean, a calculational proof starts with the keyword
calc
, and has the following syntax:
calc
  <expr>_0  'op_1'  <expr>_1  ':='  <proof>_1
  '_'       'op_2'  <expr>_2  ':='  <proof>_2
  ...
  '_'       'op_n'  <expr>_n  ':='  <proof>_n
Note that the
calc
relations all have the same indentation. Each
<proof>_i
is a proof for
<expr>_{i-1} op_i <expr>_i
.
We can also use
_
in the first relation (right after
<expr>_0
)
which is useful to align the sequence of relation/proof pairs:
calc <expr>_0 
    '_' 'op_1' <expr>_1 ':=' <proof>_1
    '_' 'op_2' <expr>_2 ':=' <proof>_2
    ...
    '_' 'op_n' <expr>_n ':=' <proof>_n
Here is an example:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)

theorem T : a = e :=
  calc
    a = b      := h1
    _ = c + 1  := h2
    _ = d + 1  := congrArg Nat.succ h3
    _ = 1 + d  := Nat.add_comm d 1
    _ = e      := Eq.symm h4
This style of writing proofs is most effective when it is used in
conjunction with the
simp
and
rewrite
tactics, which are
discussed in greater detail in the next chapter. For example, using
the abbreviation
rw
for rewrite, the proof above could be written
as follows:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)
theorem T : a = e :=
  calc
    a = b      := by rw [h1]
    _ = c + 1  := by rw [h2]
    _ = d + 1  := by rw [h3]
    _ = 1 + d  := by rw [Nat.add_comm]
    _ = e      := by rw [h4]
Essentially, the
rw
tactic uses a given equality (which can be a
hypothesis, a theorem name, or a complex term) to "rewrite" the
goal. If doing so reduces the goal to an identity
t = t
, the
tactic applies reflexivity to prove it.
Rewrites can be applied sequentially, so that the proof above can be
shortened to this:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)
theorem T : a = e :=
  calc
    a = d + 1  := by rw [h1, h2, h3]
    _ = 1 + d  := by rw [Nat.add_comm]
    _ = e      := by rw [h4]
Or even this:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)
theorem T : a = e :=
  by rw [h1, h2, h3, Nat.add_comm, h4]
The
simp
tactic, instead, rewrites the goal by applying the given
identities repeatedly, in any order, anywhere they are applicable in a
term. It also uses other rules that have been previously declared to
the system, and applies commutativity wisely to avoid looping. As a
result, we can also prove the theorem as follows:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)
theorem T : a = e :=
  by simp [h1, h2, h3, Nat.add_comm, h4]
We will discuss variations of
rw
and
simp
in the next chapter.
The
calc
command can be configured for any relation that supports
some form of transitivity. It can even combine different relations.
example (a b c d : Nat) (h1 : a = b) (h2 : b ≤ c) (h3 : c + 1 < d) : a < d :=
  calc
    a = b     := h1
    _ < b + 1 := Nat.lt_succ_self b
    _ ≤ c + 1 := Nat.succ_le_succ h2
    _ < d     := h3
You can "teach"
calc
new transitivity theorems by adding new instances
of the
Trans
type class. Type classes are introduced later, but the following
small example demonstrates how to extend the
calc
notation using new
Trans
instances.
def divides (x y : Nat) : Prop :=
  ∃ k, k*x = y

def divides_trans (h₁ : divides x y) (h₂ : divides y z) : divides x z :=
  let ⟨k₁, d₁⟩ := h₁
  let ⟨k₂, d₂⟩ := h₂
  ⟨k₁ * k₂, by rw [Nat.mul_comm k₁ k₂, Nat.mul_assoc, d₁, d₂]⟩

def divides_mul (x : Nat) (k : Nat) : divides x (k*x) :=
  ⟨k, rfl⟩

instance : Trans divides divides divides where
  trans := divides_trans

example (h₁ : divides x y) (h₂ : y = z) : divides x (2*z) :=
  calc
    divides x y     := h₁
    _ = z           := h₂
    divides _ (2*z) := divides_mul ..

infix:50 " ∣ " => divides

example (h₁ : divides x y) (h₂ : y = z) : divides x (2*z) :=
  calc
    x ∣ y   := h₁
    _ = z   := h₂
    _ ∣ 2*z := divides_mul ..
The example above also makes it clear that you can use
calc
even if you
do not have an infix notation for your relation. Finally we remark that
the vertical bar
∣
in the example above is the unicode one. We use
unicode to make sure we do not overload the ASCII
|
used in the
match .. with
expression.
With
calc
, we can write the proof in the last section in a more
natural and perspicuous way.
example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  calc
    (x + y) * (x + y) = (x + y) * x + (x + y) * y  := by rw [Nat.mul_add]
    _ = x * x + y * x + (x + y) * y                := by rw [Nat.add_mul]
    _ = x * x + y * x + (x * y + y * y)            := by rw [Nat.add_mul]
    _ = x * x + y * x + x * y + y * y              := by rw [←Nat.add_assoc]
The alternative
calc
notation is worth considering here. When the
first expression is taking this much space, using
_
in the first
relation naturally aligns all relations:
example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  calc (x + y) * (x + y)
    _ = (x + y) * x + (x + y) * y       := by rw [Nat.mul_add]
    _ = x * x + y * x + (x + y) * y     := by rw [Nat.add_mul]
    _ = x * x + y * x + (x * y + y * y) := by rw [Nat.add_mul]
    _ = x * x + y * x + x * y + y * y   := by rw [←Nat.add_assoc]
Here the left arrow before
Nat.add_assoc
tells rewrite to use the
identity in the opposite direction. (You can enter it with
\l
or
use the ascii equivalent,
<-
.) If brevity is what we are after,
both
rw
and
simp
can do the job on their own:
example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  by rw [Nat.mul_add, Nat.add_mul, Nat.add_mul, ←Nat.add_assoc]

example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  by simp [Nat.mul_add, Nat.add_mul, Nat.add_assoc]
The Existential Quantifier
Finally, consider the existential quantifier, which can be written as
either
exists x : α, p x
or
∃ x : α, p x
.  Both versions are
actually notationally convenient abbreviations for a more long-winded
expression,
Exists (fun x : α => p x)
, defined in Lean's library.
As you should by now expect, the library includes both an introduction
rule and an elimination rule. The introduction rule is
straightforward: to prove
∃ x : α, p x
, it suffices to provide a
suitable term
t
and a proof of
p t
. Here are some examples:
example : ∃ x : Nat, x > 0 :=
  have h : 1 > 0 := Nat.zero_lt_succ 0
  Exists.intro 1 h

example (x : Nat) (h : x > 0) : ∃ y, y < x :=
  Exists.intro 0 h

example (x y z : Nat) (hxy : x < y) (hyz : y < z) : ∃ w, x < w ∧ w < z :=
  Exists.intro y (And.intro hxy hyz)

#check @Exists.intro -- ∀ {α : Sort u_1} {p : α → Prop} (w : α), p w → Exists p
We can use the anonymous constructor notation
⟨t, h⟩
for
Exists.intro t h
, when the type is clear from the context.
example : ∃ x : Nat, x > 0 :=
  have h : 1 > 0 := Nat.zero_lt_succ 0
  ⟨1, h⟩

example (x : Nat) (h : x > 0) : ∃ y, y < x :=
  ⟨0, h⟩

example (x y z : Nat) (hxy : x < y) (hyz : y < z) : ∃ w, x < w ∧ w < z :=
  ⟨y, hxy, hyz⟩
Note that
Exists.intro
has implicit arguments: Lean has to infer
the predicate
p : α → Prop
in the conclusion
∃ x, p x
.  This
is not a trivial affair. For example, if we have
hg : g 0 0 = 0
and write
Exists.intro 0 hg
, there are many possible values
for the predicate
p
, corresponding to the theorems
∃ x, g x x = x
,
∃ x, g x x = 0
,
∃ x, g x 0 = x
, etc. Lean uses the
context to infer which one is appropriate. This is illustrated in the
following example, in which we set the option
pp.explicit
to true
to ask Lean's pretty-printer to show the implicit arguments.
variable (g : Nat → Nat → Nat)
variable (hg : g 0 0 = 0)

theorem gex1 : ∃ x, g x x = x := ⟨0, hg⟩
theorem gex2 : ∃ x, g x 0 = x := ⟨0, hg⟩
theorem gex3 : ∃ x, g 0 0 = x := ⟨0, hg⟩
theorem gex4 : ∃ x, g x x = 0 := ⟨0, hg⟩

set_option pp.explicit true  -- display implicit arguments
#print gex1
#print gex2
#print gex3
#print gex4
We can view
Exists.intro
as an information-hiding operation, since
it hides the witness to the body of the assertion. The existential
elimination rule,
Exists.elim
, performs the opposite operation. It
allows us to prove a proposition
q
from
∃ x : α, p x
, by
showing that
q
follows from
p w
for an arbitrary value
w
. Roughly speaking, since we know there is an
x
satisfying
p x
, we can give it a name, say,
w
. If
q
does not mention
w
, then showing that
q
follows from
p w
is tantamount to
showing that
q
follows from the existence of any such
x
. Here
is an example:
variable (α : Type) (p q : α → Prop)

example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  Exists.elim h
    (fun w =>
     fun hw : p w ∧ q w =>
     show ∃ x, q x ∧ p x from ⟨w, hw.right, hw.left⟩)
It may be helpful to compare the exists-elimination rule to the
or-elimination rule: the assertion
∃ x : α, p x
can be thought of
as a big disjunction of the propositions
p a
, as
a
ranges over
all the elements of
α
. Note that the anonymous constructor
notation
⟨w, hw.right, hw.left⟩
abbreviates a nested constructor
application; we could equally well have written
⟨w, ⟨hw.right, hw.left⟩⟩
.
Notice that an existential proposition is very similar to a sigma
type, as described in dependent types section.  The difference is that
given
a : α
and
h : p a
, the term
Exists.intro a h
has
type
(∃ x : α, p x) : Prop
and
Sigma.mk a h
has type
(Σ x : α, p x) : Type
. The similarity between
∃
and
Σ
is another
instance of the Curry-Howard isomorphism.
Lean provides a more convenient way to eliminate from an existential
quantifier with the
match
expression:
variable (α : Type) (p q : α → Prop)

example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  match h with
  | ⟨w, hw⟩ => ⟨w, hw.right, hw.left⟩
The
match
expression is part of Lean's function definition system,
which provides convenient and expressive ways of defining complex
functions.  Once again, it is the Curry-Howard isomorphism that allows
us to co-opt this mechanism for writing proofs as well.  The
match
statement "destructs" the existential assertion into the components
w
and
hw
, which can then be used in the body of the statement
to prove the proposition. We can annotate the types used in the match
for greater clarity:
variable (α : Type) (p q : α → Prop)
example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  match h with
  | ⟨(w : α), (hw : p w ∧ q w)⟩ => ⟨w, hw.right, hw.left⟩
We can even use the match statement to decompose the conjunction at the same time:
variable (α : Type) (p q : α → Prop)
example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  match h with
  | ⟨w, hpw, hqw⟩ => ⟨w, hqw, hpw⟩
Lean also provides a pattern-matching
let
expression:
variable (α : Type) (p q : α → Prop)
example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  let ⟨w, hpw, hqw⟩ := h
  ⟨w, hqw, hpw⟩
This is essentially just alternative notation for the
match
construct above. Lean will even allow us to use an implicit
match
in the
fun
expression:
variable (α : Type) (p q : α → Prop)
example : (∃ x, p x ∧ q x) → ∃ x, q x ∧ p x :=
  fun ⟨w, hpw, hqw⟩ => ⟨w, hqw, hpw⟩
We will see in
Chapter Induction and Recursion
that all these variations are
instances of a more general pattern-matching construct.
In the following example, we define
is_even a
as
∃ b, a = 2 * b
,
and then we show that the sum of two even numbers is an even number.
def is_even (a : Nat) := ∃ b, a = 2 * b

