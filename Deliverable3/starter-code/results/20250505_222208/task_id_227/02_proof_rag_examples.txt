2. Basics — Mathematics in Lean 0.1 documentation
Mathematics in Lean
1. Introduction
2. Basics
2.1. Calculating
2.2. Proving Identities in Algebraic Structures
2.3. Using Theorems and Lemmas
2.4. More examples using apply and rw
2.5. Proving Facts about Algebraic Structures
3. Logic
4. Sets and Functions
5. Elementary Number Theory
6. Structures
7. Hierarchies
8. Groups and Rings
9. Linear algebra
10. Topology
11. Differential Calculus
12. Integration and Measure Theory
Index
Mathematics in Lean
2.
Basics
View page source
2.
Basics

This chapter is designed to introduce you to the nuts and
bolts of mathematical reasoning in Lean: calculating,
applying lemmas and theorems,
and reasoning about generic structures.
2.1.
Calculating

We generally learn to carry out mathematical calculations
without thinking of them as proofs.
But when we justify each step in a calculation,
as Lean requires us to do,
the net result is a proof that the left-hand side of the calculation
is equal to the right-hand side.
In Lean, stating a theorem is tantamount to stating a goal,
namely, the goal of proving the theorem.
Lean provides the rewriting tactic
rw
,
to replace the left-hand side of an identity by the right-hand side
in the goal. If
a
,
b
, and
c
are real numbers,
mul_assoc
a
b
c
is the identity
a
*
b
*
c
=
a
*
(b
*
c)
and
mul_comm
a
b
is the identity
a
*
b
=
b
*
a
.
Lean provides automation that generally eliminates the need
to refer the facts like these explicitly,
but they are useful for the purposes of illustration.
In Lean, multiplication associates to the left,
so the left-hand side of
mul_assoc
could also be written
(a
*
b)
*
c
.
However, it is generally good style to be mindful of Lean’s
notational conventions and leave out parentheses when Lean does as well.
Let’s try out
rw
.
example
(
a
b
c
:
ℝ
)
:
a
*
b
*
c
=
b
*
(
a
*
c
)
:=
by
rw
[
mul_comm
a
b
]
rw
[
mul_assoc
b
a
c
]
The
import
lines at the beginning of the associated examples file
import the theory of the real numbers from Mathlib, as well as useful automation.
For the sake of brevity,
we generally suppress information like this in the textbook.
You are welcome to make changes to see what happens.
You can type the
ℝ
character as
\R
or
\real
in VS Code.
The symbol doesn’t appear until you hit space or the tab key.
If you hover over a symbol when reading a Lean file,
VS Code will show you the syntax that can be used to enter it.
If you are curious to see all available abbreviations, you can hit Ctrl-Shift-P
and then type abbreviations to get access to the
Lean
4:
Show
Unicode
Input
Abbreviations
command.
If your keyboard does not have an easily accessible backslash,
you can change the leading character by changing the
lean4.input.leader
setting.
When a cursor is in the middle of a tactic proof,
Lean reports on the current
proof state
in the
Lean Infoview
window.
As you move your cursor past each step of the proof,
you can see the state change.
A typical proof state in Lean might look as follows:
1
goal
x
y
:
ℕ
,
h₁
:
Prime
x
,
h₂
:
¬
Even
x
,
h₃
:
y
>
x
⊢
y
≥
4
The lines before the one that begins with
⊢
denote the
context
:
they are the objects and assumptions currently at play.
In this example, these include two objects,
x
and
y
,
each a natural number.
They also include three assumptions,
labelled
h₁
,
h₂
, and
h₃
.
In Lean, everything in a context is labelled with an identifier.
You can type these subscripted labels as
h\1
,
h\2
, and
h\3
,
but any legal identifiers would do:
you can use
h1
,
h2
,
h3
instead,
or
foo
,
bar
, and
baz
.
The last line represents the
goal
,
that is, the fact to be proved.
Sometimes people use
target
for the fact to be proved,
and
goal
for the combination of the context and the target.
In practice, the intended meaning is usually clear.
Try proving these identities,
in each case replacing
sorry
by a tactic proof.
With the
rw
tactic, you can use a left arrow (
\l
)
to reverse an identity.
For example,
rw
[←
mul_assoc
a
b
c]
replaces
a
*
(b
*
c)
by
a
*
b
*
c
in the current goal. Note that
the left-pointing arrow refers to going from right to left in the identity provided
by
mul_assoc
, it has nothing to do with the left or right side of the goal.
example
(
a
b
c
:
ℝ
)
:
c
*
b
*
a
=
b
*
(
a
*
c
)
:=
by
sorry
example
(
a
b
c
:
ℝ
)
:
a
*
(
b
*
c
)
=
b
*
(
a
*
c
)
:=
by
sorry
You can also use identities like
mul_assoc
and
mul_comm
without arguments.
In this case, the rewrite tactic tries to match the left-hand side with
an expression in the goal,
using the first pattern it finds.
example
(
a
b
c
:
ℝ
)
:
a
*
b
*
c
=
b
*
c
*
a
:=
by
rw
[
mul_assoc
]
rw
[
mul_comm
]
You can also provide
partial
information.
For example,
mul_comm
a
matches any pattern of the form
a
*
?
and rewrites it to
?
*
a
.
Try doing the first of these examples without
providing any arguments at all,
and the second with only one argument.
example
(
a
b
c
:
ℝ
)
:
a
*
(
b
*
c
)
=
b
*
(
c
*
a
)
:=
by
sorry
example
(
a
b
c
:
ℝ
)
:
a
*
(
b
*
c
)
=
b
*
(
a
*
c
)
:=
by
sorry
You can also use
rw
with facts from the local context.
example
(
a
b
c
d
e
f
:
ℝ
)
(
h
:
a
*
b
=
c
*
d
)
(
h'
:
e
=
f
)
:
a
*
(
b
*
e
)
=
c
*
(
d
*
f
)
:=
by
rw
[
h'
]
rw
[
←
mul_assoc
]
rw
[
h
]
rw
[
mul_assoc
]
Try these, using the theorem
sub_self
for the second one:
example
(
a
b
c
d
e
f
:
ℝ
)
(
h
:
b
*
c
=
e
*
f
)
:
a
*
b
*
c
*
d
=
a
*
e
*
f
*
d
:=
by
sorry
example
(
a
b
c
d
:
ℝ
)
(
hyp
:
c
=
b
*
a
-
d
)
(
hyp'
:
d
=
a
*
b
)
:
c
=
0
:=
by
sorry
Multiple rewrite commands can be carried out with a single command,
by listing the relevant identities separated by commas inside the square brackets.
example
(
a
b
c
d
e
f
:
ℝ
)
(
h
:
a
*
b
=
c
*
d
)
(
h'
:
e
=
f
)
:
a
*
(
b
*
e
)
=
c
*
(
d
*
f
)
:=
by
rw
[
h'
,
←
mul_assoc
,
h
,
mul_assoc
]
You still see the incremental progress by placing the cursor after
a comma in any list of rewrites.
Another trick is that we can declare variables once and for all outside
an example or theorem. Lean then includes them automatically.
variable
(
a
b
c
d
e
f
:
ℝ
)
example
(
h
:
a
*
b
=
c
*
d
)
(
h'
:
e
=
f
)
:
a
*
(
b
*
e
)
=
c
*
(
d
*
f
)
:=
by
rw
[
h'
,
←
mul_assoc
,
h
,
mul_assoc
]
Inspection of the tactic state at the beginning of the above proof
reveals that Lean indeed included all variables.
We can delimit the scope of the declaration by putting it
in a
section
...
end
block.
Finally, recall from the introduction that Lean provides us with a
command to determine the type of an expression:
section
variable
(
a
b
c
:
ℝ
)
#check
a
#check
a
+
b
#check
(
a
:
ℝ
)
#check
mul_comm
a
b
#check
(
mul_comm
a
b
:
a
*
b
=
b
*
a
)
#check
mul_assoc
c
a
b
#check
mul_comm
a
#check
mul_comm
end
The
#check
command works for both objects and facts.
In response to the command
#check
a
, Lean reports that
a
has type
ℝ
.
In response to the command
#check
mul_comm
a
b
,
Lean reports that
mul_comm
a
b
is a proof of the fact
a
*
b
=
b
*
a
.
The command
#check
(a
:
ℝ)
states our expectation that the
type of
a
is
ℝ
,
and Lean will raise an error if that is not the case.
We will explain the output of the last three
#check
commands later,
but in the meanwhile, you can take a look at them,
and experiment with some
#check
commands of your own.
Let’s try some more examples. The theorem
two_mul
a
says
that
2
*
a
=
a
+
a
. The theorems
add_mul
and
mul_add
express the distributivity of multiplication over addition,
and the theorem
add_assoc
expresses the associativity of addition.
Use the
#check
command to see the precise statements.
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
rw
[
mul_add
,
add_mul
,
add_mul
]
rw
[
←
add_assoc
,
add_assoc
(
a
*
a
)]
rw
[
mul_comm
b
a
,
←
two_mul
]
Whereas it is possible to figure out what is going on in this proof
by stepping through it in the editor,
it is hard to read on its own.
Lean provides a more structured way of writing proofs like this
using the
calc
keyword.
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
calc
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
b
*
a
+
(
a
*
b
+
b
*
b
)
:=
by
rw
[
mul_add
,
add_mul
,
add_mul
]
_
=
a
*
a
+
(
b
*
a
+
a
*
b
)
+
b
*
b
:=
by
rw
[
←
add_assoc
,
add_assoc
(
a
*
a
)]
_
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
rw
[
mul_comm
b
a
,
←
two_mul
]
Notice that the proof does
not
begin with
by
:
an expression that begins with
calc
is a
proof term
.
A
calc
expression can also be used inside a tactic proof,
but Lean interprets it as the instruction to use the resulting
proof term to solve the goal.
The
calc
syntax is finicky: the underscores and justification
have to be in the format indicated above.
Lean uses indentation to determine things like where a block
of tactics or a
calc
block begins and ends;
try changing the indentation in the proof above to see what happens.
One way to write a
calc
proof is to outline it first
using the
sorry
tactic for justification,
make sure Lean accepts the expression modulo these,
and then justify the individual steps using tactics.
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
calc
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
b
*
a
+
(
a
*
b
+
b
*
b
)
:=
by
sorry
_
=
a
*
a
+
(
b
*
a
+
a
*
b
)
+
b
*
b
:=
by
sorry
_
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
sorry
Try proving the following identity using both a pure
rw
proof
and a more structured
calc
proof:
example
:
(
a
+
b
)
*
(
c
+
d
)
=
a
*
c
+
a
*
d
+
b
*
c
+
b
*
d
:=
by
sorry
The following exercise is a little more challenging.
You can use the theorems listed underneath.
example
(
a
b
:
ℝ
)
:
(
a
+
b
)
*
(
a
-
b
)
=
a
^
2
-
b
^
2
:=
by
sorry
#check
pow_two
a
#check
mul_sub
a
b
c
#check
add_mul
a
b
c
#check
add_sub
a
b
c
#check
sub_sub
a
b
c
#check
add_zero
a
We can also perform rewriting in an assumption in the context.
For example,
rw
[mul_comm
a
b]
at
hyp
replaces
a
*
b
by
b
*
a
in the assumption
hyp
.
example
(
a
b
c
d
:
ℝ
)
(
hyp
:
c
=
d
*
a
+
b
)
(
hyp'
:
b
=
a
*
d
)
:
c
=
2
*
a
*
d
:=
by
rw
[
hyp'
]
at
hyp
rw
[
mul_comm
d
a
]
at
hyp
rw
[
←
two_mul
(
a
*
d
)]
at
hyp
rw
[
←
mul_assoc
2
a
d
]
at
hyp
exact
hyp
In the last step, the
exact
tactic can use
hyp
to solve the goal
because at that point
hyp
matches the goal exactly.
We close this section by noting that Mathlib provides a
useful bit of automation with a
ring
tactic,
which is designed to prove identities in any commutative ring as long as they follow
purely from the ring axioms, without using any local assumption.
example
:
c
*
b
*
a
=
b
*
(
a
*
c
)
:=
by
ring
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
ring
example
:
(
a
+
b
)
*
(
a
-
b
)
=
a
^
2
-
b
^
2
:=
by
ring
example
(
hyp
:
c
=
d
*
a
+
b
)
(
hyp'
:
b
=
a
*
d
)
:
c
=
2
*
a
*
d
:=
by
rw
[
hyp
,
hyp'
]
ring
The
ring
tactic is imported indirectly when we
import
Mathlib.Data.Real.Basic
,
but we will see in the next section that it can be used
for calculations on structures other than the real numbers.
It can be imported explicitly with the command
import
Mathlib.Tactic
.
We will see there are similar tactics for other common kind of algebraic
structures.
There is a variation of
rw
called
nth_rw
that allows you to replace only particular instances of an expression in the goal.
Possible matches are enumerated starting with 1,
so in the following example,
nth_rw
2
[h]
replaces the second
occurrence of
a
+
b
with
c
.
example
(
a
b
c
:
ℕ
)
(
h
:
a
+
b
=
c
)
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
c
+
b
*
c
:=
by
nth_rw
2
[
h
]
rw
[
add_mul
]
2.2.
Proving Identities in Algebraic Structures

Mathematically, a ring consists of a collection of objects,
\(R\)
, operations
\(+\)
\(\times\)
, and constants
\(0\)
and
\(1\)
, and an operation
\(x \mapsto -x\)
such that:
\(R\)
with
\(+\)
is an
abelian group
, with
\(0\)
as the additive identity and negation as inverse.
Multiplication is associative with identity
\(1\)
,
and multiplication distributes over addition.
In Lean, the collection of objects is represented as a
type
,
R
.
The ring axioms are as follows:
variable
(
R
:
Type
*
)
[
Ring
R
]
#check
(
add_assoc
:
∀
a
b
c
:
R
,
a
+
b
+
c
=
a
+
(
b
+
c
))
#check
(
add_comm
:
∀
a
b
:
R
,
a
+
b
=
b
+
a
)
#check
(
zero_add
:
∀
a
:
R
,
0
+
a
=
a
)
#check
(
neg_add_cancel
:
∀
a
:
R
,
-
a
+
a
=
0
)
#check
(
mul_assoc
:
∀
a
b
c
:
R
,
a
*
b
*
c
=
a
*
(
b
*
c
))
#check
(
mul_one
:
∀
a
:
R
,
a
*
1
=
a
)
#check
(
one_mul
:
∀
a
:
R
,
1
*
a
=
a
)
#check
(
mul_add
:
∀
a
b
c
:
R
,
a
*
(
b
+
c
)
=
a
*
b
+
a
*
c
)
#check
(
add_mul
:
∀
a
b
c
:
R
,
(
a
+
b
)
*
c
=
a
*
c
+
b
*
c
)
You will learn more about the square brackets in the first line later,
but for the time being,
suffice it to say that the declaration gives us a type,
R
,
and a ring structure on
R
.
Lean then allows us to use generic ring notation with elements of
R
,
and to make use of a library of theorems about rings.
The names of some of the theorems should look familiar:
they are exactly the ones we used to calculate with the real numbers
in the last section.
Lean is good not only for proving things about concrete mathematical
structures like the natural numbers and the integers,
but also for proving things about abstract structures,
characterized axiomatically, like rings.
Moreover, Lean supports
generic reasoning
about
both abstract and concrete structures,
and can be trained to recognize appropriate instances.
So any theorem about rings can be applied to concrete rings
like the integers,
ℤ
, the rational numbers,
ℚ
,
and the complex numbers
ℂ
.
It can also be applied to any instance of an abstract
structure that extends rings,
such as any ordered ring or any field.
Not all important properties of the real numbers hold in an
arbitrary ring, however.
For example, multiplication on the real numbers
is commutative,
but that does not hold in general.
If you have taken a course in linear algebra,
you will recognize that, for every
\(n\)
,
the
\(n\)
by
\(n\)
matrices of real numbers
form a ring in which commutativity usually fails. If we declare
R
to be a
commutative
ring, in fact, all the theorems
in the last section continue to hold when we replace
ℝ
by
R
.
variable
(
R
:
Type
*
)
[
CommRing
R
]
variable
(
a
b
c
d
:
R
)
example
:
c
*
b
*
a
=
b
*
(
a
*
c
)
:=
by
ring
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
ring
example
:
(
a
+
b
)
*
(
a
-
b
)
=
a
^
2
-
b
^
2
:=
by
ring
example
(
hyp
:
c
=
d
*
a
+
b
)
(
hyp'
:
b
=
a
*
d
)
:
c
=
2
*
a
*
d
:=
by
rw
[
hyp
,
hyp'
]
ring
We leave it to you to check that all the other proofs go through unchanged.
Notice that when a proof is short, like
by
ring
or
by
linarith
or
by
sorry
,
it is common (and permissible) to put it on the same line as
the
by
.
Good proof-writing style should strike a balance between concision and readability.
The goal of this section is to strengthen the skills
you have developed in the last section
and apply them to reasoning axiomatically about rings.
We will start with the axioms listed above,
and use them to derive other facts.
Most of the facts we prove are already in Mathlib.
We will give the versions we prove the same names
to help you learn the contents of the library
as well as the naming conventions.
Lean provides an organizational mechanism similar
to those used in programming languages:
when a definition or theorem
foo
is introduced in a
namespace
bar
, its full name is
bar.foo
.
The command
open
bar
later
opens
the namespace,
which allows us to use the shorter name
foo
.
To avoid errors due to name clashes,
in the next example we put our versions of the library
theorems in a new namespace called
MyRing.
The next example shows that we do not need
add_zero
or
add_neg_cancel
as ring axioms, because they follow from the other axioms.
namespace
MyRing
variable
{
R
:
Type
*
}
[
Ring
R
]
theorem
add_zero
(
a
:
R
)
:
a
+
0
=
a
:=
by
rw
[
add_comm
,
zero_add
]
theorem
add_neg_cancel
(
a
:
R
)
:
a
+
-
a
=
0
:=
by
rw
[
add_comm
,
neg_add_cancel
]
#check
MyRing.add_zero
#check
add_zero
end
MyRing
The net effect is that we can temporarily reprove a theorem in the library,
and then go on using the library version after that.
But don’t cheat!
In the exercises that follow, take care to use only the
general facts about rings that we have proved earlier in this section.
(If you are paying careful attention, you may have noticed that we
changed the round brackets in
(R
:
Type*)
for
curly brackets in
{R
:
Type*}
.
This declares
R
to be an
implicit argument
.
We will explain what this means in a moment,
but don’t worry about it in the meanwhile.)
Here is a useful theorem:
theorem
neg_add_cancel_left
(
a
b
:
R
)
:
-
a
+
(
a
+
b
)
=
b
:=
by
rw
[
←
add_assoc
,
neg_add_cancel
,
zero_add
]
Prove the companion version:
theorem
add_neg_cancel_right
(
a
b
:
R
)
:
a
+
b
+
-
b
=
a
:=
by
sorry
Use these to prove the following:
theorem
add_left_cancel
{
a
b
c
:
R
}
(
h
:
a
+
b
=
a
+
c
)
:
b
=
c
:=
by
sorry
theorem
add_right_cancel
{
a
b
c
:
R
}
(
h
:
a
+
b
=
c
+
b
)
:
a
=
c
:=
by
sorry
With enough planning, you can do each of them with three rewrites.
We will now explain the use of the curly braces.
Imagine you are in a situation where you have
a
,
b
, and
c
in your context,
as well as a hypothesis
h
:
a
+
b
=
a
+
c
,
and you would like to draw the conclusion
b
=
c
.
In Lean, you can apply a theorem to hypotheses and facts just
the same way that you can apply them to objects,
so you might think that
add_left_cancel
a
b
c
h
is a
proof of the fact
b
=
c
.
But notice that explicitly writing
a
,
b
, and
c
is redundant, because the hypothesis
h
makes it clear that
those are the objects we have in mind.
In this case, typing a few extra characters is not onerous,
but if we wanted to apply
add_left_cancel
to more complicated expressions,
writing them would be tedious.
In cases like these,
Lean allows us to mark arguments as
implicit
,
meaning that they are supposed to be left out and inferred by other means,
such as later arguments and hypotheses.
The curly brackets in
{a
b
c
:
R}
do exactly that.
So, given the statement of the theorem above,
the correct expression is simply
add_left_cancel
h
.
To illustrate, let us show that
a
*
0
=
0
follows from the ring axioms.
theorem
mul_zero
(
a
:
R
)
:
a
*
0
=
0
:=
by
have
h
:
a
*
0
+
a
*
0
=
a
*
0
+
0
:=
by
rw
[
←
mul_add
,
add_zero
,
add_zero
]
rw
[
add_left_cancel
h
]
We have used a new trick!
If you step through the proof,
you can see what is going on.
The
have
tactic introduces a new goal,
a
*
0
+
a
*
0
=
a
*
0
+
0
,
with the same context as the original goal.
The fact that the next line is indented indicates that Lean
is expecting a block of tactics that serves to prove this
new goal.
The indentation therefore promotes a modular style of proof:
the indented subproof establishes the goal
that was introduced by the
have
.
After that, we are back to proving the original goal,
except a new hypothesis
h
has been added:
having proved it, we are now free to use it.
At this point, the goal is exactly the result of
add_left_cancel
h
.
We could equally well have closed the proof with
apply
add_left_cancel
h
or
exact
add_left_cancel
h
.
The
exact
tactic takes as argument a proof term which completely proves the
current goal, without creating any new goal. The
apply
tactic is a variant
whose argument is not necessarily a complete proof. The missing pieces are either
inferred automatically by Lean or become new goals to prove.
While the
exact
tactic is technically redundant since it is strictly less powerful
than
apply
, it makes proof scripts slightly clearer to
human readers and easier to maintain when the library evolves.
Remember that multiplication is not assumed to be commutative,
so the following theorem also requires some work.
theorem
zero_mul
(
a
:
R
)
:
0
*
a
=
0
:=
by
sorry
By now, you should also be able replace each
sorry
in the next
exercise with a proof,
still using only facts about rings that we have
established in this section.
theorem
neg_eq_of_add_eq_zero
{
a
b
:
R
}
(
h
:
a
+
b
=
0
)
:
-
a
=
b
:=
by
sorry
theorem
eq_neg_of_add_eq_zero
{
a
b
:
R
}
(
h
:
a
+
b
=
0
)
:
a
=
-
b
:=
by
sorry
theorem
neg_zero
:
(
-
0
:
R
)
=
0
:=
by
apply
neg_eq_of_add_eq_zero
rw
[
add_zero
]
theorem
neg_neg
(
a
:
R
)
:
-
-
a
=
a
:=
by
sorry
We had to use the annotation
(-0
:
R)
instead of
0
in the third theorem
because without specifying
R
it is impossible for Lean to infer which
0
we have in mind,
and by default it would be interpreted as a natural number.
In Lean, subtraction in a ring is provably equal to
addition of the additive inverse.
example
(
a
b
:
R
)
:
a
-
b
=
a
+
-
b
:=
sub_eq_add_neg
a
b
On the real numbers, it is
defined
that way:
example
(
a
b
:
ℝ
)
:
a
-
b
=
a
+
-
b
:=
rfl
example
(
a
b
:
ℝ
)
:
a
-
b
=
a
+
-
b
:=
by
rfl
The proof term
rfl
is short for “reflexivity”.
Presenting it as a proof of
a
-
b
=
a
+
-b
forces Lean
to unfold the definition and recognize both sides as being the same.
The
rfl
tactic does the same.
This is an instance of what is known as a
definitional equality
in Lean’s underlying logic.
This means that not only can one rewrite with
sub_eq_add_neg
to replace
a
-
b
=
a
+
-b
,
but in some contexts, when dealing with the real numbers,
you can use the two sides of the equation interchangeably.
For example, you now have enough information to prove the theorem
self_sub
from the last section:
theorem
self_sub
(
a
:
R
)
:
a
-
a
=
0
:=
by
sorry
Show that you can prove this using
rw
,
but if you replace the arbitrary ring
R
by
the real numbers, you can also prove it
using either
apply
or
exact
.
Lean knows that
1
+
1
=
2
holds in any ring.
With a bit of effort,
you can use that to prove the theorem
two_mul
from
the last section:
theorem
one_add_one_eq_two
:
1
+
1
=
(
2
:
R
)
:=
by
norm_num
theorem
two_mul
(
a
:
R
)
:
2
*
a
=
a
+
a
:=
by
sorry
We close this section by noting that some of the facts about
addition and negation that we established above do not
need the full strength of the ring axioms, or even
commutativity of addition. The weaker notion of a
group
can be axiomatized as follows:
variable
(
A
:
Type
*
)
[
AddGroup
A
]
#check
(
add_assoc
:
∀
a
b
c
:
A
,
a
+
b
+
c
=
a
+
(
b
+
c
))
#check
(
zero_add
:
∀
a
:
A
,
0
+
a
=
a
)
#check
(
neg_add_cancel
:
∀
a
:
A
,
-
a
+
a
=
0
)
It is conventional to use additive notation when
the group operation is commutative,
and multiplicative notation otherwise.
So Lean defines a multiplicative version as well as the
additive version (and also their abelian variants,
AddCommGroup
and
CommGroup
).
variable
{
G
:
Type
*
}
[
Group
G
]
#check
(
mul_assoc
:
∀
a
b
c
:
G
,
a
*
b
*
c
=
a
*
(
b
*
c
))
#check
(
one_mul
:
∀
a
:
G
,
1
*
a
=
a
)
#check
(
inv_mul_cancel
:
∀
a
:
G
,
a
⁻¹
*
a
=
1
)
If you are feeling cocky, try proving the following facts about
groups, using only these axioms.
You will need to prove a number of helper lemmas along the way.
The proofs we have carried out in this section provide some hints.
theorem
mul_inv_cancel
(
a
:
G
)
:
a
*
a
⁻¹
=
1
:=
by
sorry
theorem
mul_one
(
a
:
G
)
:
a
*
1
=
a
:=
by
sorry
theorem
mul_inv_rev
(
a
b
:
G
)
:
(
a
*
b
)
⁻¹
=
b
⁻¹
*
a
⁻¹
:=
by
sorry
Explicitly invoking those lemmas is tedious, so Mathlib provides
tactics similar to
ring
in order to cover most uses:
group
is for non-commutative multiplicative groups,
abel
for abelian
additive groups, and
noncomm_ring
for non-commutative rings.
It may seem odd that the algebraic structures are called
Ring
and
CommRing
while the tactics are named
noncomm_ring
and
ring
. This is partly for historical reasons,
but also for the convenience of using a shorter name for the
tactic that deals with commutative rings, since it is used more often.
2.3.
Using Theorems and Lemmas

Rewriting is great for proving equations,
but what about other sorts of theorems?
For example, how can we prove an inequality,
like the fact that
\(a + e^b \le a + e^c\)
holds whenever
\(b \le c\)
?
We have already seen that theorems can be applied to arguments and hypotheses,
and that the
apply
and
exact
tactics can be used to solve goals.
In this section, we will make good use of these tools.
Consider the library theorems
le_refl
and
le_trans
:
#check
(
le_refl
:
∀
a
:
ℝ
,
a
≤
a
)
#check
(
le_trans
:
a
≤
b
→
b
≤
c
→
a
≤
c
)
As we explain in more detail in
Section 3.1
,
the implicit parentheses in the statement of
le_trans
associate to the right, so it should be interpreted as
a
≤
b
→
(b
≤
c
→
a
≤
c)
.
The library designers have set the arguments
a
,
b
and
c
to
le_trans
implicit,
so that Lean will
not
let you provide them explicitly (unless you
really insist, as we will discuss later).
Rather, it expects to infer them from the context in which they are used.
For example, when hypotheses

ifier.
But it can’t hurt to see the alternative syntax, especially if there is
a chance you will find yourself in the company of computer scientists.
To illustrate one way that
rcases
can be used,
we prove an old mathematical chestnut:
if two integers
x
and
y
can each be written as
a sum of two squares,
then so can their product,
x
*
y
.
In fact, the statement is true for any commutative
ring, not just the integers.
In the next example,
rcases
unpacks two existential
quantifiers at once.
We then provide the magic values needed to express
x
*
y
as a sum of squares as a list to the
use
statement,
and we use
ring
to verify that they work.
variable
{
α
:
Type
*
}
[
CommRing
α
]
def
SumOfSquares
(
x
:
α
)
:=
∃
a
b
,
x
=
a
^
2
+
b
^
2
theorem
sumOfSquares_mul
{
x
y
:
α
}
(
sosx
:
SumOfSquares
x
)
(
sosy
:
SumOfSquares
y
)
:
SumOfSquares
(
x
*
y
)
:=
by
rcases
sosx
with
⟨
a
,
b
,
xeq
⟩
rcases
sosy
with
⟨
c
,
d
,
yeq
⟩
rw
[
xeq
,
yeq
]
use
a
*
c
-
b
*
d
,
a
*
d
+
b
*
c
ring
This proof doesn’t provide much insight,
but here is one way to motivate it.
A
Gaussian integer
is a number of the form
\(a + bi\)
where
\(a\)
and
\(b\)
are integers and
\(i = \sqrt{-1}\)
.
The
norm
of the Gaussian integer
\(a + bi\)
is, by definition,
\(a^2 + b^2\)
.
So the norm of a Gaussian integer is a sum of squares,
and any sum of squares can be expressed in this way.
The theorem above reflects the fact that norm of a product of
Gaussian integers is the product of their norms:
if
\(x\)
is the norm of
\(a + bi\)
and
\(y\)
in the norm of
\(c + di\)
,
then
\(xy\)
is the norm of
\((a + bi) (c + di)\)
.
Our cryptic proof illustrates the fact that
the proof that is easiest to formalize isn’t always
the most perspicuous one.
In
Section 6.3
,
we will provide you with the means to define the Gaussian
integers and use them to provide an alternative proof.
The pattern of unpacking an equation inside an existential quantifier
and then using it to rewrite an expression in the goal
comes up often,
so much so that the
rcases
tactic provides
an abbreviation:
if you use the keyword
rfl
in place of a new identifier,
rcases
does the rewriting automatically (this trick doesn’t work
with pattern-matching lambdas).
theorem
sumOfSquares_mul'
{
x
y
:
α
}
(
sosx
:
SumOfSquares
x
)
(
sosy
:
SumOfSquares
y
)
:
SumOfSquares
(
x
*
y
)
:=
by
rcases
sosx
with
⟨
a
,
b
,
rfl
⟩
rcases
sosy
with
⟨
c
,
d
,
rfl
⟩
use
a
*
c
-
b
*
d
,
a
*
d
+
b
*
c
ring
As with the universal quantifier,
you can find existential quantifiers hidden all over
if you know how to spot them.
For example, divisibility is implicitly an “exists” statement.
example
(
divab
:
a
∣
b
)
(
divbc
:
b
∣
c
)
:
a
∣
c
:=
by
rcases
divab
with
⟨
d
,
beq
⟩
rcases
divbc
with
⟨
e
,
ceq
⟩
rw
[
ceq
,
beq
]
use
d
*
e
;
ring
And once again, this provides a nice setting for using
rcases
with
rfl
.
Try it out in the proof above.
It feels pretty good!
Then try proving the following:
example
(
divab
:
a
∣
b
)
(
divac
:
a
∣
c
)
:
a
∣
b
+
c
:=
by
sorry
For another important example, a function
\(f : \alpha \to \beta\)
is said to be
surjective
if for every
\(y\)
in the
codomain,
\(\beta\)
,
there is an
\(x\)
in the domain,
\(\alpha\)
,
such that
\(f(x) = y\)
.
Notice that this statement includes both a universal
and an existential quantifier, which explains
why the next example makes use of both
intro
and
use
.
example
{
c
:
ℝ
}
:
Surjective
fun
x
↦
x
+
c
:=
by
intro
x
use
x
-
c
dsimp
;
ring
Try this example yourself using the theorem
mul_div_cancel₀
.:
example
{
c
:
ℝ
}
(
h
:
c
≠
0
)
:
Surjective
fun
x
↦
c
*
x
:=
by
sorry
At this point, it is worth mentioning that there is a tactic,
field_simp
,
that will often clear denominators in a useful way.
It can be used in conjunction with the
ring
tactic.
example
(
x
y
:
ℝ
)
(
h
:
x
-
y
≠
0
)
:
(
x
^
2
-
y
^
2
)
/
(
x
-
y
)
=
x
+
y
:=
by
field_simp
[
h
]
ring
The next example uses a surjectivity hypothesis
by applying it to a suitable value.
Note that you can use
rcases
with any expression,
not just a hypothesis.
example
{
f
:
ℝ
→
ℝ
}
(
h
:
Surjective
f
)
:
∃
x
,
f
x
^
2
=
4
:=
by
rcases
h
2
with
⟨
x
,
hx
⟩
use
x
rw
[
hx
]
norm_num
See if you can use these methods to show that
the composition of surjective functions is surjective.
variable
{
α
:
Type
*
}
{
β
:
Type
*
}
{
γ
:
Type
*
}
variable
{
g
:
β
→
γ
}
{
f
:
α
→
β
}
example
(
surjg
:
Surjective
g
)
(
surjf
:
Surjective
f
)
:
Surjective
fun
x
↦
g
(
f
x
)
:=
by
sorry
3.3.
Negation

The symbol
¬
is meant to express negation,
so
¬
x
<
y
says that
x
is not less than
y
,
¬
x
=
y
(or, equivalently,
x
≠
y
) says that
x
is not equal to
y
,
and
¬
∃
z,
x
<
z
∧
z
<
y
says that there does not exist a
z
strictly between
x
and
y
.
In Lean, the notation
¬
A
abbreviates
A
→
False
,
which you can think of as saying that
A
implies a contradiction.
Practically speaking, this means that you already know something
about how to work with negations:
you can prove
¬
A
by introducing a hypothesis
h
:
A
and proving
False
,
and if you have
h
:
¬
A
and
h'
:
A
,
then applying
h
to
h'
yields
False
.
To illustrate, consider the irreflexivity principle
lt_irrefl
for a strict order,
which says that we have
¬
a
<
a
for every
a
.
The asymmetry principle
lt_asymm
says that we have
a
<
b
→
¬
b
<
a
. Let’s show that
lt_asymm
follows
from
lt_irrefl
.
example
(
h
:
a
<
b
)
:
¬
b
<
a
:=
by
intro
h'
have
:
a
<
a
:=
lt_trans
h
h'
apply
lt_irrefl
a
this
This example introduces a couple of new tricks.
First, when you use
have
without providing
a label,
Lean uses the name
this
,
providing a convenient way to refer back to it.
Because the proof is so short, we provide an explicit proof term.
But what you should really be paying attention to in this
proof is the result of the
intro
tactic,
which leaves a goal of
False
,
and the fact that we eventually prove
False
by applying
lt_irrefl
to a proof of
a
<
a
.
Here is another example, which uses the
predicate
FnHasUb
defined in the last section,
which says that a function has an upper bound.
example
(
h
:
∀
a
,
∃
x
,
f
x
>
a
)
:
¬
FnHasUb
f
:=
by
intro
fnub
rcases
fnub
with
⟨
a
,
fnuba
⟩
rcases
h
a
with
⟨
x
,
hx
⟩
have
:
f
x
≤
a
:=
fnuba
x
linarith
Remember that it is often convenient to use
linarith
when a goal follows from linear equations and
inequalities that are in the context.
See if you can prove these in a similar way:
example
(
h
:
∀
a
,
∃
x
,
f
x
<
a
)
:
¬
FnHasLb
f
:=
sorry
example
:
¬
FnHasUb
fun
x
↦
x
:=
sorry
Mathlib offers a number of useful theorems for relating orders
and negations:
#check
(
not_le_of_gt
:
a
>
b
→
¬
a
≤
b
)
#check
(
not_lt_of_ge
:
a
≥
b
→
¬
a
<
b
)
#check
(
lt_of_not_ge
:
¬
a
≥
b
→
a
<
b
)
#check
(
le_of_not_gt
:
¬
a
>
b
→
a
≤
b
)
Recall the predicate
Monotone
f
,
which says that
f
is nondecreasing.
Use some of the theorems just enumerated to prove the following:
example
(
h
:
Monotone
f
)
(
h'
:
f
a
<
f
b
)
:
a
<
b
:=
by
sorry
example
(
h
:
a
≤
b
)
(
h'
:
f
b
<
f
a
)
:
¬
Monotone
f
:=
by
sorry
We can show that the first example in the last snippet
cannot be proved if we replace
<
by
≤
.
Notice that we can prove the negation of a universally
quantified statement by giving a counterexample.
Complete the proof.
example
:
¬∀
{
f
:
ℝ
→
ℝ
},
Monotone
f
→
∀
{
a
b
},
f
a
≤
f
b
→
a
≤
b
:=
by
intro
h
let
f
:=
fun
x
:
ℝ
↦
(
0
:
ℝ
)
have
monof
:
Monotone
f
:=
by
sorry
have
h'
:
f
1
≤
f
0
:=
le_refl
_
sorry
This example introduces the
let
tactic,
which adds a
local definition
to the context.
If you put the cursor after the
let
command,
in the goal window you will see that the definition
f
:
ℝ
→
ℝ
:=
fun
x
↦
0
has been added to the context.
Lean will unfold the definition of
f
when it has to.
In particular, when we prove
f
1
≤
f
0
with
le_refl
,
Lean reduces
f
1
and
f
0
to
0
.
Use
le_of_not_gt
to prove the following:
example
(
x
:
ℝ
)
(
h
:
∀
ε
>
0
,
x
<
ε
)
:
x
≤
0
:=
by
sorry
Implicit in many of the proofs we have just done
is the fact that if
P
is any property,
saying that there is nothing with property
P
is the same as saying that everything fails to have
property
P
,
and saying that not everything has property
P
is equivalent to saying that something fails to have property
P
.
In other words, all four of the following implications
are valid (but one of them cannot be proved with what we explained so
far):
variable
{
α
:
Type
*
}
(
P
:
α
→
Prop
)
(
Q
:
Prop
)
example
(
h
:
¬∃
x
,
P
x
)
:
∀
x
,
¬
P
x
:=
by
sorry
example
(
h
:
∀
x
,
¬
P
x
)
:
¬∃
x
,
P
x
:=
by
sorry
example
(
h
:
¬∀
x
,
P
x
)
:
∃
x
,
¬
P
x
:=
by
sorry
example
(
h
:
∃
x
,
¬
P
x
)
:
¬∀
x
,
P
x
:=
by
sorry
The first, second, and fourth are straightforward to
prove using the methods you have already seen.
We encourage you to try it.
The third is more difficult, however,
because it concludes that an object exists
from the fact that its nonexistence is contradictory.
This is an instance of
classical
mathematical reasoning.
We can use proof by contradiction
to prove the third implication as follows.
example
(
h
:
¬∀
x
,
P
x
)
:
∃
x
,
¬
P
x
:=
by
by_contra
h'
apply
h
intro
x
show
P
x
by_contra
h''
exact
h'
⟨
x
,
h''
⟩
Make sure you understand how this works.
The
by_contra
tactic
allows us to prove a goal
Q
by assuming
¬
Q
and deriving a contradiction.
In fact, it is equivalent to using the
equivalence
not_not
:
¬
¬
Q
↔
Q
.
Confirm that you can prove the forward direction
of this equivalence using
by_contra
,
while the reverse direction follows from the
ordinary rules for negation.
example
(
h
:
¬¬
Q
)
:
Q
:=
by
sorry
example
(
h
:
Q
)
:
¬¬
Q
:=
by
sorry
Use proof by contradiction to establish the following,
which is the converse of one of the implications we proved above.
(Hint: use
intro
first.)
example
(
h
:
¬
FnHasUb
f
)
:
∀
a
,
∃
x
,
f
x
>
a
:=
by
sorry
It is often tedious to work with compound statements with
a negation in front,
and it is a common mathematical pattern to replace such
statements with equivalent forms in which the negation
has been pushed inward.
To facilitate this, Mathlib offers a
push_neg
tactic,
which restates the goal in this way.
The command
push_neg
at
h
restates the hypothesis
h
.
example
(
h
:
¬∀
a
,
∃
x
,
f
x
>
a
)
:
FnHasUb
f
:=
by
push_neg
at
h
exact
h
example
(
h
:
¬
FnHasUb
f
)
:
∀
a
,
∃
x
,
f
x
>
a
:=
by
dsimp
only
[
FnHasUb
,
FnUb
]
at
h
push_neg
at
h
exact
h
In the second example, we use dsimp to
expand the definitions of
FnHasUb
and
FnUb
.
(We need to use
dsimp
rather than
rw
to expand
FnUb
,
because it appears in the scope of a quantifier.)
You can verify that in the examples above
with
¬∃
x,
P
x
and
¬∀
x,
P
x
,
the
push_neg
tactic does the expected thing.
Without even knowing how to use the conjunction
symbol,
you should be able to use
push_neg
to prove the following:
example
(
h
:
¬
Monotone
f
)
:
∃
x
y
,
x
≤
y
∧
f
y
<
f
x
:=
by
sorry
Mathlib also has a tactic,
contrapose
,
which transforms a goal
A
→
B
to
¬B
→
¬A
.
Similarly, given a goal of proving
B
from
hypothesis
h
:
A
,
contrapose
h
leaves you with a goal of proving
¬A
from hypothesis
¬B
.
Using
contrapose!
instead of
contrapose
applies
push_neg
to the goal and the relevant
hypothesis as well.
example
(
h
:
¬
FnHasUb
f
)
:
∀
a
,
∃
x
,
f
x
>
a
:=
by
contrapose
!
h
exact
h
example
(
x
:
ℝ
)
(
h
:
∀
ε
>
0
,
x
≤
ε
)
:
x
≤
0
:=
by
contrapose
!
h
use
x
/
2
constructor
<;>
linarith
We have not yet explained the
constructor
command
or the use of the semicolon after it,
but we will do that in the next section.
We close this section with
the principle of
ex falso
,
which says that anything follows from a contradiction.
In Lean, this is represented by
False.elim
,
which establishes
False
→
P
for any proposition
P
.
This may seem like a strange principle,
but it comes up fairly often.
We often prove a theorem by splitting on cases,
and sometimes we can show that one of
the cases is contradictory.
In that case, we need to assert that the contradiction
establishes the goal so we can move on to the next one.
(We will see instances of reasoning by cases in
Section 3.5
.)
Lean provides a number of ways of closing
a goal once a contradiction has been reached.
example
(
h
:
0
<
0
)
:
a
>
37
:=
by
exfalso
apply
lt_irrefl
0
h
example
(
h
:
0
<
0
)
:
a
>
37
:=
absurd
h
(
lt_irrefl
0
)
example
(
h
:
0
<
0
)
:
a
>
37
:=
by
have
h'
:
¬
0
<
0
:=
lt_irrefl
0
contradiction
The
exfalso
tactic replaces the current goal with
the goal of proving
False
.
Given
h
:
P
and
h'
:
¬
P
,
the term
absurd
h
h'
establishes any proposition.
Finally, the
contradiction
tactic tries to close a goal
by finding a contradiction in the hypotheses,
such as a pair of the form
h
:
P
and
h'
:
¬
P
.
Of course, in this example,
linarith
also works.
3.4.
Conjunction and Iff

You have already seen that the conjunction symbol,
∧
,
is used to express “and.”
The
constructor
tactic allows you to prove a statement of
the form
A
∧
B
by proving
A
and then proving
B
.
example
{
x
y
:
ℝ
}
(
h₀
:
x
≤
y
)
(
h₁
:
¬
y
≤
x
)
:
x
≤
y
∧
x
≠
y
:=
by
constructor
·
assumption
intro
h
apply
h₁
rw
[
h
]
In this example, the
assumption
tactic
tells Lean to find an assumption that will solve the goal.
Notice that the final
rw
finishes the goal by
applying the reflexivity of
≤
.
The following are alternative ways of carrying out
the previous examples using the anonymous constructor
angle brackets.
The first is a slick proof-term version of the
previous proof,
which drops into tactic mode at the keyword
by
.
example
{
x
y
:
ℝ
}
(
h₀
:
x
≤
y
)
(
h₁
:
¬
y
≤
x
)
:
x
≤
y
∧
x
≠
y
:=
⟨
h₀
,
fun
h
↦
h₁
(
by
rw
[
h
])⟩
example
{
x
y
:
ℝ
}
(
h₀
:
x
≤
y
)
(
h₁
:
¬
y
≤
x
)
:
x
≤
y
∧
x
≠
y
:=
have
h
:
x
≠
y
:=
by
contrapose
!
h₁
rw
[
h₁
]
⟨
h₀
,
h
⟩
Using
a conjunction instead of proving one involves unpacking the proofs of the
two parts.
You can use the
rcases
tactic for that,
as well as
rintro
or a pattern-matching
fun
,
all in a manner similar to the way they are used with
the existential quantifier.
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
rcases
h
with
⟨
h₀
,
h₁
⟩
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
example
{
x
y
:
ℝ
}
:
x
≤
y
∧
x
≠
y
→
¬
y
≤
x
:=
by
rintro
⟨
h₀
,
h₁
⟩
h'
exact
h₁
(
le_antisymm
h₀
h'
)
example
{
x
y
:
ℝ
}
:
x
≤
y
∧
x
≠
y
→
¬
y
≤
x
:=
fun
⟨
h₀
,
h₁
⟩
h'
↦
h₁
(
le_antisymm
h₀
h'
)
In analogy to the
obtain
tactic, there is also a pattern-matching
have
:
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
have
⟨
h₀
,
h₁
⟩
:=
h
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
In contrast to
rcases
, here the
have
tactic leaves
h
in the context.
And even though we won’t use them, once again we have the computer scientists’
pattern-matching syntax:
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
cases
h
case
intro
h₀
h₁
=>
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
cases
h
next
h₀
h₁
=>
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
match
h
with
|
⟨
h₀
,
h₁
⟩
=>
contrapose
!
h₁
exact
le_antisymm
h₀
h₁
In contrast to using an existential quantifier,
you can also extract proofs of the two components
of a hypothesis
h
:
A
∧
B
by writing
h.left
and
h.right
,
or, equivalently,
h.1
and
h.2
.
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
by
intro
h'
apply
h.right
exact
le_antisymm
h.left
h'
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
∧
x
≠
y
)
:
¬
y
≤
x
:=
fun
h'
↦
h.right
(
le_antisymm
h.left
h'
)
Try using these techniques to come up with various ways of proving of the following:
example
{
m
n
:
ℕ
}
(
h
:
m
∣
n
∧
m
≠
n
)
:
m
∣
n
∧
¬
n
∣
m
:=
sorry
You can nest uses of
∃
and
∧
with anonymous constructors,
rintro
, and
rcases
.
example
:
∃
x
:
ℝ
,
2
<
x
∧
x
<
4
:=
⟨
5
/
2
,
by
norm_num
,
by
norm_num
⟩
example
(
x
y
:
ℝ
)
:
(
∃
z
:
ℝ
,
x
<
z
∧
z
<
y
)
→
x
<
y
:=
by
rintro
⟨
z
,
xltz
,
zlty
⟩
exact
lt_trans
xltz
zlty
example
(
x
y
:
ℝ
)
:
(
∃
z
:
ℝ
,
x
<
z
∧
z
<
y
)
→
x
<
y
:=
fun
⟨
z
,
xltz
,
zlty
⟩
↦
lt_trans
xltz
zlty
You can also use the
use
tactic:
example
:
∃
x
:
ℝ
,
2
<
x
∧
x
<
4
:=
by
use
5
/
2
constructor
<;>
norm_num
example
:
∃
m
n
:
ℕ
,
4
<
m
∧
m
<
n
∧
n
<
10
∧
Nat.Prime
m
∧
Nat.Prime
n
:=
by
use
5
use
7
norm_num
example
{
x
y
:
ℝ
}
:
x
≤
y
∧
x
≠
y
→
x
≤
y
∧
¬
y
≤
x
:=
by
rintro
⟨
h₀
,
h₁
⟩
use
h₀
exact
fun
h'
↦
h₁
(
le_antisymm
h₀
h'
)
In the first example, the semicolon after the
constructor
command tells Lean to use the
norm_num
tactic on both of the goals that result.
In Lean,
A
↔
B
is
not
defined to be
(A
→
B)
∧
(B
→
A)
,
but it could have been,
and it behaves roughly the same way.
You have already seen that you can write
h.mp
and
h.mpr
or
h.1
and
h.2
for the two directions of
h
:
A
↔
B
.
You can also use
cases
and friends.
To prove an if-and-only-if statement,
you can use
constructor
or angle brackets,
just as you would if you were proving a conjunction.
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
)
:
¬
y
≤
x
↔
x
≠
y
:=
by
constructor
·
contrapose
!
rintro
rfl
rfl
contrapose
!
exact
le_antisymm
h
example
{
x
y
:
ℝ
}
(
h
:
x
≤
y
)
:
¬
y
≤
x
↔
x
≠
y
:=
⟨
fun
h₀
h₁
↦
h₀
(
by
rw
[
h₁
]),
fun
h₀
h₁
↦
h₀
(
le_antisymm
h
h₁
)⟩
The last proof term is inscrutable. Remember that you can
use underscores while writing an expression like that to
see what Lean expects.
Try out the various techniques and gadgets you have just seen
in order to prove the following:
example
{
x
y
:
ℝ
}
:
x
≤
y
∧
¬
y
≤
x
↔
x
≤
y
∧
x
≠
y
:=
sorry
For a more interesting exercise, show that for any
two real numbers
x
and
y
,
x^2
+
y^2
=
0
if and only if
x
=
0
and
y
=
0
.
We suggest proving an auxiliary lemma using
linarith
,
pow_two_nonneg
, and
pow_eq_zero
.
theorem
aux
{
x
y
:
ℝ
}
(
h
:
x
^
2
+
y
^
2
=
0
)
:
x
=
0
:=
have
h'
:
x
^
2
=
0
:=
by
sorry
pow_eq_zero
h'
example
(
x
y
:
ℝ
)
:
x
^
2
+
y
^
2
=
0
↔
x
=
0
∧
y
=
0
:=
sorry
In Lean, bi-implication leads a double-life.
You can treat it like a conjunction and use its two
parts separately.
But Lean also knows that it is a reflexive, symmetric,
and transitive relation between propositions,
and you can also use it with
calc
and
rw
.
It is often convenient to rewrite a statement to
an equivalent one.
In the next example, we use
abs_lt
to
replace an expression of the form
|x|
<
y
by the equivalent expression
-
y
<
x
∧
x
<
y
,
and in the one after that we use
Nat.dvd_gcd_iff
to replace an expression of the form
m
∣
Nat.gcd
n
k
by the equivalent expression
m
∣
n
∧
m
∣
k
.
example
(
x
:
ℝ
)
:
|
x
+
3
|
<
5
→
-
8
<
x
∧
x
<
2
:=
by
rw
[
abs_lt
]
intro
h
constructor
<;>
linarith
example
:
3
∣
Nat.gcd
6
15
:=
by
rw
[
Nat.dvd_gcd_iff
]
constructor
<;>
norm_num
See if you can use
rw
with the theorem below
to provide a short proof that negation is not a
nondecreasing function. (Note that
push_neg
won’t
unfold definitions for you, so the
rw
[Monotone]
in
the proof of the theorem is needed.)
theorem
not_monotone_iff
{
f
:
ℝ
→
ℝ
}
:
¬
Monotone
f
↔
∃
x
y
,
x
≤
y
∧
f
x
>
f
y
:=
by
rw
[
Monotone
]
push_neg
rfl
example
:
¬
Monotone
fun
x
:
ℝ
↦
-
x
:=
by
sorry
The remaining exercises in this section are designed
to give you some more practice with conjunction and
bi-implication. Remember that a
partial order
is a
binary relation that is transitive, reflexive, and
antisymmetric.
An even weaker notion sometimes arises:
a
preorder
is just a reflexive, transitive relation.
For any pre-order
≤
,
Lean axiomatizes the associated strict pre-order by
a
<
b
↔
a
≤
b
∧
¬
b
≤
a
.
Show that if
≤
is a partial order,
then
a
<
b
is equivalent to
a
≤
b
∧
a
≠
b
:
variable
{
α
:
Type
*
}
[
PartialOrder
α
]
variable
(
a
b
:
α
)
example
:
a
<
b
↔
a
≤
b
∧
a
≠
b
:=
by
rw
[
lt_iff_le_not_le
]
sorry
Beyond logical operations, you do not need
anything more than
le_refl
and
le_trans
.
Show that even in the case where
≤
is only assumed to be a preorder,
we can prove that the strict order is irreflexive
and transitive.
In the second example,
for convenience, we use the simplifier rather than
rw
to express
<
in terms of
≤
and
¬
.
We will come back to the simplifier later,
but here we are only relying on the fact that it will
use the indicated lemma repeatedly, even if it needs
to be instantiated to different values.
variable
{
α
:
Type
*
}
[
Preorder
α
]
variable
(
a
b
c
:
α
)
example
:
¬
a
<
a
:=
by
rw
[
lt_iff_le_not_le
]
sorry
example
:
a
<
b
→
b
<
c
→
a
<
c
:=
by
simp
only
[
lt_iff_le_not_le
]
sorry
3.5.
Disjunction

The canonical way to prove a disjunction
A
∨
B
is to prove
A
or to prove
B
.
The
left
tactic chooses
A
,
and the
right
tactic chooses
B
.
variable
{
x
y
:
ℝ
}
example


incrementally. Start writing the proof from the top down, using
sorry
to fill in subproofs. Make sure Lean accepts the term with
all the
sorry
's; if not, there are errors that you need to
correct. Then go back and replace each
sorry
with an actual proof,
until no more remain.
Here is another useful trick. Instead of using
sorry
, you can use
an underscore
_
as a placeholder. Recall this tells Lean that
the argument is implicit, and should be filled in automatically. If
Lean tries to do so and fails, it returns with an error message "don't
know how to synthesize placeholder," followed by the type of
the term it is expecting, and all the objects and hypotheses available
in the context. In other words, for each unresolved placeholder, Lean
reports the subgoal that needs to be filled at that point. You can
then construct a proof by incrementally filling in these placeholders.
For reference, here are two sample proofs of validities taken from the
list above.
open Classical

-- distributivity
example (p q r : Prop) : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) :=
  Iff.intro
    (fun h : p ∧ (q ∨ r) =>
      have hp : p := h.left
      Or.elim (h.right)
        (fun hq : q =>
          show (p ∧ q) ∨ (p ∧ r) from Or.inl ⟨hp, hq⟩)
        (fun hr : r =>
          show (p ∧ q) ∨ (p ∧ r) from Or.inr ⟨hp, hr⟩))
    (fun h : (p ∧ q) ∨ (p ∧ r) =>
      Or.elim h
        (fun hpq : p ∧ q =>
          have hp : p := hpq.left
          have hq : q := hpq.right
          show p ∧ (q ∨ r) from ⟨hp, Or.inl hq⟩)
        (fun hpr : p ∧ r =>
          have hp : p := hpr.left
          have hr : r := hpr.right
          show p ∧ (q ∨ r) from ⟨hp, Or.inr hr⟩))

-- an example that requires classical reasoning
example (p q : Prop) : ¬(p ∧ ¬q) → (p → q) :=
  fun h : ¬(p ∧ ¬q) =>
  fun hp : p =>
  show q from
    Or.elim (em q)
      (fun hq : q => hq)
      (fun hnq : ¬q => absurd (And.intro hp hnq) h)
Exercises
Prove the following identities, replacing the "sorry" placeholders with actual proofs.
variable (p q r : Prop)

-- commutativity of ∧ and ∨
example : p ∧ q ↔ q ∧ p := sorry
example : p ∨ q ↔ q ∨ p := sorry

-- associativity of ∧ and ∨
example : (p ∧ q) ∧ r ↔ p ∧ (q ∧ r) := sorry
example : (p ∨ q) ∨ r ↔ p ∨ (q ∨ r) := sorry

-- distributivity
example : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) := sorry
example : p ∨ (q ∧ r) ↔ (p ∨ q) ∧ (p ∨ r) := sorry

-- other properties
example : (p → (q → r)) ↔ (p ∧ q → r) := sorry
example : ((p ∨ q) → r) ↔ (p → r) ∧ (q → r) := sorry
example : ¬(p ∨ q) ↔ ¬p ∧ ¬q := sorry
example : ¬p ∨ ¬q → ¬(p ∧ q) := sorry
example : ¬(p ∧ ¬p) := sorry
example : p ∧ ¬q → ¬(p → q) := sorry
example : ¬p → (p → q) := sorry
example : (¬p ∨ q) → (p → q) := sorry
example : p ∨ False ↔ p := sorry
example : p ∧ False ↔ False := sorry
example : (p → q) → (¬q → ¬p) := sorry
Prove the following identities, replacing the "sorry" placeholders
with actual proofs. These require classical reasoning.
open Classical

variable (p q r : Prop)

example : (p → q ∨ r) → ((p → q) ∨ (p → r)) := sorry
example : ¬(p ∧ q) → ¬p ∨ ¬q := sorry
example : ¬(p → q) → p ∧ ¬q := sorry
example : (p → q) → (¬p ∨ q) := sorry
example : (¬q → ¬p) → (p → q) := sorry
example : p ∨ ¬p := sorry
example : (((p → q) → p) → p) := sorry
Prove
¬(p ↔ ¬p)
without using classical logic.