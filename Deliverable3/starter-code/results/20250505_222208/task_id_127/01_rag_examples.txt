2. Basics — Mathematics in Lean 0.1 documentation
Mathematics in Lean
1. Introduction
2. Basics
2.1. Calculating
2.2. Proving Identities in Algebraic Structures
2.3. Using Theorems and Lemmas
2.4. More examples using apply and rw
2.5. Proving Facts about Algebraic Structures
3. Logic
4. Sets and Functions
5. Elementary Number Theory
6. Structures
7. Hierarchies
8. Groups and Rings
9. Linear algebra
10. Topology
11. Differential Calculus
12. Integration and Measure Theory
Index
Mathematics in Lean
2.
Basics
View page source
2.
Basics

This chapter is designed to introduce you to the nuts and
bolts of mathematical reasoning in Lean: calculating,
applying lemmas and theorems,
and reasoning about generic structures.
2.1.
Calculating

We generally learn to carry out mathematical calculations
without thinking of them as proofs.
But when we justify each step in a calculation,
as Lean requires us to do,
the net result is a proof that the left-hand side of the calculation
is equal to the right-hand side.
In Lean, stating a theorem is tantamount to stating a goal,
namely, the goal of proving the theorem.
Lean provides the rewriting tactic
rw
,
to replace the left-hand side of an identity by the right-hand side
in the goal. If
a
,
b
, and
c
are real numbers,
mul_assoc
a
b
c
is the identity
a
*
b
*
c
=
a
*
(b
*
c)
and
mul_comm
a
b
is the identity
a
*
b
=
b
*
a
.
Lean provides automation that generally eliminates the need
to refer the facts like these explicitly,
but they are useful for the purposes of illustration.
In Lean, multiplication associates to the left,
so the left-hand side of
mul_assoc
could also be written
(a
*
b)
*
c
.
However, it is generally good style to be mindful of Lean’s
notational conventions and leave out parentheses when Lean does as well.
Let’s try out
rw
.
example
(
a
b
c
:
ℝ
)
:
a
*
b
*
c
=
b
*
(
a
*
c
)
:=
by
rw
[
mul_comm
a
b
]
rw
[
mul_assoc
b
a
c
]
The
import
lines at the beginning of the associated examples file
import the theory of the real numbers from Mathlib, as well as useful automation.
For the sake of brevity,
we generally suppress information like this in the textbook.
You are welcome to make changes to see what happens.
You can type the
ℝ
character as
\R
or
\real
in VS Code.
The symbol doesn’t appear until you hit space or the tab key.
If you hover over a symbol when reading a Lean file,
VS Code will show you the syntax that can be used to enter it.
If you are curious to see all available abbreviations, you can hit Ctrl-Shift-P
and then type abbreviations to get access to the
Lean
4:
Show
Unicode
Input
Abbreviations
command.
If your keyboard does not have an easily accessible backslash,
you can change the leading character by changing the
lean4.input.leader
setting.
When a cursor is in the middle of a tactic proof,
Lean reports on the current
proof state
in the
Lean Infoview
window.
As you move your cursor past each step of the proof,
you can see the state change.
A typical proof state in Lean might look as follows:
1
goal
x
y
:
ℕ
,
h₁
:
Prime
x
,
h₂
:
¬
Even
x
,
h₃
:
y
>
x
⊢
y
≥
4
The lines before the one that begins with
⊢
denote the
context
:
they are the objects and assumptions currently at play.
In this example, these include two objects,
x
and
y
,
each a natural number.
They also include three assumptions,
labelled
h₁
,
h₂
, and
h₃
.
In Lean, everything in a context is labelled with an identifier.
You can type these subscripted labels as
h\1
,
h\2
, and
h\3
,
but any legal identifiers would do:
you can use
h1
,
h2
,
h3
instead,
or
foo
,
bar
, and
baz
.
The last line represents the
goal
,
that is, the fact to be proved.
Sometimes people use
target
for the fact to be proved,
and
goal
for the combination of the context and the target.
In practice, the intended meaning is usually clear.
Try proving these identities,
in each case replacing
sorry
by a tactic proof.
With the
rw
tactic, you can use a left arrow (
\l
)
to reverse an identity.
For example,
rw
[←
mul_assoc
a
b
c]
replaces
a
*
(b
*
c)
by
a
*
b
*
c
in the current goal. Note that
the left-pointing arrow refers to going from right to left in the identity provided
by
mul_assoc
, it has nothing to do with the left or right side of the goal.
example
(
a
b
c
:
ℝ
)
:
c
*
b
*
a
=
b
*
(
a
*
c
)
:=
by
sorry
example
(
a
b
c
:
ℝ
)
:
a
*
(
b
*
c
)
=
b
*
(
a
*
c
)
:=
by
sorry
You can also use identities like
mul_assoc
and
mul_comm
without arguments.
In this case, the rewrite tactic tries to match the left-hand side with
an expression in the goal,
using the first pattern it finds.
example
(
a
b
c
:
ℝ
)
:
a
*
b
*
c
=
b
*
c
*
a
:=
by
rw
[
mul_assoc
]
rw
[
mul_comm
]
You can also provide
partial
information.
For example,
mul_comm
a
matches any pattern of the form
a
*
?
and rewrites it to
?
*
a
.
Try doing the first of these examples without
providing any arguments at all,
and the second with only one argument.
example
(
a
b
c
:
ℝ
)
:
a
*
(
b
*
c
)
=
b
*
(
c
*
a
)
:=
by
sorry
example
(
a
b
c
:
ℝ
)
:
a
*
(
b
*
c
)
=
b
*
(
a
*
c
)
:=
by
sorry
You can also use
rw
with facts from the local context.
example
(
a
b
c
d
e
f
:
ℝ
)
(
h
:
a
*
b
=
c
*
d
)
(
h'
:
e
=
f
)
:
a
*
(
b
*
e
)
=
c
*
(
d
*
f
)
:=
by
rw
[
h'
]
rw
[
←
mul_assoc
]
rw
[
h
]
rw
[
mul_assoc
]
Try these, using the theorem
sub_self
for the second one:
example
(
a
b
c
d
e
f
:
ℝ
)
(
h
:
b
*
c
=
e
*
f
)
:
a
*
b
*
c
*
d
=
a
*
e
*
f
*
d
:=
by
sorry
example
(
a
b
c
d
:
ℝ
)
(
hyp
:
c
=
b
*
a
-
d
)
(
hyp'
:
d
=
a
*
b
)
:
c
=
0
:=
by
sorry
Multiple rewrite commands can be carried out with a single command,
by listing the relevant identities separated by commas inside the square brackets.
example
(
a
b
c
d
e
f
:
ℝ
)
(
h
:
a
*
b
=
c
*
d
)
(
h'
:
e
=
f
)
:
a
*
(
b
*
e
)
=
c
*
(
d
*
f
)
:=
by
rw
[
h'
,
←
mul_assoc
,
h
,
mul_assoc
]
You still see the incremental progress by placing the cursor after
a comma in any list of rewrites.
Another trick is that we can declare variables once and for all outside
an example or theorem. Lean then includes them automatically.
variable
(
a
b
c
d
e
f
:
ℝ
)
example
(
h
:
a
*
b
=
c
*
d
)
(
h'
:
e
=
f
)
:
a
*
(
b
*
e
)
=
c
*
(
d
*
f
)
:=
by
rw
[
h'
,
←
mul_assoc
,
h
,
mul_assoc
]
Inspection of the tactic state at the beginning of the above proof
reveals that Lean indeed included all variables.
We can delimit the scope of the declaration by putting it
in a
section
...
end
block.
Finally, recall from the introduction that Lean provides us with a
command to determine the type of an expression:
section
variable
(
a
b
c
:
ℝ
)
#check
a
#check
a
+
b
#check
(
a
:
ℝ
)
#check
mul_comm
a
b
#check
(
mul_comm
a
b
:
a
*
b
=
b
*
a
)
#check
mul_assoc
c
a
b
#check
mul_comm
a
#check
mul_comm
end
The
#check
command works for both objects and facts.
In response to the command
#check
a
, Lean reports that
a
has type
ℝ
.
In response to the command
#check
mul_comm
a
b
,
Lean reports that
mul_comm
a
b
is a proof of the fact
a
*
b
=
b
*
a
.
The command
#check
(a
:
ℝ)
states our expectation that the
type of
a
is
ℝ
,
and Lean will raise an error if that is not the case.
We will explain the output of the last three
#check
commands later,
but in the meanwhile, you can take a look at them,
and experiment with some
#check
commands of your own.
Let’s try some more examples. The theorem
two_mul
a
says
that
2
*
a
=
a
+
a
. The theorems
add_mul
and
mul_add
express the distributivity of multiplication over addition,
and the theorem
add_assoc
expresses the associativity of addition.
Use the
#check
command to see the precise statements.
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
rw
[
mul_add
,
add_mul
,
add_mul
]
rw
[
←
add_assoc
,
add_assoc
(
a
*
a
)]
rw
[
mul_comm
b
a
,
←
two_mul
]
Whereas it is possible to figure out what is going on in this proof
by stepping through it in the editor,
it is hard to read on its own.
Lean provides a more structured way of writing proofs like this
using the
calc
keyword.
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
calc
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
b
*
a
+
(
a
*
b
+
b
*
b
)
:=
by
rw
[
mul_add
,
add_mul
,
add_mul
]
_
=
a
*
a
+
(
b
*
a
+
a
*
b
)
+
b
*
b
:=
by
rw
[
←
add_assoc
,
add_assoc
(
a
*
a
)]
_
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
rw
[
mul_comm
b
a
,
←
two_mul
]
Notice that the proof does
not
begin with
by
:
an expression that begins with
calc
is a
proof term
.
A
calc
expression can also be used inside a tactic proof,
but Lean interprets it as the instruction to use the resulting
proof term to solve the goal.
The
calc
syntax is finicky: the underscores and justification
have to be in the format indicated above.
Lean uses indentation to determine things like where a block
of tactics or a
calc
block begins and ends;
try changing the indentation in the proof above to see what happens.
One way to write a
calc
proof is to outline it first
using the
sorry
tactic for justification,
make sure Lean accepts the expression modulo these,
and then justify the individual steps using tactics.
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
calc
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
b
*
a
+
(
a
*
b
+
b
*
b
)
:=
by
sorry
_
=
a
*
a
+
(
b
*
a
+
a
*
b
)
+
b
*
b
:=
by
sorry
_
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
sorry
Try proving the following identity using both a pure
rw
proof
and a more structured
calc
proof:
example
:
(
a
+
b
)
*
(
c
+
d
)
=
a
*
c
+
a
*
d
+
b
*
c
+
b
*
d
:=
by
sorry
The following exercise is a little more challenging.
You can use the theorems listed underneath.
example
(
a
b
:
ℝ
)
:
(
a
+
b
)
*
(
a
-
b
)
=
a
^
2
-
b
^
2
:=
by
sorry
#check
pow_two
a
#check
mul_sub
a
b
c
#check
add_mul
a
b
c
#check
add_sub
a
b
c
#check
sub_sub
a
b
c
#check
add_zero
a
We can also perform rewriting in an assumption in the context.
For example,
rw
[mul_comm
a
b]
at
hyp
replaces
a
*
b
by
b
*
a
in the assumption
hyp
.
example
(
a
b
c
d
:
ℝ
)
(
hyp
:
c
=
d
*
a
+
b
)
(
hyp'
:
b
=
a
*
d
)
:
c
=
2
*
a
*
d
:=
by
rw
[
hyp'
]
at
hyp
rw
[
mul_comm
d
a
]
at
hyp
rw
[
←
two_mul
(
a
*
d
)]
at
hyp
rw
[
←
mul_assoc
2
a
d
]
at
hyp
exact
hyp
In the last step, the
exact
tactic can use
hyp
to solve the goal
because at that point
hyp
matches the goal exactly.
We close this section by noting that Mathlib provides a
useful bit of automation with a
ring
tactic,
which is designed to prove identities in any commutative ring as long as they follow
purely from the ring axioms, without using any local assumption.
example
:
c
*
b
*
a
=
b
*
(
a
*
c
)
:=
by
ring
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
ring
example
:
(
a
+
b
)
*
(
a
-
b
)
=
a
^
2
-
b
^
2
:=
by
ring
example
(
hyp
:
c
=
d
*
a
+
b
)
(
hyp'
:
b
=
a
*
d
)
:
c
=
2
*
a
*
d
:=
by
rw
[
hyp
,
hyp'
]
ring
The
ring
tactic is imported indirectly when we
import
Mathlib.Data.Real.Basic
,
but we will see in the next section that it can be used
for calculations on structures other than the real numbers.
It can be imported explicitly with the command
import
Mathlib.Tactic
.
We will see there are similar tactics for other common kind of algebraic
structures.
There is a variation of
rw
called
nth_rw
that allows you to replace only particular instances of an expression in the goal.
Possible matches are enumerated starting with 1,
so in the following example,
nth_rw
2
[h]
replaces the second
occurrence of
a
+
b
with
c
.
example
(
a
b
c
:
ℕ
)
(
h
:
a
+
b
=
c
)
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
c
+
b
*
c
:=
by
nth_rw
2
[
h
]
rw
[
add_mul
]
2.2.
Proving Identities in Algebraic Structures

Mathematically, a ring consists of a collection of objects,
\(R\)
, operations
\(+\)
\(\times\)
, and constants
\(0\)
and
\(1\)
, and an operation
\(x \mapsto -x\)
such that:
\(R\)
with
\(+\)
is an
abelian group
, with
\(0\)
as the additive identity and negation as inverse.
Multiplication is associative with identity
\(1\)
,
and multiplication distributes over addition.
In Lean, the collection of objects is represented as a
type
,
R
.
The ring axioms are as follows:
variable
(
R
:
Type
*
)
[
Ring
R
]
#check
(
add_assoc
:
∀
a
b
c
:
R
,
a
+
b
+
c
=
a
+
(
b
+
c
))
#check
(
add_comm
:
∀
a
b
:
R
,
a
+
b
=
b
+
a
)
#check
(
zero_add
:
∀
a
:
R
,
0
+
a
=
a
)
#check
(
neg_add_cancel
:
∀
a
:
R
,
-
a
+
a
=
0
)
#check
(
mul_assoc
:
∀
a
b
c
:
R
,
a
*
b
*
c
=
a
*
(
b
*
c
))
#check
(
mul_one
:
∀
a
:
R
,
a
*
1
=
a
)
#check
(
one_mul
:
∀
a
:
R
,
1
*
a
=
a
)
#check
(
mul_add
:
∀
a
b
c
:
R
,
a
*
(
b
+
c
)
=
a
*
b
+
a
*
c
)
#check
(
add_mul
:
∀
a
b
c
:
R
,
(
a
+
b
)
*
c
=
a
*
c
+
b
*
c
)
You will learn more about the square brackets in the first line later,
but for the time being,
suffice it to say that the declaration gives us a type,
R
,
and a ring structure on
R
.
Lean then allows us to use generic ring notation with elements of
R
,
and to make use of a library of theorems about rings.
The names of some of the theorems should look familiar:
they are exactly the ones we used to calculate with the real numbers
in the last section.
Lean is good not only for proving things about concrete mathematical
structures like the natural numbers and the integers,
but also for proving things about abstract structures,
characterized axiomatically, like rings.
Moreover, Lean supports
generic reasoning
about
both abstract and concrete structures,
and can be trained to recognize appropriate instances.
So any theorem about rings can be applied to concrete rings
like the integers,
ℤ
, the rational numbers,
ℚ
,
and the complex numbers
ℂ
.
It can also be applied to any instance of an abstract
structure that extends rings,
such as any ordered ring or any field.
Not all important properties of the real numbers hold in an
arbitrary ring, however.
For example, multiplication on the real numbers
is commutative,
but that does not hold in general.
If you have taken a course in linear algebra,
you will recognize that, for every
\(n\)
,
the
\(n\)
by
\(n\)
matrices of real numbers
form a ring in which commutativity usually fails. If we declare
R
to be a
commutative
ring, in fact, all the theorems
in the last section continue to hold when we replace
ℝ
by
R
.
variable
(
R
:
Type
*
)
[
CommRing
R
]
variable
(
a
b
c
d
:
R
)
example
:
c
*
b
*
a
=
b
*
(
a
*
c
)
:=
by
ring
example
:
(
a
+
b
)
*
(
a
+
b
)
=
a
*
a
+
2
*
(
a
*
b
)
+
b
*
b
:=
by
ring
example
:
(
a
+
b
)
*
(
a
-
b
)
=
a
^
2
-
b
^
2
:=
by
ring
example
(
hyp
:
c
=
d
*
a
+
b
)
(
hyp'
:
b
=
a
*
d
)
:
c
=
2
*
a
*
d
:=
by
rw
[
hyp
,
hyp'
]
ring
We leave it to you to check that all the other proofs go through unchanged.
Notice that when a proof is short, like
by
ring
or
by
linarith
or
by
sorry
,
it is common (and permissible) to put it on the same line as
the
by
.
Good proof-writing style should strike a balance between concision and readability.
The goal of this section is to strengthen the skills
you have developed in the last section
and apply them to reasoning axiomatically about rings.
We will start with the axioms listed above,
and use them to derive other facts.
Most of the facts we prove are already in Mathlib.
We will give the versions we prove the same names
to help you learn the contents of the library
as well as the naming conventions.
Lean provides an organizational mechanism similar
to those used in programming languages:
when a definition or theorem
foo
is introduced in a
namespace
bar
, its full name is
bar.foo
.
The command
open
bar
later
opens
the namespace,
which allows us to use the shorter name
foo
.
To avoid errors due to name clashes,
in the next example we put our versions of the library
theorems in a new namespace called
MyRing.
The next example shows that we do not need
add_zero
or
add_neg_cancel
as ring axioms, because they follow from the other axioms.
namespace
MyRing
variable
{
R
:
Type
*
}
[
Ring
R
]
theorem
add_zero
(
a
:
R
)
:
a
+
0
=
a
:=
by
rw
[
add_comm
,
zero_add
]
theorem
add_neg_cancel
(
a
:
R
)
:
a
+
-
a
=
0
:=
by
rw
[
add_comm
,
neg_add_cancel
]
#check
MyRing.add_zero
#check
add_zero
end
MyRing
The net effect is that we can temporarily reprove a theorem in the library,
and then go on using the library version after that.
But don’t cheat!
In the exercises that follow, take care to use only the
general facts about rings that we have proved earlier in this section.
(If you are paying careful attention, you may have noticed that we
changed the round brackets in
(R
:
Type*)
for
curly brackets in
{R
:
Type*}
.
This declares
R
to be an
implicit argument
.
We will explain what this means in a moment,
but don’t worry about it in the meanwhile.)
Here is a useful theorem:
theorem
neg_add_cancel_left
(
a
b
:
R
)
:
-
a
+
(
a
+
b
)
=
b
:=
by
rw
[
←
add_assoc
,
neg_add_cancel
,
zero_add
]
Prove the companion version:
theorem
add_neg_cancel_right
(
a
b
:
R
)
:
a
+
b
+
-
b
=
a
:=
by
sorry
Use these to prove the following:
theorem
add_left_cancel
{
a
b
c
:
R
}
(
h
:
a
+
b
=
a
+
c
)
:
b
=
c
:=
by
sorry
theorem
add_right_cancel
{
a
b
c
:
R
}
(
h
:
a
+
b
=
c
+
b
)
:
a
=
c
:=
by
sorry
With enough planning, you can do each of them with three rewrites.
We will now explain the use of the curly braces.
Imagine you are in a situation where you have
a
,
b
, and
c
in your context,
as well as a hypothesis
h
:
a
+
b
=
a
+
c
,
and you would like to draw the conclusion
b
=
c
.
In Lean, you can apply a theorem to hypotheses and facts just
the same way that you can apply them to objects,
so you might think that
add_left_cancel
a
b
c
h
is a
proof of the fact
b
=
c
.
But notice that explicitly writing
a
,
b
, and
c
is redundant, because the hypothesis
h
makes it clear that
those are the objects we have in mind.
In this case, typing a few extra characters is not onerous,
but if we wanted to apply
add_left_cancel
to more complicated expressions,
writing them would be tedious.
In cases like these,
Lean allows us to mark arguments as
implicit
,
meaning that they are supposed to be left out and inferred by other means,
such as later arguments and hypotheses.
The curly brackets in
{a
b
c
:
R}
do exactly that.
So, given the statement of the theorem above,
the correct expression is simply
add_left_cancel
h
.
To illustrate, let us show that
a
*
0
=
0
follows from the ring axioms.
theorem
mul_zero
(
a
:
R
)
:
a
*
0
=
0
:=
by
have
h
:
a
*
0
+
a
*
0
=
a
*
0
+
0
:=
by
rw
[
←
mul_add
,
add_zero
,
add_zero
]
rw
[
add_left_cancel
h
]
We have used a new trick!
If you step through the proof,
you can see what is going on.
The
have
tactic introduces a new goal,
a
*
0
+
a
*
0
=
a
*
0
+
0
,
with the same context as the original goal.
The fact that the next line is indented indicates that Lean
is expecting a block of tactics that serves to prove this
new goal.
The indentation therefore promotes a modular style of proof:
the indented subproof establishes the goal
that was introduced by the
have
.
After that, we are back to proving the original goal,
except a new hypothesis
h
has been added:
having proved it, we are now free to use it.
At this point, the goal is exactly the result of
add_left_cancel
h
.
We could equally well have closed the proof with
apply
add_left_cancel
h
or
exact
add_left_cancel
h
.
The
exact
tactic takes as argument a proof term which completely proves the
current goal, without creating any new goal. The
apply
tactic is a variant
whose argument is not necessarily a complete proof. The missing pieces are either
inferred automatically by Lean or become new goals to prove.
While the
exact
tactic is technically redundant since it is strictly less powerful
than
apply
, it makes proof scripts slightly clearer to
human readers and easier to maintain when the library evolves.
Remember that multiplication is not assumed to be commutative,
so the following theorem also requires some work.
theorem
zero_mul
(
a
:
R
)
:
0
*
a
=
0
:=
by
sorry
By now, you should also be able replace each
sorry
in the next
exercise with a proof,
still using only facts about rings that we have
established in this section.
theorem
neg_eq_of_add_eq_zero
{
a
b
:
R
}
(
h
:
a
+
b
=
0
)
:
-
a
=
b
:=
by
sorry
theorem
eq_neg_of_add_eq_zero
{
a
b
:
R
}
(
h
:
a
+
b
=
0
)
:
a
=
-
b
:=
by
sorry
theorem
neg_zero
:
(
-
0
:
R
)
=
0
:=
by
apply
neg_eq_of_add_eq_zero
rw
[
add_zero
]
theorem
neg_neg
(
a
:
R
)
:
-
-
a
=
a
:=
by
sorry
We had to use the annotation
(-0
:
R)
instead of
0
in the third theorem
because without specifying
R
it is impossible for Lean to infer which
0
we have in mind,
and by default it would be interpreted as a natural number.
In Lean, subtraction in a ring is provably equal to
addition of the additive inverse.
example
(
a
b
:
R
)
:
a
-
b
=
a
+
-
b
:=
sub_eq_add_neg
a
b
On the real numbers, it is
defined
that way:
example
(
a
b
:
ℝ
)
:
a
-
b
=
a
+
-
b
:=
rfl
example
(
a
b
:
ℝ
)
:
a
-
b
=
a
+
-
b
:=
by
rfl
The proof term
rfl
is short for “reflexivity”.
Presenting it as a proof of
a
-
b
=
a
+
-b
forces Lean
to unfold the definition and recognize both sides as being the same.
The
rfl
tactic does the same.
This is an instance of what is known as a
definitional equality
in Lean’s underlying logic.
This means that not only can one rewrite with
sub_eq_add_neg
to replace
a
-
b
=
a
+
-b
,
but in some contexts, when dealing with the real numbers,
you can use the two sides of the equation interchangeably.
For example, you now have enough information to prove the theorem
self_sub
from the last section:
theorem
self_sub
(
a
:
R
)
:
a
-
a
=
0
:=
by
sorry
Show that you can prove this using
rw
,
but if you replace the arbitrary ring
R
by
the real numbers, you can also prove it
using either
apply
or
exact
.
Lean knows that
1
+
1
=
2
holds in any ring.
With a bit of effort,
you can use that to prove the theorem
two_mul
from
the last section:
theorem
one_add_one_eq_two
:
1
+
1
=
(
2
:
R
)
:=
by
norm_num
theorem
two_mul
(
a
:
R
)
:
2
*
a
=
a
+
a
:=
by
sorry
We close this section by noting that some of the facts about
addition and negation that we established above do not
need the full strength of the ring axioms, or even
commutativity of addition. The weaker notion of a
group
can be axiomatized as follows:
variable
(
A
:
Type
*
)
[
AddGroup
A
]
#check
(
add_assoc
:
∀
a
b
c
:
A
,
a
+
b
+
c
=
a
+
(
b
+
c
))
#check
(
zero_add
:
∀
a
:
A
,
0
+
a
=
a
)
#check
(
neg_add_cancel
:
∀
a
:
A
,
-
a
+
a
=
0
)
It is conventional to use additive notation when
the group operation is commutative,
and multiplicative notation otherwise.
So Lean defines a multiplicative version as well as the
additive version (and also their abelian variants,
AddCommGroup
and
CommGroup
).
variable
{
G
:
Type
*
}
[
Group
G
]
#check
(
mul_assoc
:
∀
a
b
c
:
G
,
a
*
b
*
c
=
a
*
(
b
*
c
))
#check
(
one_mul
:
∀
a
:
G
,
1
*
a
=
a
)
#check
(
inv_mul_cancel
:
∀
a
:
G
,
a
⁻¹
*
a
=
1
)
If you are feeling cocky, try proving the following facts about
groups, using only these axioms.
You will need to prove a number of helper lemmas along the way.
The proofs we have carried out in this section provide some hints.
theorem
mul_inv_cancel
(
a
:
G
)
:
a
*
a
⁻¹
=
1
:=
by
sorry
theorem
mul_one
(
a
:
G
)
:
a
*
1
=
a
:=
by
sorry
theorem
mul_inv_rev
(
a
b
:
G
)
:
(
a
*
b
)
⁻¹
=
b
⁻¹
*
a
⁻¹
:=
by
sorry
Explicitly invoking those lemmas is tedious, so Mathlib provides
tactics similar to
ring
in order to cover most uses:
group
is for non-commutative multiplicative groups,
abel
for abelian
additive groups, and
noncomm_ring
for non-commutative rings.
It may seem odd that the algebraic structures are called
Ring
and
CommRing
while the tactics are named
noncomm_ring
and
ring
. This is partly for historical reasons,
but also for the convenience of using a shorter name for the
tactic that deals with commutative rings, since it is used more often.
2.3.
Using Theorems and Lemmas

Rewriting is great for proving equations,
but what about other sorts of theorems?
For example, how can we prove an inequality,
like the fact that
\(a + e^b \le a + e^c\)
holds whenever
\(b \le c\)
?
We have already seen that theorems can be applied to arguments and hypotheses,
and that the
apply
and
exact
tactics can be used to solve goals.
In this section, we will make good use of these tools.
Consider the library theorems
le_refl
and
le_trans
:
#check
(
le_refl
:
∀
a
:
ℝ
,
a
≤
a
)
#check
(
le_trans
:
a
≤
b
→
b
≤
c
→
a
≤
c
)
As we explain in more detail in
Section 3.1
,
the implicit parentheses in the statement of
le_trans
associate to the right, so it should be interpreted as
a
≤
b
→
(b
≤
c
→
a
≤
c)
.
The library designers have set the arguments
a
,
b
and
c
to
le_trans
implicit,
so that Lean will
not
let you provide them explicitly (unless you
really insist, as we will discuss later).
Rather, it expects to infer them from the context in which they are used.
For example, when hypotheses

(
h
:
y
>
x
^
2
)
:
y
>
0
∨
y
<
-
1
:=
by
left
linarith
[
pow_two_nonneg
x
]
example
(
h
:
-
y
>
x
^
2
+
1
)
:
y
>
0
∨
y
<
-
1
:=
by
right
linarith
[
pow_two_nonneg
x
]
We cannot use an anonymous constructor to construct a proof
of an “or” because Lean would have to guess
which disjunct we are trying to prove.
When we write proof terms we can use
Or.inl
and
Or.inr
instead
to make the choice explicitly.
Here,
inl
is short for “introduction left” and
inr
is short for “introduction right.”
example
(
h
:
y
>
0
)
:
y
>
0
∨
y
<
-
1
:=
Or.inl
h
example
(
h
:
y
<
-
1
)
:
y
>
0
∨
y
<
-
1
:=
Or.inr
h
It may seem strange to prove a disjunction by proving one side
or the other.
In practice, which case holds usually depends on a case distinction
that is implicit or explicit in the assumptions and the data.
The
rcases
tactic allows us to make use of a hypothesis
of the form
A
∨
B
.
In contrast to the use of
rcases
with conjunction or an
existential quantifier,
here the
rcases
tactic produces
two
goals.
Both have the same conclusion, but in the first case,
A
is assumed to be true,
and in the second case,
B
is assumed to be true.
In other words, as the name suggests,
the
rcases
tactic carries out a proof by cases.
As usual, we can tell Lean what names to use for the hypotheses.
In the next example, we tell Lean
to use the name
h
on each branch.
example
:
x
<
|
y
|
→
x
<
y
∨
x
<
-
y
:=
by
rcases
le_or_gt
0
y
with
h
|
h
·
rw
[
abs_of_nonneg
h
]
intro
h
;
left
;
exact
h
·
rw
[
abs_of_neg
h
]
intro
h
;
right
;
exact
h
Notice that the pattern changes from
⟨h₀,
h₁⟩
in the case of
a conjunction to
h₀
|
h₁
in the case of a disjunction.
Think of the first pattern as matching against data the contains
both
an
h₀
and a
h₁
, whereas second pattern, with the bar,
matches against data that contains
either
an
h₀
or
h₁
.
In this case, because the two goals are separate, we have chosen
to use the same name,
h
, in each case.
The absolute value function is defined in such a way
that we can immediately prove that
x
≥
0
implies
|x|
=
x
(this is the theorem
abs_of_nonneg
)
and
x
<
0
implies
|x|
=
-x
(this is
abs_of_neg
).
The expression
le_or_gt
0
x
establishes
0
≤
x
∨
x
<
0
,
allowing us to split on those two cases.
Lean also supports the computer scientists’ pattern-matching
syntax for disjunction. Now the
cases
tactic is more attractive,
because it allows us to name each
case
, and name the hypothesis
that is introduced closer to where it is used.
example
:
x
<
|
y
|
→
x
<
y
∨
x
<
-
y
:=
by
cases
le_or_gt
0
y
case
inl
h
=>
rw
[
abs_of_nonneg
h
]
intro
h
;
left
;
exact
h
case
inr
h
=>
rw
[
abs_of_neg
h
]
intro
h
;
right
;
exact
h
The names
inl
and
inr
are short for “intro left” and “intro right,”
respectively. Using
case
has the advantage that you can prove the
cases in either order; Lean uses the tag to find the relevant goal.
If you don’t care about that, you can use
next
, or
match
,
or even a pattern-matching
have
.
example
:
x
<
|
y
|
→
x
<
y
∨
x
<
-
y
:=
by
cases
le_or_gt
0
y
next
h
=>
rw
[
abs_of_nonneg
h
]
intro
h
;
left
;
exact
h
next
h
=>
rw
[
abs_of_neg
h
]
intro
h
;
right
;
exact
h
example
:
x
<
|
y
|
→
x
<
y
∨
x
<
-
y
:=
by
match
le_or_gt
0
y
with
|
Or.inl
h
=>
rw
[
abs_of_nonneg
h
]
intro
h
;
left
;
exact
h
|
Or.inr
h
=>
rw
[
abs_of_neg
h
]
intro
h
;
right
;
exact
h
In the case of
match
, we need to use the full names
Or.inl
and
Or.inr
of the canonical ways to prove a disjunction.
In this textbook, we will generally use
rcases
to split on the
cases of a disjunction.
Try proving the triangle inequality using the
first two theorems in the next snippet.
They are given the same names they have in Mathlib.
namespace
MyAbs
theorem
le_abs_self
(
x
:
ℝ
)
:
x
≤
|
x
|
:=
by
sorry
theorem
neg_le_abs_self
(
x
:
ℝ
)
:
-
x
≤
|
x
|
:=
by
sorry
theorem
abs_add
(
x
y
:
ℝ
)
:
|
x
+
y
|
≤
|
x
|
+
|
y
|
:=
by
sorry
In case you enjoyed these (pun intended) and
you want more practice with disjunction,
try these.
theorem
lt_abs
:
x
<
|
y
|
↔
x
<
y
∨
x
<
-
y
:=
by
sorry
theorem
abs_lt
:
|
x
|
<
y
↔
-
y
<
x
∧
x
<
y
:=
by
sorry
You can also use
rcases
and
rintro
with nested disjunctions.
When these result in a genuine case split with multiple goals,
the patterns for each new goal are separated by a vertical bar.
example
{
x
:
ℝ
}
(
h
:
x
≠
0
)
:
x
<
0
∨
x
>
0
:=
by
rcases
lt_trichotomy
x
0
with
xlt
|
xeq
|
xgt
·
left
exact
xlt
·
contradiction
·
right
;
exact
xgt
You can still nest patterns and use the
rfl
keyword
to substitute equations:
example
{
m
n
k
:
ℕ
}
(
h
:
m
∣
n
∨
m
∣
k
)
:
m
∣
n
*
k
:=
by
rcases
h
with
⟨
a
,
rfl
⟩
|
⟨
b
,
rfl
⟩
·
rw
[
mul_assoc
]
apply
dvd_mul_right
·
rw
[
mul_comm
,
mul_assoc
]
apply
dvd_mul_right
See if you can prove the following with a single (long) line.
Use
rcases
to unpack the hypotheses and split on cases,
and use a semicolon and
linarith
to solve each branch.
example
{
z
:
ℝ
}
(
h
:
∃
x
y
,
z
=
x
^
2
+
y
^
2
∨
z
=
x
^
2
+
y
^
2
+
1
)
:
z
≥
0
:=
by
sorry
On the real numbers, an equation
x
*
y
=
0
tells us that
x
=
0
or
y
=
0
.
In Mathlib, this fact is known as
eq_zero_or_eq_zero_of_mul_eq_zero
,
and it is another nice example of how a disjunction can arise.
See if you can use it to prove the following:
example
{
x
:
ℝ
}
(
h
:
x
^
2
=
1
)
:
x
=
1
∨
x
=
-
1
:=
by
sorry
example
{
x
y
:
ℝ
}
(
h
:
x
^
2
=
y
^
2
)
:
x
=
y
∨
x
=
-
y
:=
by
sorry
Remember that you can use the
ring
tactic to help
with calculations.
In an arbitrary ring
\(R\)
, an element
\(x\)
such
that
\(x y = 0\)
for some nonzero
\(y\)
is called
a
left zero divisor
,
an element
\(x\)
such
that
\(y x = 0\)
for some nonzero
\(y\)
is called
a
right zero divisor
,
and an element that is either a left or right zero divisor
is called simply a
zero divisor
.
The theorem
eq_zero_or_eq_zero_of_mul_eq_zero
says that the real numbers have no nontrivial zero divisors.
A commutative ring with this property is called an
integral domain
.
Your proofs of the two theorems above should work equally well
in any integral domain:
variable
{
R
:
Type
*
}
[
CommRing
R
]
[
IsDomain
R
]
variable
(
x
y
:
R
)
example
(
h
:
x
^
2
=
1
)
:
x
=
1
∨
x
=
-
1
:=
by
sorry
example
(
h
:
x
^
2
=
y
^
2
)
:
x
=
y
∨
x
=
-
y
:=
by
sorry
In fact, if you are careful, you can prove the first
theorem without using commutativity of multiplication.
In that case, it suffices to assume that
R
is
a
Ring
instead of an
CommRing
.
Sometimes in a proof we want to split on cases
depending on whether some statement is true or not.
For any proposition
P
, we can use
em
P
:
P
∨
¬
P
.
The name
em
is short for “excluded middle.”
example
(
P
:
Prop
)
:
¬¬
P
→
P
:=
by
intro
h
cases
em
P
·
assumption
·
contradiction
Alternatively, you can use the
by_cases
tactic.
example
(
P
:
Prop
)
:
¬¬
P
→
P
:=
by
intro
h
by_cases
h'
:
P
·
assumption
contradiction
Notice that the
by_cases
tactic lets you
specify a label for the hypothesis that is
introduced in each branch,
in this case,
h'
:
P
in one and
h'
:
¬
P
in the other.
If you leave out the label,
Lean uses
h
by default.
Try proving the following equivalence,
using
by_cases
to establish one direction.
example
(
P
Q
:
Prop
)
:
P
→
Q
↔
¬
P
∨
Q
:=
by
sorry
3.6.
Sequences and Convergence

We now have enough skills at our disposal to do some real mathematics.
In Lean, we can represent a sequence
\(s_0, s_1, s_2, \ldots\)
of
real numbers as a function
s
:
ℕ
→
ℝ
.
Such a sequence is said to
converge
to a number
\(a\)
if for every
\(\varepsilon > 0\)
there is a point beyond which the sequence
remains within
\(\varepsilon\)
of
\(a\)
,
that is, there is a number
\(N\)
such that for every
\(n \ge N\)
,
\(| s_n - a | < \varepsilon\)
.
In Lean, we can render this as follows:
def
ConvergesTo
(
s
:
ℕ
→
ℝ
)
(
a
:
ℝ
)
:=
∀
ε
>
0
,
∃
N
,
∀
n
≥
N
,
|
s
n
-
a
|
<
ε
The notation
∀
ε
>
0,
...
is a convenient abbreviation
for
∀
ε,
ε
>
0
→
...
, and, similarly,
∀
n
≥
N,
...
abbreviates
∀
n,
n
≥
N
→
...
.
And remember that
ε
>
0
, in turn, is defined as
0
<
ε
,
and
n
≥
N
is defined as
N
≤
n
.
In this section, we’ll establish some properties of convergence.
But first, we will discuss three tactics for working with equality
that will prove useful.
The first, the
ext
tactic,
gives us a way of proving that two functions are equal.
Let
\(f(x) = x + 1\)
and
\(g(x) = 1 + x\)
be functions from reals to reals.
Then, of course,
\(f = g\)
, because they return the same
value for every
\(x\)
.
The
ext
tactic enables us to prove an equation between functions
by proving that their values are the same
at all the values of their arguments.
example
:
(
fun
x
y
:
ℝ
↦
(
x
+
y
)
^
2
)
=
fun
x
y
:
ℝ
↦
x
^
2
+
2
*
x
*
y
+
y
^
2
:=
by
ext
ring
We’ll see later that
ext
is actually more general, and also one can
specify the name of the variables that appear.
For instance you can try to replace
ext
with
ext
u
v
in the
above proof.
The second tactic, the
congr
tactic,
allows us to prove an equation between two expressions
by reconciling the parts that are different:
example
(
a
b
:
ℝ
)
:
|
a
|
=
|
a
-
b
+
b
|
:=
by
congr
ring
Here the
congr
tactic peels off the
abs
on each side,
leaving us to prove
a
=
a
-
b
+
b
.
Finally, the
convert
tactic is used to apply a theorem
to a goal when the conclusion of the theorem doesn’t quite match.
For example, suppose we want to prove
a
<
a
*
a
from
1
<
a
.
A theorem in the library,
mul_lt_mul_right
,
will let us prove
1
*
a
<
a
*
a
.
One possibility is to work backwards and rewrite the goal
so that it has that form.
Instead, the
convert
tactic lets us apply the theorem
as it is,
and leaves us with the task of proving the equations that
are needed to make the goal match.
example
{
a
:
ℝ
}
(
h
:
1
<
a
)
:
a
<
a
*
a
:=
by
convert
(
mul_lt_mul_right
_
)
.
2
h
·
rw
[
one_mul
]
exact
lt_trans
zero_lt_one
h
This example illustrates another useful trick: when we apply an
expression with an underscore
and Lean can’t fill it in for us automatically,
it simply leaves it for us as another goal.
The following shows that any constant sequence
\(a, a, a, \ldots\)
converges.
theorem
convergesTo_const
(
a
:
ℝ
)
:
ConvergesTo
(
fun
x
:
ℕ
↦
a
)
a
:=
by
intro
ε
εpos
use
0
intro
n
nge
rw
[
sub_self
,
abs_zero
]
apply
εpos
Lean has a tactic,
simp
, which can often save you the
trouble of carrying out steps like
rw
[sub_self,
abs_zero]
by hand.
We will tell you more about it soon.
For a more interesting theorem, let’s show that if
s
converges to
a
and
t
converges to
b
, then
fun
n
↦
s
n
+
t
n
converges to
a
+
b
.
It is helpful to have a clear pen-and-paper
proof in mind before you start writing a formal one.
Given
ε
greater than
0
,
the idea is to use the hypotheses to obtain an
Ns
such that beyond that point,
s
is within
ε
/
2
of
a
,
and an
Nt
such that beyond that point,
t
is within
ε
/
2
of
b
.
Then, whenever
n
is greater than or equal to the
maximum of
Ns
and
Nt
,
the sequence
fun
n
↦
s
n
+
t
n
should be within
ε
of
a
+
b
.
The following example begins to implement this strategy.
See if you can finish it off.
theorem
convergesTo_add
{
s
t
:
ℕ
→
ℝ
}
{
a
b
:
ℝ
}
(
cs
:
ConvergesTo
s
a
)
(
ct
:
ConvergesTo
t
b
)
:
ConvergesTo
(
fun
n
↦
s
n
+
t
n
)
(
a
+
b
)
:=
by
intro
ε
εpos
dsimp
-- this line is not needed but cleans up the goal a bit.
have
ε2pos
:
0
<
ε
/
2
:=
by
linarith
rcases
cs
(
ε
/
2
)
ε2pos
with
⟨
Ns
,
hs
⟩
rcases
ct
(
ε
/
2
)
ε2pos
with
⟨
Nt
,
ht
⟩
use
max
Ns
Nt
sorry
As hints, you can use
le_of_max_le_left
and
le_of_max_le_right
,
and
norm_num
can prove
ε
/
2
+
ε
/
2
=
ε
.
Also, it is helpful to use the
congr
tactic to
show that
|s
n
+
t
n
-
(a
+
b)|
is equal to
|(s
n
-
a)
+
(t
n
-
b)|,
since then you can use the triangle inequality.
Notice that we marked all the variables
s
,
t
,
a
, and
b
implicit because they can be inferred from the hypotheses.
Proving the same theorem with multiplication in place
of addition is tricky.
We will get there by proving some auxiliary statements first.
See if you can also finish off the next proof,
which shows that if
s
converges to
a
,
then
fun
n
↦
c
*
s
n
converges to
c
*
a
.
It is helpful to split into cases depending on whether
c
is equal to zero or not.
We have taken care of the zero case,
and we have left you to prove the result with
the extra assumption that
c
is nonzero.
theorem
convergesTo_mul_const
{
s
:
ℕ
→
ℝ
}
{
a
:
ℝ
}
(
c
:
ℝ
)
(
cs
:
ConvergesTo
s
a
)
:
ConvergesTo
(
fun
n
↦
c
*
s
n
)
(
c
*
a
)
:=
by
by_cases
h
:
c
=
0
·
convert
convergesTo_const
0
·
rw
[
h
]
ring
rw
[
h
]
ring
have
acpos
:
0
<
|
c
|
:=
abs_pos.mpr
h
sorry
The next theorem is also independently interesting:
it shows that a convergent sequence is eventually bounded
in absolute value.
We have started you off; see if you can finish it.
theorem
exists_abs_le_of_convergesTo
{
s
:
ℕ
→
ℝ
}
{
a
:
ℝ
}
(
cs
:
ConvergesTo
s
a
)
:
∃
N
b
,
∀
n
,
N
≤
n
→
|
s
n
|
<
b
:=
by
rcases
cs
1
zero_lt_one
with
⟨
N
,
h
⟩
use
N
,
|
a
|
+
1
sorry
In fact, the theorem could be strengthened to assert
that there is a bound
b
that holds for all values of
n
.
But this version is strong enough for our purposes,
and we will see at the end of this section that it
holds more generally.
The next lemma is auxiliary: we prove that if
s
converges to
a
and
t
converges to
0
,
then
fun
n
↦
s
n
*
t
n
converges to
0
.
To do so, we use the previous theorem to find a
B
that bounds
s
beyond some point
N₀
.
See if you can understand the strategy we have outlined
and finish the proof.
theorem
aux
{
s
t
:
ℕ
→
ℝ
}
{
a
:
ℝ
}
(
cs
:
ConvergesTo
s
a
)
(
ct
:
ConvergesTo
t
0
)
:
ConvergesTo
(
fun
n
↦
s
n
*
t
n
)
0
:=
by
intro
ε
εpos
dsimp
rcases
exists_abs_le_of_convergesTo
cs
with
⟨
N₀
,
B
,
h₀
⟩
have
Bpos
:
0
<
B
:=
lt_of_le_of_lt
(
abs_nonneg
_
)
(
h₀
N₀
(
le_refl
_
))
have
pos₀
:
ε
/
B
>
0
:=
div_pos
εpos
Bpos
rcases
ct
_
pos₀
with
⟨
N₁
,
h₁
⟩
sorry
If you have made it this far, congratulations!
We are now within striking distance of our theorem.
The following proof finishes it off.
theorem
convergesTo_mul
{
s
t
:
ℕ
→
ℝ
}
{
a
b
:
ℝ
}
(
cs
:
ConvergesTo
s
a
)
(
ct
:
ConvergesTo
t
b
)
:
ConvergesTo
(
fun
n
↦
s
n
*
t
n
)
(
a
*
b
)
:=
by
have
h₁
:
ConvergesTo
(
fun
n
↦
s
n
*
(
t
n
+
-
b
))
0
:=
by
apply
aux
cs
convert
convergesTo_add
ct
(
convergesTo_const
(
-
b
))
ring
have
:=
convergesTo_add
h₁
(
convergesTo_mul_const
b
cs
)
convert
convergesTo_add
h₁
(
convergesTo_mul_const
b
cs
)
using
1
·
ext
;
ring
ring
For another challenging exercise,
try filling out the following sketch of a proof that limits
are unique.
(If you are feeling bold,
you can delete the proof sketch and try proving it from scratch.)
theorem
convergesTo_unique
{
s
:
ℕ
→
ℝ
}
{
a
b
:
ℝ
}
(
sa
:
ConvergesTo
s
a
)
(
sb
:
ConvergesTo
s
b
)
:
a
=
b
:=
by
by_contra
abne
have
:
|
a
-
b
|
>
0
:=
by
sorry
let
ε
:=
|
a
-
b
|
/
2
have
εpos
:
ε
>
0
:=
by
change
|
a
-
b
|
/
2
>
0
linarith
rcases
sa
ε
εpos
with
⟨
Na
,
hNa
⟩
rcases
sb
ε
εpos
with
⟨
Nb
,
hNb
⟩
let
N
:=
max
Na
Nb
have
absa
:
|
s
N
-
a
|
<
ε
:=
by
sorry
have
absb
:
|
s
N
-
b
|
<
ε
:=
by
sorry
have
:
|
a
-
b
|
<
|
a
-
b
|
:=
by
sorry
exact
lt_irrefl
_
this
We close the section with the observation that our proofs can be generalized.
For example, the only properties that we have used of the
natural numbers is that their structure carries a partial order
with
min
and
max
.
You can check that everything still works if you replace
ℕ
everywhere by any linear order
α
:
variable
{
α
:
Type
*
}
[
LinearOrder
α
]
def
ConvergesTo'
(
s
:
α
→
ℝ
)
(
a
:
ℝ
)
:=
∀
ε
>
0
,
∃
N
,
∀
n
≥
N
,
|
s
n
-
a
|
<
ε
In
Section 10.1
, we will see that Mathlib has mechanisms
for dealing with convergence in vastly more general terms,
not only abstracting away particular features of the domain
and codomain,
but also abstracting over different types of convergence.
Previous
Next
© Copyright 2020, Jeremy Avigad, Patrick Massot.
Built with
Sphinx
using a
theme
provided by
Read the Docs
.
Quantifiers and Equality - Theorem Proving in Lean 4
Theorem Proving in Lean 4
1.
Introduction
2.
Dependent Type Theory
3.
Propositions and Proofs
4.
Quantifiers and Equality
5.
Tactics
6.
Interacting with Lean
7.
Inductive Types
8.
Induction and Recursion
9.
Structures and Records
10.
Type Classes
11.
The Conversion Tactic Mode
12.
Axioms and Computation
Light (default)
Rust
Coal
Navy
Ayu
Theorem Proving in Lean 4
Quantifiers and Equality
The last chapter introduced you to methods that construct proofs of
statements involving the propositional connectives. In this chapter,
we extend the repertoire of logical constructions to include the
universal and existential quantifiers, and the equality relation.
The Universal Quantifier
Notice that if
α
is any type, we can represent a unary predicate
p
on
α
as an object of type
α → Prop
. In that case, given
x : α
,
p x
denotes the assertion that
p
holds of
x
. Similarly, an object
r : α → α → Prop
denotes a binary
relation on
α
: given
x y : α
,
r x y
denotes the assertion
that
x
is related to
y
.
The universal quantifier,
∀ x : α, p x
is supposed to denote the
assertion that "for every
x : α
,
p x
" holds. As with the
propositional connectives, in systems of natural deduction, "forall"
is governed by an introduction and elimination rule. Informally, the
introduction rule states:
Given a proof of
p x
, in a context where
x : α
is arbitrary, we obtain a proof
∀ x : α, p x
.
The elimination rule states:
Given a proof
∀ x : α, p x
and any term
t : α
, we obtain a proof of
p t
.
As was the case for implication, the propositions-as-types
interpretation now comes into play. Remember the introduction and
elimination rules for dependent arrow types:
Given a term
t
of type
β x
, in a context where
x : α
is arbitrary, we have
(fun x : α => t) : (x : α) → β x
.
The elimination rule states:
Given a term
s : (x : α) → β x
and any term
t : α
, we have
s t : β t
.
In the case where
p x
has type
Prop
, if we replace
(x : α) → β x
with
∀ x : α, p x
, we can read these as the correct rules
for building proofs involving the universal quantifier.
The Calculus of Constructions therefore identifies dependent arrow
types with forall-expressions in this way. If
p
is any expression,
∀ x : α, p
is nothing more than alternative notation for
(x : α) → p
, with the idea that the former is more natural than the latter
in cases where
p
is a proposition. Typically, the expression
p
will depend on
x : α
. Recall that, in the case of ordinary
function spaces, we could interpret
α → β
as the special case of
(x : α) → β
in which
β
does not depend on
x
. Similarly, we
can think of an implication
p → q
between propositions as the
special case of
∀ x : p, q
in which the expression
q
does not
depend on
x
.
Here is an example of how the propositions-as-types correspondence gets put into practice.
example (α : Type) (p q : α → Prop) : (∀ x : α, p x ∧ q x) → ∀ y : α, p y :=
  fun h : ∀ x : α, p x ∧ q x =>
  fun y : α =>
  show p y from (h y).left
As a notational convention, we give the universal quantifier the
widest scope possible, so parentheses are needed to limit the
quantifier over
x
to the hypothesis in the example above. The
canonical way to prove
∀ y : α, p y
is to take an arbitrary
y
,
and prove
p y
. This is the introduction rule. Now, given that
h
has type
∀ x : α, p x ∧ q x
, the expression
h y
has type
p y ∧ q y
. This is the elimination rule. Taking the left conjunct
gives the desired conclusion,
p y
.
Remember that expressions which differ up to renaming of bound
variables are considered to be equivalent. So, for example, we could
have used the same variable,
x
, in both the hypothesis and
conclusion, and instantiated it by a different variable,
z
, in the
proof:
example (α : Type) (p q : α → Prop) : (∀ x : α, p x ∧ q x) → ∀ x : α, p x :=
  fun h : ∀ x : α, p x ∧ q x =>
  fun z : α =>
  show p z from And.left (h z)
As another example, here is how we can express the fact that a relation,
r
, is transitive:
variable (α : Type) (r : α → α → Prop)
variable (trans_r : ∀ x y z, r x y → r y z → r x z)

variable (a b c : α)
variable (hab : r a b) (hbc : r b c)

#check trans_r    -- ∀ (x y z : α), r x y → r y z → r x z
#check trans_r a b c -- r a b → r b c → r a c
#check trans_r a b c hab -- r b c → r a c
#check trans_r a b c hab hbc -- r a c
Think about what is going on here. When we instantiate
trans_r
at
the values
a b c
, we end up with a proof of
r a b → r b c → r a c
.
Applying this to the "hypothesis"
hab : r a b
, we get a proof
of the implication
r b c → r a c
. Finally, applying it to the
hypothesis
hbc
yields a proof of the conclusion
r a c
.
In situations like this, it can be tedious to supply the arguments
a b c
, when they can be inferred from
hab hbc
. For that reason, it
is common to make these arguments implicit:
variable (α : Type) (r : α → α → Prop)
variable (trans_r : ∀ {x y z}, r x y → r y z → r x z)

variable (a b c : α)
variable (hab : r a b) (hbc : r b c)

#check trans_r
#check trans_r hab
#check trans_r hab hbc
The advantage is that we can simply write
trans_r hab hbc
as a
proof of
r a c
. A disadvantage is that Lean does not have enough
information to infer the types of the arguments in the expressions
trans_r
and
trans_r hab
. The output of the first
#check
command is
r ?m.1 ?m.2 → r ?m.2 ?m.3 → r ?m.1 ?m.3
, indicating
that the implicit arguments are unspecified in this case.
Here is an example of how we can carry out elementary reasoning with an equivalence relation:
variable (α : Type) (r : α → α → Prop)

variable (refl_r : ∀ x, r x x)
variable (symm_r : ∀ {x y}, r x y → r y x)
variable (trans_r : ∀ {x y z}, r x y → r y z → r x z)

example (a b c d : α) (hab : r a b) (hcb : r c b) (hcd : r c d) : r a d :=
  trans_r (trans_r hab (symm_r hcb)) hcd
To get used to using universal quantifiers, you should try some of the
exercises at the end of this section.
It is the typing rule for dependent arrow types, and the universal
quantifier in particular, that distinguishes
Prop
from other
types.  Suppose we have
α : Sort i
and
β : Sort j
, where the
expression
β
may depend on a variable
x : α
. Then
(x : α) → β
is an element of
Sort (imax i j)
, where
imax i j
is the
maximum of
i
and
j
if
j
is not 0, and 0 otherwise.
The idea is as follows. If
j
is not
0
, then
(x : α) → β
is
an element of
Sort (max i j)
. In other words, the type of
dependent functions from
α
to
β
"lives" in the universe whose
index is the maximum of
i
and
j
. Suppose, however, that
β
is of
Sort 0
, that is, an element of
Prop
. In that case,
(x : α) → β
is an element of
Sort 0
as well, no matter which
type universe
α
lives in. In other words, if
β
is a
proposition depending on
α
, then
∀ x : α, β
is again a
proposition. This reflects the interpretation of
Prop
as the type
of propositions rather than data, and it is what makes
Prop
impredicative
.
The term "predicative" stems from foundational developments around the
turn of the twentieth century, when logicians such as Poincaré and
Russell blamed set-theoretic paradoxes on the "vicious circles" that
arise when we define a property by quantifying over a collection that
includes the very property being defined. Notice that if
α
is any
type, we can form the type
α → Prop
of all predicates on
α
(the "power type of
α
"). The impredicativity of
Prop
means that we
can form propositions that quantify over
α → Prop
. In particular,
we can define predicates on
α
by quantifying over all predicates
on
α
, which is exactly the type of circularity that was once
considered problematic.
Equality
Let us now turn to one of the most fundamental relations defined in
Lean's library, namely, the equality relation. In
Chapter Inductive Types
,
we will explain
how
equality is defined from the primitives of Lean's logical framework.
In the meanwhile, here we explain how to use it.
Of course, a fundamental property of equality is that it is an equivalence relation:
#check Eq.refl    -- Eq.refl.{u_1} {α : Sort u_1} (a : α) : a = a
#check Eq.symm    -- Eq.symm.{u} {α : Sort u} {a b : α} (h : a = b) : b = a
#check Eq.trans   -- Eq.trans.{u} {α : Sort u} {a b c : α} (h₁ : a = b) (h₂ : b = c) : a = c
We can make the output easier to read by telling Lean not to insert
the implicit arguments (which are displayed here as metavariables).
universe u

#check @Eq.refl.{u}   -- @Eq.refl : ∀ {α : Sort u} (a : α), a = a
#check @Eq.symm.{u}   -- @Eq.symm : ∀ {α : Sort u} {a b : α}, a = b → b = a
#check @Eq.trans.{u}  -- @Eq.trans : ∀ {α : Sort u} {a b c : α}, a = b → b = c → a = c
The inscription
.{u}
tells Lean to instantiate the constants at the universe
u
.
Thus, for example, we can specialize the example from the previous section to the equality relation:
variable (α : Type) (a b c d : α)
variable (hab : a = b) (hcb : c = b) (hcd : c = d)

example : a = d :=
  Eq.trans (Eq.trans hab (Eq.symm hcb)) hcd
We can also use the projection notation:
variable (α : Type) (a b c d : α)
variable (hab : a = b) (hcb : c = b) (hcd : c = d)
example : a = d := (hab.trans hcb.symm).trans hcd
Reflexivity is more powerful than it looks. Recall that terms in the
Calculus of Constructions have a computational interpretation, and
that the logical framework treats terms with a common reduct as the
same. As a result, some nontrivial identities can be proved by
reflexivity:
variable (α β : Type)

example (f : α → β) (a : α) : (fun x => f x) a = f a := Eq.refl _
example (a : α) (b : β) : (a, b).1 = a := Eq.refl _
example : 2 + 3 = 5 := Eq.refl _
This feature of the framework is so important that the library defines a notation
rfl
for
Eq.refl _
:
variable (α β : Type)
example (f : α → β) (a : α) : (fun x => f x) a = f a := rfl
example (a : α) (b : β) : (a, b).1 = a := rfl
example : 2 + 3 = 5 := rfl
Equality is much more than an equivalence relation, however. It has
the important property that every assertion respects the equivalence,
in the sense that we can substitute equal expressions without changing
the truth value. That is, given
h1 : a = b
and
h2 : p a
, we
can construct a proof for
p b
using substitution:
Eq.subst h1 h2
.
example (α : Type) (a b : α) (p : α → Prop)
        (h1 : a = b) (h2 : p a) : p b :=
  Eq.subst h1 h2

example (α : Type) (a b : α) (p : α → Prop)
    (h1 : a = b) (h2 : p a) : p b :=
  h1 ▸ h2
The triangle in the second presentation is a macro built on top of
Eq.subst
and
Eq.symm
, and you can enter it by typing
\t
.
The rule
Eq.subst
is used to define the following auxiliary rules,
which carry out more explicit substitutions. They are designed to deal
with applicative terms, that is, terms of form
s t
. Specifically,
congrArg
can be used to replace the argument,
congrFun
can be
used to replace the term that is being applied, and
congr
can be
used to replace both at once.
variable (α : Type)
variable (a b : α)
variable (f g : α → Nat)
variable (h₁ : a = b)
variable (h₂ : f = g)

example : f a = f b := congrArg f h₁
example : f a = g a := congrFun h₂ a
example : f a = g b := congr h₂ h₁
Lean's library contains a large number of common identities, such as these:
variable (a b c : Nat)

example : a + 0 = a := Nat.add_zero a
example : 0 + a = a := Nat.zero_add a
example : a * 1 = a := Nat.mul_one a
example : 1 * a = a := Nat.one_mul a
example : a + b = b + a := Nat.add_comm a b
example : a + b + c = a + (b + c) := Nat.add_assoc a b c
example : a * b = b * a := Nat.mul_comm a b
example : a * b * c = a * (b * c) := Nat.mul_assoc a b c
example : a * (b + c) = a * b + a * c := Nat.mul_add a b c
example : a * (b + c) = a * b + a * c := Nat.left_distrib a b c
example : (a + b) * c = a * c + b * c := Nat.add_mul a b c
example : (a + b) * c = a * c + b * c := Nat.right_distrib a b c
Note that
Nat.mul_add
and
Nat.add_mul
are alternative names
for
Nat.left_distrib
and
Nat.right_distrib
, respectively.  The
properties above are stated for the natural numbers (type
Nat
).
Here is an example of a calculation in the natural numbers that uses
substitution combined with associativity and distributivity.
example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  have h1 : (x + y) * (x + y) = (x + y) * x + (x + y) * y :=
    Nat.mul_add (x + y) x y
  have h2 : (x + y) * (x + y) = x * x + y * x + (x * y + y * y) :=
    (Nat.add_mul x y x) ▸ (Nat.add_mul x y y) ▸ h1
  h2.trans (Nat.add_assoc (x * x + y * x) (x * y) (y * y)).symm
Notice that the second implicit parameter to
Eq.subst
, which
provides the context in which the substitution is to occur, has type
α → Prop
.  Inferring this predicate therefore requires an instance
of
higher-order unification
. In full generality, the problem of
determining whether a higher-order unifier exists is undecidable, and
Lean can at best provide imperfect and approximate solutions to the
problem. As a result,
Eq.subst
doesn't always do what you want it
to.  The macro
h ▸ e
uses more effective heuristics for computing
this implicit parameter, and often succeeds in situations where
applying
Eq.subst
fails.
Because equational reasoning is so common and important, Lean provides
a number of mechanisms to carry it out more effectively. The next
section offers syntax that allow you to write calculational proofs in
a more natural and perspicuous way. But, more importantly, equational
reasoning is supported by a term rewriter, a simplifier, and other
kinds of automation. The term rewriter and simplifier are described
briefly in the next section, and then in greater detail in the next
chapter.
Calculational Proofs
A calculational proof is just a chain of intermediate results that are
meant to be composed by basic principles such as the transitivity of
equality. In Lean, a calculational proof starts with the keyword
calc
, and has the following syntax:
calc
  <expr>_0  'op_1'  <expr>_1  ':='  <proof>_1
  '_'       'op_2'  <expr>_2  ':='  <proof>_2
  ...
  '_'       'op_n'  <expr>_n  ':='  <proof>_n
Note that the
calc
relations all have the same indentation. Each
<proof>_i
is a proof for
<expr>_{i-1} op_i <expr>_i
.
We can also use
_
in the first relation (right after
<expr>_0
)
which is useful to align the sequence of relation/proof pairs:
calc <expr>_0 
    '_' 'op_1' <expr>_1 ':=' <proof>_1
    '_' 'op_2' <expr>_2 ':=' <proof>_2
    ...
    '_' 'op_n' <expr>_n ':=' <proof>_n
Here is an example:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)

theorem T : a = e :=
  calc
    a = b      := h1
    _ = c + 1  := h2
    _ = d + 1  := congrArg Nat.succ h3
    _ = 1 + d  := Nat.add_comm d 1
    _ = e      := Eq.symm h4
This style of writing proofs is most effective when it is used in
conjunction with the
simp
and
rewrite
tactics, which are
discussed in greater detail in the next chapter. For example, using
the abbreviation
rw
for rewrite, the proof above could be written
as follows:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)
theorem T : a = e :=
  calc
    a = b      := by rw [h1]
    _ = c + 1  := by rw [h2]
    _ = d + 1  := by rw [h3]
    _ = 1 + d  := by rw [Nat.add_comm]
    _ = e      := by rw [h4]
Essentially, the
rw
tactic uses a given equality (which can be a
hypothesis, a theorem name, or a complex term) to "rewrite" the
goal. If doing so reduces the goal to an identity
t = t
, the
tactic applies reflexivity to prove it.
Rewrites can be applied sequentially, so that the proof above can be
shortened to this:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)
theorem T : a = e :=
  calc
    a = d + 1  := by rw [h1, h2, h3]
    _ = 1 + d  := by rw [Nat.add_comm]
    _ = e      := by rw [h4]
Or even this:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)
theorem T : a = e :=
  by rw [h1, h2, h3, Nat.add_comm, h4]
The
simp
tactic, instead, rewrites the goal by applying the given
identities repeatedly, in any order, anywhere they are applicable in a
term. It also uses other rules that have been previously declared to
the system, and applies commutativity wisely to avoid looping. As a
result, we can also prove the theorem as follows:
variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)
theorem T : a = e :=
  by simp [h1, h2, h3, Nat.add_comm, h4]
We will discuss variations of
rw
and
simp
in the next chapter.
The
calc
command can be configured for any relation that supports
some form of transitivity. It can even combine different relations.
example (a b c d : Nat) (h1 : a = b) (h2 : b ≤ c) (h3 : c + 1 < d) : a < d :=
  calc
    a = b     := h1
    _ < b + 1 := Nat.lt_succ_self b
    _ ≤ c + 1 := Nat.succ_le_succ h2
    _ < d     := h3
You can "teach"
calc
new transitivity theorems by adding new instances
of the
Trans
type class. Type classes are introduced later, but the following
small example demonstrates how to extend the
calc
notation using new
Trans
instances.
def divides (x y : Nat) : Prop :=
  ∃ k, k*x = y

def divides_trans (h₁ : divides x y) (h₂ : divides y z) : divides x z :=
  let ⟨k₁, d₁⟩ := h₁
  let ⟨k₂, d₂⟩ := h₂
  ⟨k₁ * k₂, by rw [Nat.mul_comm k₁ k₂, Nat.mul_assoc, d₁, d₂]⟩

def divides_mul (x : Nat) (k : Nat) : divides x (k*x) :=
  ⟨k, rfl⟩

instance : Trans divides divides divides where
  trans := divides_trans

example (h₁ : divides x y) (h₂ : y = z) : divides x (2*z) :=
  calc
    divides x y     := h₁
    _ = z           := h₂
    divides _ (2*z) := divides_mul ..

infix:50 " ∣ " => divides

example (h₁ : divides x y) (h₂ : y = z) : divides x (2*z) :=
  calc
    x ∣ y   := h₁
    _ = z   := h₂
    _ ∣ 2*z := divides_mul ..
The example above also makes it clear that you can use
calc
even if you
do not have an infix notation for your relation. Finally we remark that
the vertical bar
∣
in the example above is the unicode one. We use
unicode to make sure we do not overload the ASCII
|
used in the
match .. with
expression.
With
calc
, we can write the proof in the last section in a more
natural and perspicuous way.
example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  calc
    (x + y) * (x + y) = (x + y) * x + (x + y) * y  := by rw [Nat.mul_add]
    _ = x * x + y * x + (x + y) * y                := by rw [Nat.add_mul]
    _ = x * x + y * x + (x * y + y * y)            := by rw [Nat.add_mul]
    _ = x * x + y * x + x * y + y * y              := by rw [←Nat.add_assoc]
The alternative
calc
notation is worth considering here. When the
first expression is taking this much space, using
_
in the first
relation naturally aligns all relations:
example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  calc (x + y) * (x + y)
    _ = (x + y) * x + (x + y) * y       := by rw [Nat.mul_add]
    _ = x * x + y * x + (x + y) * y     := by rw [Nat.add_mul]
    _ = x * x + y * x + (x * y + y * y) := by rw [Nat.add_mul]
    _ = x * x + y * x + x * y + y * y   := by rw [←Nat.add_assoc]
Here the left arrow before
Nat.add_assoc
tells rewrite to use the
identity in the opposite direction. (You can enter it with
\l
or
use the ascii equivalent,
<-
.) If brevity is what we are after,
both
rw
and
simp
can do the job on their own:
example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  by rw [Nat.mul_add, Nat.add_mul, Nat.add_mul, ←Nat.add_assoc]

example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  by simp [Nat.mul_add, Nat.add_mul, Nat.add_assoc]
The Existential Quantifier
Finally, consider the existential quantifier, which can be written as
either
exists x : α, p x
or
∃ x : α, p x
.  Both versions are
actually notationally convenient abbreviations for a more long-winded
expression,
Exists (fun x : α => p x)
, defined in Lean's library.
As you should by now expect, the library includes both an introduction
rule and an elimination rule. The introduction rule is
straightforward: to prove
∃ x : α, p x
, it suffices to provide a
suitable term
t
and a proof of
p t
. Here are some examples:
example : ∃ x : Nat, x > 0 :=
  have h : 1 > 0 := Nat.zero_lt_succ 0
  Exists.intro 1 h

example (x : Nat) (h : x > 0) : ∃ y, y < x :=
  Exists.intro 0 h

example (x y z : Nat) (hxy : x < y) (hyz : y < z) : ∃ w, x < w ∧ w < z :=
  Exists.intro y (And.intro hxy hyz)

#check @Exists.intro -- ∀ {α : Sort u_1} {p : α → Prop} (w : α), p w → Exists p
We can use the anonymous constructor notation
⟨t, h⟩
for
Exists.intro t h
, when the type is clear from the context.
example : ∃ x : Nat, x > 0 :=
  have h : 1 > 0 := Nat.zero_lt_succ 0
  ⟨1, h⟩

example (x : Nat) (h : x > 0) : ∃ y, y < x :=
  ⟨0, h⟩

example (x y z : Nat) (hxy : x < y) (hyz : y < z) : ∃ w, x < w ∧ w < z :=
  ⟨y, hxy, hyz⟩
Note that
Exists.intro
has implicit arguments: Lean has to infer
the predicate
p : α → Prop
in the conclusion
∃ x, p x
.  This
is not a trivial affair. For example, if we have
hg : g 0 0 = 0
and write
Exists.intro 0 hg
, there are many possible values
for the predicate
p
, corresponding to the theorems
∃ x, g x x = x
,
∃ x, g x x = 0
,
∃ x, g x 0 = x
, etc. Lean uses the
context to infer which one is appropriate. This is illustrated in the
following example, in which we set the option
pp.explicit
to true
to ask Lean's pretty-printer to show the implicit arguments.
variable (g : Nat → Nat → Nat)
variable (hg : g 0 0 = 0)

theorem gex1 : ∃ x, g x x = x := ⟨0, hg⟩
theorem gex2 : ∃ x, g x 0 = x := ⟨0, hg⟩
theorem gex3 : ∃ x, g 0 0 = x := ⟨0, hg⟩
theorem gex4 : ∃ x, g x x = 0 := ⟨0, hg⟩

set_option pp.explicit true  -- display implicit arguments
#print gex1
#print gex2
#print gex3
#print gex4
We can view
Exists.intro
as an information-hiding operation, since
it hides the witness to the body of the assertion. The existential
elimination rule,
Exists.elim
, performs the opposite operation. It
allows us to prove a proposition
q
from
∃ x : α, p x
, by
showing that
q
follows from
p w
for an arbitrary value
w
. Roughly speaking, since we know there is an
x
satisfying
p x
, we can give it a name, say,
w
. If
q
does not mention
w
, then showing that
q
follows from
p w
is tantamount to
showing that
q
follows from the existence of any such
x
. Here
is an example:
variable (α : Type) (p q : α → Prop)

example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  Exists.elim h
    (fun w =>
     fun hw : p w ∧ q w =>
     show ∃ x, q x ∧ p x from ⟨w, hw.right, hw.left⟩)
It may be helpful to compare the exists-elimination rule to the
or-elimination rule: the assertion
∃ x : α, p x
can be thought of
as a big disjunction of the propositions
p a
, as
a
ranges over
all the elements of
α
. Note that the anonymous constructor
notation
⟨w, hw.right, hw.left⟩
abbreviates a nested constructor
application; we could equally well have written
⟨w, ⟨hw.right, hw.left⟩⟩
.
Notice that an existential proposition is very similar to a sigma
type, as described in dependent types section.  The difference is that
given
a : α
and
h : p a
, the term
Exists.intro a h
has
type
(∃ x : α, p x) : Prop
and
Sigma.mk a h
has type
(Σ x : α, p x) : Type
. The similarity between
∃
and
Σ
is another
instance of the Curry-Howard isomorphism.
Lean provides a more convenient way to eliminate from an existential
quantifier with the
match
expression:
variable (α : Type) (p q : α → Prop)

example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  match h with
  | ⟨w, hw⟩ => ⟨w, hw.right, hw.left⟩
The
match
expression is part of Lean's function definition system,
which provides convenient and expressive ways of defining complex
functions.  Once again, it is the Curry-Howard isomorphism that allows
us to co-opt this mechanism for writing proofs as well.  The
match
statement "destructs" the existential assertion into the components
w
and
hw
, which can then be used in the body of the statement
to prove the proposition. We can annotate the types used in the match
for greater clarity:
variable (α : Type) (p q : α → Prop)
example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  match h with
  | ⟨(w : α), (hw : p w ∧ q w)⟩ => ⟨w, hw.right, hw.left⟩
We can even use the match statement to decompose the conjunction at the same time:
variable (α : Type) (p q : α → Prop)
example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  match h with
  | ⟨w, hpw, hqw⟩ => ⟨w, hqw, hpw⟩
Lean also provides a pattern-matching
let
expression:
variable (α : Type) (p q : α → Prop)
example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  let ⟨w, hpw, hqw⟩ := h
  ⟨w, hqw, hpw⟩
This is essentially just alternative notation for the
match
construct above. Lean will even allow us to use an implicit
match
in the
fun
expression:
variable (α : Type) (p q : α → Prop)
example : (∃ x, p x ∧ q x) → ∃ x, q x ∧ p x :=
  fun ⟨w, hpw, hqw⟩ => ⟨w, hqw, hpw⟩
We will see in
Chapter Induction and Recursion
that all these variations are
instances of a more general pattern-matching construct.
In the following example, we define
is_even a
as
∃ b, a = 2 * b
,
and then we show that the sum of two even numbers is an even number.
def is_even (a : Nat) := ∃ b, a = 2 * b

